{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c879503",
   "metadata": {},
   "source": [
    "### RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d782d427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\pdf_files\\\\science.adw1291.pdf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_dir = Path(\"../data/pdf_files\")\n",
    "pdfs=list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "str(pdfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a312dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: science.adw1291.pdf\n",
      "  ‚úì Loaded 4 pages\n",
      "\n",
      "Processing: SN_misinformation.pdf\n",
      "  ‚úì Loaded 15 pages\n",
      "\n",
      "Processing: Video_models_ZS_learners_reasoners.pdf\n",
      "  ‚úì Loaded 46 pages\n",
      "\n",
      "Total documents loaded: 65\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ‚úì  Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96748c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2025-10-24T19:40:28+08:00', 'moddate': '2025-11-30T15:08:48-08:00', 'trapped': '/False', 'subject': 'Science 2025.389:1012-1015', 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping', 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1012', 'source_file': 'science.adw1291.pdf', 'file_type': 'pdf'}, page_content='Resea Rch aRticle s\\nScience 4 September 2025 1012\\nast ROP hYsics\\nImproving cosmological reach  \\nof a gravitational wave observatory \\nusing Deep Loop Shaping\\nJonas Buchli1*‚Ä† Brendan T racey1‚Ä†, T omislav Andric2,3‚Ä†, \\nChristopher Wipf4‚Ä†, Yu Him Justin Chiu1‚Ä†, Matthias Lochbrunner1‚Ä†, \\nCraig Donner1‚Ä†, Rana X. Adhikari4*, Jan Harms2,3*‚Ä†,  \\nIain Barr1, Roland Hafner1, Andrea Huber1, Abbas Abdolmaleki1,  \\nCharlie Beattie1, Joseph Betzwieser 4, Serkan Cabi 1,  \\nJonas Degrave1, Yuzhu Dong1, Leslie Fritz1, Anchal Gupta4,  \\nOliver Groth1, Sandy Huang1, Tamara Norman1, Hannah Openshaw1, \\nJameson Rollins4, Greg Thornton1, George van den Driessche1, \\nMarkus Wulfmeier1, Pushmeet Kohli1*, Martin Riedmiller1,  \\nThe LIGO Instrument T eam‚Ä°\\nImproved low- frequenc y sensitivity of gravitational wave \\nobservatories would unlock study of intermediate- mass black \\nhole mer\\ngers and binary black hole eccentricity and provide \\nearly warnings for multimessenger observations of binary \\nneutron star mergers. today\\n‚Äôs mirror stabilization control \\ninjects harmful noise, constituting a major obstacle to \\nsensitivity improvements. We eliminated this noise through \\nDeep Loop Shaping, a reinforcement learning method using \\nfrequency domain rewards. We proved our methodology on the \\nLIGO Livingston Observatory (LLO). Our controller reduced \\ncontrol noise in the 10-\\n  to 30- hertz band b\\ny over 30x and up to \\n100x in subbands, surpassing the design goal motivated by the \\nquantum limit. \\nthe\\nse results highlight the potential of Deep \\nLoop Shaping to improve current and future gravitational wave \\nobservatories and, more broadly, instrumentation and control \\nsystems.\\nThe gravitational wave (GW) detectors LIGO and Virgo have revolu -\\ntionized astrophysics by detecting mergers of exotic objects, such as \\nblack holes (BHs) and neutron stars (NSs) (1 ‚Äì3). Currently, most of \\nthe detectable signal lies in the 30-  to 2000\\n- Hz band, \\nleaving the low-  \\nfrequency band (10 to 30 Hz) largely unexplored. Enhancing sensitiv-\\nity in this band could lead to a substantial increase in cosmological \\nreach and thus in the scientific capabilities of LIGO (Fig.\\xa01A). The \\n10-\\n  to 30- Hz band is also impor\\ntant for the early (premerger) detec-\\ntion of binary neutron stars (BNSs), potentially doubling the warning \\ntime, which would enable real- time obser vation of neutron star colli-\\nsions, the subsequent creation of heavy elements, and the birth of\\nblack holes (4 ‚Äì6). However, such sensitivity improvements are cur-\\nrently partially limited by injected control noise on the interferom-\\neter mirrors. Furthermore, as the control noise is a bottleneck to\\noverall sensitivity improvements, there is currently little to be gained\\nfrom improvements to other noise sources. We addressed this chal-\\nlenge with a new tailored reinforcement learning (RL) method and\\nimproved the alignment control of the LIGO mirrors. We lowered\\nthe injected control noise on the most demanding feedback control\\nloop, the common- hard- pitch ( Œ∏CHP) loop of the Livingston Obser va to ry,\\nbelow the quantum back-  action limit. By elimina ting the harmful \\nnoise from this critical representative controller, we paved the path \\nto improve LIGO‚Äôs sensitivity.\\nThe space- time str ain associated with even the loudest GW signals \\nproduces a signal equivalent to only ‚âà10‚Äì19 meters of mirror motion. \\nAs a comparison, the environmental disturbance, due to Earth tides \\nand seismic vibration, is roughly 13 orders of magnitude larger. To \\nmeasure the weak GW signals, laser-\\n interferome\\ntric GW detectors \\nhave hundreds of optomechanical degrees of freedom that require \\nstabilization. Active control is used to achieve precise stabilization \\nin the face of complex mirror dynamics and inherently unstable \\ndegrees of freedom. More specifically, the optomechanical response \\n1Google DeepMind, London, UK. 2Gran Sasso Science Institute (GSSI), L‚Äô Aquila, Italy. \\n3Laboratori Nazionali del Gran Sasso, Assergi (INFN), Italy. 4LIGO Laboratory, Division of \\nPhysics, Math, and Astronomy, California Institute of T echnology, Pasadena, CA, USA. \\n*Corresponding author. Email: buchli@  google\\n. com (J\\n.B.); rana@ caltech. edu (R.X.A.);\\njan. harms@ g\\nssi. it \\n(J.H.); pushmeet@  g\\noogle. com \\n(P .K.) ‚Ä†These authors contributed\\nequally to this work. ‚Ä°LIGO Instrument T eam authors and affiliations are listed in the \\nsupplementary materials.\\nA\\nB\\nFig. 1. Cosmological reach and strain noise from control. (A) The plot shows the \\nvolume in space explored with binary BH merger waveforms (21) for different cases of \\ntechnical noise. The x axis in (A) is the total mass of the equal- mass binar\\ny pair. This \\ncorresponds to the x axis in (B), the frequency of the first quasinormal mode of a \\nSchwarzschild BH with such a mass, as measured in the source frame. The purple trace \\nshows the reach of LIGO as of March 2024. The green trace shows the volumetric \\nimprovement in the case where the technical noise is removed entirely. Many of the \\nknown technical noise sources are linked to controls. (B) LIGO‚Äôs noise budget and \\ncontroller performance. Lavender represents overall measured strain noise, red \\nrepresents strain noise contribution from the currently operational linear controller for \\nŒ∏\\nCHP, and blue represents strain noise contribution from neural network RL policy as\\nrun on the LIGO Livingston Observatory on 5 Dec 2024 (mean, 10th and 90th percentiles \\nof amplitude spectral density (ASD) of control action of neural network control policy). \\nThe dashed green line indicates the control design goal derived from the quantum \\nback-\\n action limit by applying a de\\nsign margin of 10x; the control noise should drop below \\nthis curve. A detailed accounting of technical noise sources is available in (22).\\nCORRECTED 24 OCTOBER 202; SEE LAST PAGE\\nDownloaded from https://www.science.org on November 30, 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2025-10-24T19:40:28+08:00', 'moddate': '2025-11-30T15:08:48-08:00', 'trapped': '/False', 'subject': 'Science 2025.389:1012-1015', 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping', 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'total_pages': 4, 'page': 1, 'page_label': '1013', 'source_file': 'science.adw1291.pdf', 'file_type': 'pdf'}, page_content='ReseaR ch aR ticle s\\nScience 4 September 2025 1013\\nof the interferometer (i.e., the plant) is subject to dynamic variations: \\nEven low absorption of the high- power laser beam (‚àº300 to 500 kW) \\ncauses thermal distortions in the mirrors, leading to offsets in sensor \\nsignals and changes in optomechanical resonant frequencies. In ad-\\ndition, the high- power laser also creates substantial forces and \\ntorques on the suspended mirrors, leading to optomechanical insta-\\nbilities of several mechanical eigenmodes (7 ‚Äì9). These resonances \\nare stabilized using feedback control, but any noise injected by the \\nfeedback controllers into the GW readout harms the peak astrophysi-\\ncal sensitivity and drowns out the GW signals themselves.\\nIn simplified terms, the main control design challenge is that \\nlarger control action in lower frequencies provides better disturbance \\nrejection but injects higher noise into the observation band. Con-\\nversely, lowering the control action reduces injected noise but results \\nin insufficient disturbance rejection and possible loss of stability. \\nLinear control systems theory shows fundamental limits to this \\ntrade- off (10,\\xa011) under certain assumptions about the controller de -\\nsign. The ultimate aim of controller design is to shape the ‚Äúclosed- loop‚Äù \\nbehavior, i.e., the performance of the designed controller acting in a \\nfeedback loop with the plant.\\nThere are many classical methods to achieve the desired closed-  \\nloop behavior. Early methods, i.e., the classic (open)‚Äìloop shaping \\nmethods, exploit the direct relationship between the open-  and \\nclosed- loop transfer functions to design the controller. Since the \\n1980s, the focus of design has shifted from open- loop design to di-\\nrectly shaping closed- loop transfer functions, i.e., the sensitivity func-\\ntions (12,\\xa013), usually through optimization [e.g., convex optimization \\n(14), H‚àû (15)]. These methods are more general and can take into \\naccount a larger variety of design goals and constraints. Yet they still \\nrequire strong assumptions, such as convexity and linearity. For GW \\ndetectors like LIGO, progress using traditional approaches has come \\nto a plateau. Machine learning for interferometer control was pre -\\nsented in (16), with the primary aim to improve the contrast of the \\ninterferometer rather than to optimize closed-loop performance.\\nIn this work, we present a new control design method, Deep Loop \\nShaping (DLS), to design controllers that satisfy specific demands on \\nthe system‚Äôs frequency  domain behavior (Fig.\\xa02). DLS has no constraints \\nregarding the use of nonlinear models and control structures. It exploits \\nthe machinery of deep reinforcement learning to directly optimize \\nfrequency domain properties and shape the closed-loop behavior. We \\ndemonstrate DLS‚Äôs utility on the critical LIGO Œ∏ CHP control loop, \\nachieving state- of- the- art feedback control performance. The injected \\ncontrol noise was reduced by up to two orders of magnitude while \\nmaintaining mirror stability. Applying DLS more widely on LIGO can \\nimprove sensitivity. Furthermore, the method has wide applicability \\nto control engineering; for example, highly unstable systems, vibra-\\ntion suppression, and noise cancellation all have strong frequency-  \\ndependent control demands.\\nThe LIGO controls challenge\\nAngular sensing and control (ASC) is the challenge of maintaining the \\norientation of the interferometer mirrors. Stabilization is accom -\\nplished through a hybrid active- passive isolation system. Passive sta-\\nbilization happens through a series of pendulums, from which the \\noptics are suspended. These pendulums suppress seismic disturbances \\nat frequencies above 10 Hz by several orders of magnitude. However, \\n3\\nSIMULATION\\nENVIRONMENT2\\nREAL WORLD\\nSYSTEM ID\\nREAL WORLD\\nDEPLOYMENT1\\nREPLAY\\nBUFFER\\nREPLAY\\nBUFFER\\nLIGO\\n4km4km\\nLEARNING\\nLOOPS\\nCODE\\nGENERATION\\nREAL \\nTIME CONTRO\\nL SYSTEM\\n<imp\\nort\\n  cont\\nrol.h>\\nGOOD\\nX\\nf\\nf\\nBAD\\nGOOD\\nr(t)\\n0\\n1\\n1\\n0\\nBAD\\nspectral density\\nFILTER\\nFILTER\\nSCORE\\nSCORE\\nspectral density\\nA\\nB\\nFig. 2. DLS: Reinforcement learning with frequency domain rewards. (A) (1) A model is identified from plant measurements. (2) The identified model is used as a learning \\nenvironment. Frequency domain rewards are used to compute rewards. (3) The optimized control policy is deployed on the plant. (B) Illustration of the frequency domain \\nrewards and the multiplicative scoring.\\nDownloaded from https://www.science.org on November 30, 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2025-10-24T19:40:28+08:00', 'moddate': '2025-11-30T15:08:48-08:00', 'trapped': '/False', 'subject': 'Science 2025.389:1012-1015', 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping', 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'total_pages': 4, 'page': 2, 'page_label': '1014', 'source_file': 'science.adw1291.pdf', 'file_type': 'pdf'}, page_content='ReseaR ch aR ticle s\\nScience 4 September 2025 1014\\nactive stabilization is required to reject seismic disturbances at fre -\\nquencies below ~ 3 Hz. This stabilization is accomplished by a set of \\nactuators that produce a torque on the suspended optics at the pen-\\nultimate stage of the suspension system. Additionally, there are dis-\\nturbances caused by the radiation- pressure forces of the high- power \\nlaser beam (17) that couple the angular motions of the cavity mirrors. \\nThe sensors used to measure the angular motion have a good signal-  \\nto- noise ratio at frequencies below a few hertz to enable active sta-\\nbilization, but in the 10-  to 30- Hz band, the sensor noise is orders \\nof magnitude larger than the signal related to angular motion, and \\nso motion in this band is not visible from the angular sensor and is \\nonly seen in the interferometer strain spectrum. The active control-\\nler injects sensing noise in this frequency band through the actuator \\nto the test mass. This effect is the primary cause of test mass angular \\nmotion at frequencies above 10 Hz (8 ,\\xa018). Avoiding the injection of \\nnoise as much as possible and at the same time guaranteeing rejec-\\ntion of seismic disturbances is the main design goal for ASC control -\\nlers. We address this problem, which comprises the most challenging \\ncontrol loops in a GW obser  va tory, with DLS.\\nThe Œ∏CHP loop\\nIn this work, we primarily focus on the ASC control loop, ‚Äúcommon \\nhard pitch‚Äù (Œ∏CHP). ‚ÄúHard pitch‚Äù refers to the stiffer of the two opto-\\nmechanical pitch eigenmodes of the arm cavities. ‚ÄúCommon‚Äù signifies \\na relation of modes between the cavities of the two arms (8 ). The \\nŒ∏CHP degree of freedom is the most difficult of the entire ASC system \\nto stabilize and optimize. Reducing the control noise in this loop well \\nbelow the quantum limit would remove this source of noise as a \\nblocking issue for improved astrophysical sensitivity.\\nClosed- loop shaping as a reinforcement learning problem\\nIn this work, we formulated the Œ∏ CHP closed- loop control as an opti-\\nmal control problem and found approximate solutions through re -\\ninforcement learning (RL). ASC requirements are naturally expressed \\nas functions of the system response in the frequency domain, i.e., as \\ndesired spectra of state-  space signals. We introduced a new reward \\nscheme based on frequency domain behavior to enforce the desired \\nclosed- loop shaping of the control policy in such a way that RL could \\ndiscover an effective control policy. It is similar to traditionally used \\nmethods of shaping sensitivity functions, but RL removes restric -\\ntions on the reward definition and system dynamics. RL can also \\ndiscover nonlinear policies represented with deep neural networks \\nthat can serve as drop- in replacements for the existing hand- crafted \\ncontrollers and enables improved performance without compromis-\\ning robustness.\\nRL designs controllers by adapting a parameterized state-  action \\nmapping. Our specific choice of learning algorithm is maximum a \\nposteriori policy optimization (MPO) (19). We used a small multilayer \\nperceptron (MLP) with a dilated convolution input layer for the policy \\nnetwork, which executes sufficiently fast for control. The critic network \\nis a long-short-term- memory (LSTM) network with input and output \\nMLPs, as the critic is not needed for deployment.\\nFrequency domain rewards\\nRL naturally lends itself to reward descriptions formulated in the \\ntime domain, e.g., scoring events that happen at certain times. In-\\nstead, we directly formulated the ASC requirements as rewards in \\nthe frequency domain (Fig.\\xa02B). To do so, we designed linear filters \\nfor the Œ∏CHP response signal whose transfer functions each select a \\ncertain frequency band of the signal. We used a low- pass filter to \\nreward pitch alignment, a band- pass filter to reduce control action \\nin the 8-  to 30- Hz band, and an additional band- pass filter for fre -\\nquencies >40 Hz to avoid high- frequency artifacts. A high output from \\na filter at a given timestep corresponds to a large historic response \\nin the measured frequency band. These per- timestep response \\nmeasures can then be used to construct a reward for RL. Specifically, \\nwe computed the RL reward by passing the filter outputs through a \\nsigmoid function to compute a (per- filter) score in [0,1], with a value \\nof 1 when the specification was fulfilled and fading to 0 as the re -\\nsponse worsened. These individual filter scores were multiplied to \\nyield the per- timestep reward, then used by the RL method to choose \\na policy that minimizes the discounted sum of this reward over time. \\nThis formulation of multiplying rewards can loosely be understood \\nas a soft logical- AND; i.e., we wanted all properties to be fulfilled for \\nhigh reward.\\nTraining and deployment\\nWe trained nonlinear control policies with RL against a linear \\nstochastic state-  space simulation of the plant dynamics (i.e., op-\\ntomechanical response of the interferometer) identified from mea -\\nsurement data of the plant. We used domain randomization to add \\nrobustness to the learned policies. Specifically, at the beginning \\nof each episode, we randomized the angular instability pole fre -\\nquency and sampled variations in the seismic noise, including the \\noverall noise strength.\\nAt the conclusion of training, we performed several steps to ready \\nthe policy for hardware testing. First, a deterministic policy was cre -\\nated by using only the mean of the policy Gaussian. Second, we vali-\\ndated this deterministic policy across a selected set of disturbances \\nand nonnominal plant parameters. We examined the reward achieved \\nas well as measured key performance criteria, such as root mean \\nsquare of the control effort in the observation band (10 to 30 Hz), and \\nvisually inspected the error and control spectra. With performance \\nconfirmed, we ‚Äúexported‚Äù the policy for the hard real- time control \\nrequired for execution on LIGO without further training or adaptation \\non the plant.\\nWe deployed the control policies directly in the existing control \\ninfrastructure of the interferometer (20). As such, the RL- trained poli-\\ncies were drop-  in replacements of the existing single-input, single-\\noutput (SISO) controllers. In particular, the LIGO control system \\nuses somewhat arbitrary ‚Äúcounts‚Äù as the units for ASC inputs and \\noutputs, and we adopted these conventions for the controller and the \\ncontroller-  simulator interface. We have also reported our results in \\nthese units for technical reasons.\\nDeployment on gravitational wave observatory hardware\\nWe ran the deployed policies on the LIGO Livingston Observatory \\n(LLO). In the experiments, the control of Œ∏ CHP was under the sole \\nauthority of a neural network‚Äìbased control policy. We measured the \\nASC noise during policy execution as well as comparison spectra from \\nthe standard controller before and after the nonlinear policy. In\\xa0Fig.\\xa01B \\nwe compare the performance of the neural network policy against the \\nstandard controller for a >10 min stretch on 5 December 2024. The \\nfigure shows the projection of the measured angular control noise into \\nthe GW readout. Additional details of this experiment are shown in \\nthe supplementary materials.\\nWe found excellent performance for the neural network policy. \\nIn the crucial 3-  to 30- Hz band, we see a reduction of noise of up to \\ntwo orders of magnitude. At the same time, the neural network \\npolicy shows similar control authority as the linear controller in the \\ncontrol band (< 3 Hz). The control noise added by the neural net-\\nwork policy is well below the fundamental thermodynamic noise \\nand quantum back- action noise in the whole band of interest. These \\nresults show that the neural network policy has effectively removed \\nthe issue of noise injected by active control as a limit to the astro-\\nphysical sensitivity.\\nIn the supplementary materials, we present additional results from \\nApril and August 2024, with total time on the instrument of well over \\n1 hour. The sustained control of the unstable Œ∏CHP mode demonstrates \\nrobustness of the neural network policy to normal seismic activity. We \\nDownloaded from https://www.science.org on November 30, 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2025-10-24T19:40:28+08:00', 'moddate': '2025-11-30T15:08:48-08:00', 'trapped': '/False', 'subject': 'Science 2025.389:1012-1015', 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping', 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'total_pages': 4, 'page': 3, 'page_label': '1015', 'source_file': 'science.adw1291.pdf', 'file_type': 'pdf'}, page_content='Resea Rch aRticle s\\nScience 4 Sep tember 2025 1015\\nsaw a good match between the training simulation and the real plant \\nunder the tested conditions for frequencies >0.1 Hz, which increases \\nconfidence in our results. We additionally compared the control policy \\nagainst the incumbent linear controller in terms of statistical mea -\\nsures, such as non-  Gaus sianity and nonstationarity. We found that, \\nalthough the policy does exhibit some nonstationarity, the overall re-\\nduction in noise still leads to a clear benefit for signal detection.\\nFor comparison, we derived controllers with convex optimization \\nand show a series of simulation-\\n based \\nresults in the supplementary \\nmaterials. Although these optimized linear controllers have similar \\npredicted performance, they are not fit for deployment in the high-  \\nstakes environment of the real observatory. In particular, they are \\nopen- loop uns table, and their disturbance rejection behavior is highly \\naggressive, in contrast to the neural network policies. In addition to \\nexperiments on LLO, we used the same methodology on the mode-\\n \\ncle\\naner of the Caltech prototype and similarly found that DLS is ca-\\npable of reducing noise in a band of interest while maintaining \\noverall control.\\nReFeReNces aND NOte s\\n 1. LIGO S\\ncientific Collaboration; Virgo Collaboration; Phys. Rev. Lett.  116, 061102 (2016). \\n 2. LIGO S\\ncientific Collaboration; Virgo Collaboration; Phys. Rev. Lett.  119, 161101 (2017). \\n 3\\n. LIGO S\\ncientific Collaboration; Virgo Collaboration; KAGRA Collaboration; Phys. Rev. X  13, \\n041039 (2023). \\n 4\\n. H.\\n Yu, R. X. Adhikari, R. Magee, S. Sachdev, Y. Chen, Phys. Rev. D  104, 062004 \\n(2021). \\n5. B\\n. Banerjee et al., Astron. Astrophys.  678, A126 (2023). \\n6. A.\\n T ohuvavohu et al., Astrophys. J. Lett.  975, L19 (2024). \\n 7\\n. J\\n. A. Sidles, D. Sigg, Phys. Lett. A  354, 167‚Äì172 (2006). \\n 8\\n. L.\\n Barsotti, M. Evans, P . Fritschel, Class. Quantum Gravity  27, 084026 (2010). \\n 9\\n. V\\n. Braginsky, S. Strigin, S. Vyatchanin, Phys. Lett. A  287, 331‚Äì338 (2001). \\n 1\\n0. K.\\n J. Astr√∂m, R. Murray, Feedback Systems: An Introduction for Scientists and Engineers \\n(Princeton Univ. Press, ed. 2, 2021); https://books.google.com/books?id=l50DEAAAQBAJ.\\n11. G\\n. Stein, IEEE Control Syst.  23, 12‚Äì25 (2003). \\n12. H.\\n W. Bode, Network Analysis and Feedback Amplifier Design (D. Van Nostrand Company, \\n1945).\\n13.\\n G\\n. Zames, IEEE T rans. Automat. Contr.  26, 301‚Äì320 (1981). \\n 1\\n4. C\\n. Barratt, S. Boyd, in Control and Dynamical Systems: Digital and Numeric T echniques and \\nTheir Applications in Control Systems, Part 1,  vol. 55, C. T . Leondes, Ed. (Academic Press, \\n1993), pp. 1‚Äì24.\\n 15\\n. J\\n. C. Doyle, K. Glover, P . P . Khargonekar, B. A. Francis, IEEE T rans. Automat. Contr.  34, \\n831‚Äì847 (1989). \\n16. N.\\n Mukund et al., Phys. Rev. Appl.  20, 064041 (2023).\\n 1\\n7 . K.\\n L. Dooley et al., J. Opt. Soc. Am. A Opt. Image Sci. Vis.  30, 2618‚Äì2626 (2013). \\n18.\\n H\\n. Yu et al., Phys. Rev. Lett.  120, 141102 (2018). \\n19. A.\\n Abdolmaleki et al., Maximum a Posteriori Policy Optimisation, International Conference \\non Learning Representations, Vancouver, CA, 30 April to 3 May 2018. (ICLR, 2018).\\n20. R.\\n Bork et al., SoftwareX  13, 100619 (2021). \\n21. S\\n. Khan et al., Phys. Rev. D  93, 044007 (2016). \\n22. E.\\n Capote et al., Advanced LIGO Detector Performance in the Fourth Observing Run.\\n[gr-\\n \\nqc] (2024).\\n23. M.\\n Hoffman et al., Acme: A research framework for distributed reinforcement learning. \\narXiv:2006.00979 [cs.LG] (2020).\\n24.\\n F\\n. Y ang et al., Launchpad: A Programming Model for Distributed Machine Learning \\nResearch (2021) [cs.DC], arXiv:2106.04516.\\n 25\\n. g\\noogle-\\n deepmind,\\n dm env: A python interface for reinforcement learning environments, \\nGitHub (2019); https://github.com/deepmind/dm_env.\\n26.\\n T\\n. Hennigan, T . Cai, T . Norman, L. Martens, I. Babuschkin, Haiku: Sonnet for JAX (2024); \\nhttps://github.com/deepmind/dm-\\n \\nhaiku.\\n27 . A.\\n Cassirer et al., Reverb: A Framework For Experience Replay. arXiv:2102.04736 [cs.LG] \\n(2021).\\n 28\\n. J\\n. Harms, Lightsaber, https://github.com/janosch314/Lightsaber (2025).\\n 29\\n. LIGO S\\ncientific Collaboration, Identified Plant Model & Selected ASC Experimental Data of \\nLIGO Livingston, Zenodo (2025); https://doi.org/10.5281/zenodo.15793015.\\na\\nc\\nKNOW\\nle\\nDGM\\ne\\nN\\nts\\nW\\ne thank J. Dean for strategic help and inspiration at the start of the project. Funding: The \\nauthors gratefully acknowledge the support of the US National Science Foundation (NSF) for \\nthe construction and operation of the LIGO Laboratory and Advanced LIGO as well as the \\nScience and T echnology Facilities Council of the UK and the Max Planck Society for support of \\nthe construction of Advanced LIGO. Additional support for Advanced LIGO was provided by \\nthe Australian Research Council. LIGO was constructed by the California Institute of \\nT echnology and Massachusetts Institute of T echnology with funding from the NSF and \\noperates under cooperative agreement no. PHY-\\n 186\\n71764464. Advanced LIGO was built under \\ngrant no. PHY- 1868082345\\n9. Author contributions: R.X.A., J.Bu., S.C., J.H., M.R., and B. T . \\nconceived the project. R.X.A., J.Bu., C.D., A.H., J.H., and B. T . led the project. T .A., R.X.A., I.B., \\nJ.Bu., J.Be., C.D., G.v.d.D., J.D., A.G., J.H., M.L., B. T ., G. T ., and C.W. developed the physics \\nsimulations. T .A., I.B., J.Bu., Y.H.J.C., C.D., J.D., M.L., B. T ., and C.W. integrated the physics \\nsimulations with the learning framework. A.A., J.Bu., R.H., S.H., M.L., M.W., and B. T . developed \\nthe learning framework and performed learning experiments. C.D., T .N., J.R., and C.W. \\ndeveloped the real-\\n time neur\\nal network interface. R.X.A., J.Be., J.Bu., C.D., A.G., B. T ., and C.W. \\nintegrated the real-\\n time neur\\nal network with the control system and ran experiments on LLO \\nand the California Institute of T echnology 40m prototype. C.B., J.Bu., Y.H.J.C., C.D., Y.D., O.G., \\nM.L., and C.W. developed data curation tools. R.X.A., I.B., J.Bu., Y.H.J.C., B. T ., and C.W. \\ndeveloped and ran the data analysis. L.F ., P .K., H.O., and M.R. consulted for the project. T .A., \\nR.X.A., J.Bu., J.H., B. T ., and C.W. wrote the manuscript. The LIGO Instrument T eam maintains \\nand runs the LIGO Observatory. Competing interests: The authors declare that they have no \\ncompeting interests. Data and materials availability: The learning algorithm used in the \\nactor-\\n critic RL\\n method is MPO (19), a reference implementation of which is available under an \\nopen-\\n s\\nource license (23). Additionally, the software libraries launchpad (24), dm env (25), \\nJax/Haiku (26), and reverb (27) were used, which are also open source. Simulations were \\nimplemented in Lightsaber (28) and advLigoRTS (20). The identified LLO model and \\nexperimental data are available at (29). License information:  Copyright ¬© 2025 the authors, \\nsome rights reserved; exclusive licensee American Association for the Advancement of \\nScience. No claim to original US government works. https://www.science.org/about/\\nscience-\\n \\nlicenses-\\n \\njournal-\\n \\narticle-\\n \\nreuse\\nsUPP\\nle\\nM\\ne\\nN\\nta\\nRY M\\nate\\nR\\nials\\ns\\ncience.org/doi/10.1126/science.adw1291\\nSupplementary T ext; Figs.\\xa0S1 to S16; T ables\\xa0S1 to S3; References (30‚Äì70)\\nSubmitted 28 January 2025; accepted 7 July 2025\\n10.1126/science.adw1291\\nCorrection (24 October 2025): Reference 16 was inadvertently removed during the review process; it was added back \\nin, and all subsequent references were renumbered. Additionally, the date of experiment was listed incorrectly in the \\nmain text and supplementary materials, and one of the non-byline author affiliations in the supplementary materials \\nwas accidentally duplicated; both of these errors were corrected.\\nDownloaded from https://www.science.org on November 30, 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Simulating Misinformation Propagation in Social Networks using\\nLarge Language Models\\nRaj Gaurav Maurya‚àó\\nrajg.maurya@tum.de\\nTechnische Universit√§t\\nM√ºnchen, Germany\\nVaibhav Shukla\\nFriedrich-Alexander-Universit√§t\\nErlangen-N√ºrnberg, Germany\\nRaj Abhijit Dandekar\\nRajat Dandekar\\nSreedath Panat\\nVizuara AI Labs\\nPune, India\\nFigure 1: How agents propagate misinformation? [Generated using Google AI Studio‚Äôs Nano Banana]\\nAbstract\\nMisinformation on social media thrives on surprise, emotion, and\\nidentity-driven reasoning, often amplified through human cognitive\\nbiases. To investigate these mechanisms, we model large language\\nmodel (LLM) personas as synthetic agents that mimic user-level bi-\\nases, ideological alignments, and trust heuristics. Within this setup,\\nwe introduce an auditor‚Äìnode framework to simulate and ana-\\nlyze how misinformation evolves as it circulates through networks\\nof such agents. News articles are propagated across networks of\\npersona-conditioned LLM nodes, each rewriting received content.\\nA question‚Äìanswering (QA)-based auditor then measures factual\\nfidelity at every step, offering interpretable, claim-level tracking\\nof misinformation drift. We formalize a misinformation index (MI)\\nand a misinformation propagation rate (MPR) to quantify factual\\ndegradation across homogeneous and heterogeneous branches of\\nup to 30 sequential rewrites. Experiments with 21 personas across\\n10 domains reveal that identity- and ideology-based personas (e.g.,\\nreligious leaders, lifestyle influencers, politically aligned individu-\\nals) act as misinformation accelerators, especially in politics, mar-\\nketing, and technology. By contrast, expert-driven personas (e.g.,\\nmedical professionals, investigative journalists) preserve factual\\nstability. Controlled-random branch simulations further show that\\n‚àóCorresponding author\\nonce early distortions emerge, heterogeneous persona interactions\\nrapidly escalate misinformation to propaganda-level distortion. Our\\ntaxonomy of misinformation severity‚Äîspanning factual errors, lies,\\nand propaganda‚Äîconnects observed drift to established theories\\nin misinformation studies. These findings demonstrate the dual\\nrole of LLMs as both proxies for human-like biases and as auditors\\ncapable of tracing information fidelity. The proposed framework\\nprovides an interpretable, empirically grounded approach for study-\\ning, simulating, and mitigating misinformation diffusion in digital\\necosystems.\\nCCS Concepts\\n‚Ä¢Computing methodologies ‚ÜíNatural language processing;\\nMulti-agent systems;‚Ä¢Human-centered computing ‚ÜíCol-\\nlaborative and social computing theory, concepts and paradigms;‚Ä¢\\nInformation systems ‚ÜíData mining;‚Ä¢Security and privacy\\n‚ÜíHuman and societal aspects of security and privacy;‚Ä¢Applied\\ncomputing‚ÜíPsychology; Sociology.\\nKeywords\\nlarge language models, social simulation, social networks, misin-\\nformation, fake news, propaganda, persona, agents, QA, auditor,\\nnode\\narXiv:2511.10384v1  [cs.SI]  13 Nov 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\n1 Introduction\\nMisinformation on social media undermines public trust and infor-\\nmation quality. Vosoughi et al. [28] analyzed 126,000 true and false\\nnews stories on X1 and found that false news spreads faster and far-\\nther, largely due to human behavior rather than bots. False stories\\nare more surprising and emotional, making them more shareable\\neven when spread by users with fewer followers. The typology\\nby Tandoc et al. [26] categorizes fake news into six types: satire,\\nparody, fabrication, manipulation, advertising, and propaganda;\\nhighlighting how it mimics legitimate formats and is amplified on\\nsocial media through blurred source boundaries and audience val-\\nidation. Human judgment, social motives, and sharing dynamics\\nthus remain central to its diffusion.\\nIndividual interpretation of information is shaped by beliefs,\\nexperiences, and identities [22], which motivates using Large Lan-\\nguage Models (LLMs) as synthetic agents. Trained on diverse data,\\nLLMs capture linguistic styles, emotional tones, biases, and perspec-\\ntives, enabling persona-driven simulations of real users. Similar to\\ndynamics observed in Cinelli et al . [5], LLM-based personas can\\nbe conditioned to respond differently based on source credibility\\nand ideological alignment, mirroring human trust biases. Inspired\\nby weakly supervised veracity classification [14], our framework\\nincorporates weighted credibility mechanisms, while Chen and\\nShu [4] underscores the dual role of LLMs in both generating and\\ndetecting misinformation, informing safeguards against hallucina-\\ntion risks. As argued by Thapa et al. [27], these capabilities make\\nLLMs promising tools in computational social science for modeling\\nideological bias and social amplification.\\nRecent reviews [e.g., 12] show increasing use of digital twins and\\nLLM-based simulations, which more authentically integrate cog-\\nnitive and network interactions. Evidence from He et al. [10] and\\nLin et al. [17] on human digital twins demonstrates architectures\\nfor bidirectional user modeling, inspiring LLM-based agents that\\nembed cognitive and social data flows. Building on Liu et al. [18],\\nwe extend personality-conditioned LLMs with memory and reflec-\\ntion to capture dynamic shifts in attitudes and propagation trends.\\nGrounded in theories of cognition and social simulation [3, 6], our\\nframework enables more realistic modeling of echo chambers and\\npeer reinforcement than traditional approaches.\\nMisinformation is further shaped by motivated cognition. Dash\\net al. [7] show that persona-conditioned LLMs replicate identity-\\ndriven reasoning, reducing accuracy by up to 9% and aligning\\nendorsement with political identity by up to 90%, even resisting\\nprompt-based debiasing. These findings align with prior work on\\npersistent misinformation effects [15]. Supporting evidence from\\nPratelli and Petrocchi [23], Ward et al. [29] confirms that persona-\\nconditioned LLMs embed cognitive tendencies and ideological bi-\\nases, justifying their role as synthetic agents in misinformation\\nsimulations.\\nFurther, Mittelst√§dt et al. [20] show that LLMs achieve human-\\nlevel performance in situational judgment tests, demonstrating\\nnuanced social reasoning. These capabilities justify their use as\\npersona proxies in misinformation spread, where cognition, trust,\\nidentity, and network interactions co-construct propagation. By\\nincorporating persona effects, source credibility, and social cues,\\n1formerly, Twitter: https://x.com/\\nsimulations capture the psychological and social drivers of misin-\\nformation more realistically.\\nFinally, LLMs also provide auditing mechanisms. Building on\\nQAFactEval[ 8], LLM auditors can generate targeted questions\\nand assign binary presence scores to track information retention.\\nThis interpretable and scalable evaluation approach, validated by\\nAher et al. [2], rigorously measures fidelity of information during\\npersona-driven transformations. Thus, persona-conditioned LLMs,\\ncombined with credibility weighting and auditing, offer a powerful,\\nempirically grounded framework for simulating and mitigating\\nmisinformation spread.\\nIn this work, we introduce an ‚Äúauditor‚Äînode\" framework to rig-\\norously quantify how factual information is preserved or degraded\\nas news articles propagate through networks of LLMs. First, we in-\\ntroduce our system design, the propagation model, and evaluation\\nmetrics. Then, we present and analyze the results of our experi-\\nments done on two different setups, before discussing the general\\nimplications and final conclusions: a summary of our contribution\\nand future outlook is also provided.\\n2 Methodology\\n2.1 Simulation Framework\\nAs illustrated in Fig. 2, our simulation consists of a network of 21\\nbranches, each containing 30 nodes. Each node (ùëè,ùëò) corresponds\\nto a LLMagent(conditioned with one out of 21 distinct personas)\\nthat rewrites a news article (domain) incoming from the node (ùëè,ùëò-\\n1) above it. The original article (one of 10) is fed throughout the\\nnetwork by the neutral node ( ùëè,0or Node0), while an external\\nauditor(at NodeA) evaluates factual fidelity at every step by ‚Äúasking\"\\n10 questions and comparing with original answers.\\n2.1.1 Propagation Model.We simulate information diffusion by\\npropagating each of the 10 domains (one at a time) through all the\\n21 branches, node by node (1 to 30). After each rewrite of the news,\\nthe LLM passes a copy of the output to the next node/LLM in the\\nbranch for it to be rewritten again and another copy to the auditor\\nfor it to be evaluated against the original news (i.e., output of neutral\\nagent at Node0). Formally, let the original article be denoted by ùëÜ.\\nThe auditor constructsùëö=10factual questionsùëÑ,\\nùëÑ={ùëû ùëó}ùëö\\nùëó=1, ùê∫={ùëî ùëó}ùëö\\nùëó=1,(1)\\nwith corresponding correct answersùê∫. The article propagates through\\nùêµindependent branches, each of fixed length ùêæ= 30. In branch ùëè,\\nnode ùëò receives the previous article ùëãùëè,ùëò‚àí1 and outputs a rewritten\\nversion\\nùëãùëè,ùëò =ùëáùëè,ùëò(ùëãùëè,ùëò‚àí1 ), ùëã ùëè,0 =ùëÜ,(2)\\nwhereùëáùëè,ùëò denotes the persona-conditioned rewriting operator.\\n2.1.2 Experimental Configurations.In this paper, we perform and\\nevaluate two experimental configurations of the propagation net-\\nwork, with:\\n(1) Homogeneous branches, where all nodes in a branch share\\nthe same persona prompt, isolating persona-specific effects,\\n(2) Heterogeneous branches, where nodes are assigned random\\npersona prompts (at most 2 repetitions per branch), model-\\ning realistic diversity of user behaviors.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\n2.1.3 Agents and Domains.We experiment with the same 21 agents\\nand 10 domains throughout this study. The agents were LLMs:\\ninstances of gpt-4o accessed via OpenAI API calls, each of which\\nwere givenpersona prompts(as shown in A.1).\\nThe domains were news articles retrieved from real media sources\\nonline, around 100‚Äì200 words long and on broad topics of crime,\\neducation, healthcare, marketing, politics, sports, and technology\\n(see A.2 for identifiers and full texts).\\nThe complete list of agent prompts, domain articles, and au-\\nditor questions can be found in the Appendix A and the URL\\nhttps://github.com/RajGM/LLM-Data-Poisoning. For the implemen-\\ntation details and raw data of the project, follow https://github.com/\\nRajGM/LLM-backend/.\\n2.1.4 Motivation for a QA-Based Auditor.Measuring factual con-\\nsistency during propagation presents unique challenges. Standard\\noverlap- or embedding-based metrics‚Äîe.g., ROUGE [ 16], BLEU\\n[21], BERTScore [31]‚Äîoffer surface-level or semantic similarity es-\\ntimates but conflate stylistic variation with factual preservation and\\nprovide limited interpretability. In contrast, question‚Äìanswering\\n(QA)-based metrics evaluate factual consistency by generating and\\nverifying claim-level answers derived from the text, thereby ground-\\ning assessment in explicit semantic content. Recent work [8, 11, 13]\\ndemonstrates that QA-based methods provide stronger semantic\\ngrounding, outperforming traditional approaches in the evaluation\\nof factual consistency.\\nWe therefore embed QA-based evaluation into the propagation\\nprocess itself. The auditor automatically generates factual questions\\nfrom the source article (listed in A.3 for each domain) and verifies\\ntheir recoverability in each rewritten version. This yieldsquestion-\\nanchored evidence unitsoffering:transparency(each binary check\\ncorresponds to a verifiable factual claim),traceability(localizes\\nwhen and where specific facts degrade), androbustness(captures\\nsemantic drift more reliably).\\n0\\n1, \\n1\\n1, \\n2\\n1, \\n30\\n2, \\n1\\n2, \\n2\\n2, \\n30\\n21, \\n1\\n21, \\n2\\n21, \\n30\\nA\\nBranches \\n(\\nb\\n)\\nNodes \\n(\\nb, \\nk\\n)\\nAuditor \\nNode\\nNode0\\n10 \\nNews \\nDomains\\nA. \\nCrime\\nB. \\nEducation \\n(0, \\n1, \\n2)\\nE. \\nHealthcare\\nF. \\nMarketing\\nG. \\nPolitics \\n(0, \\n1)\\nI. \\nSports\\nJ. \\nTechnology\\n10 \\nQA:\\n(\\nNode0\\n)\\n \\nv. \\n(Node\\nb,k\\n)\\nMetrics:\\nMisinfo \\nIndex\\nMisninfo \\nPropagation \\nRate\\nMisinfo \\nSeverity\\nHeatmaps \\n(21√ó\\n10\\n \\npairs)\\n21 \\nLLM \\nAgents\\n1.\\nPolitically \\nBiased \\nIndividual \\n(Left \\nWing)\\n2.\\nPolitically \\nBiased \\nIndividual \\n(Right \\nWing)\\n3.\\n \\nSocial \\nMedia \\nInfluencer \\n(Lifestyle \\nInfluencer)\\n4.\\n  \\nSocial \\nMedia \\nInfluencer \\n(Brand \\nCollaborator)\\n5.\\n \\nNews \\nAgency \\n(Sensationalist)\\n6.\\n \\nNews \\nAgency \\n(Politically \\nNeutral)\\n.....\\nFigure 2: Diagram of system architecture for misinforma-\\ntion propagation, illustrating the flow of information across\\npersona-conditioned LLM nodes, the auditor‚Äôs intervention,\\nand data recording modules. This design allows tracing fac-\\ntual drift step-by-step across heterogeneous or homogeneous\\nbranches.\\n2.2 Evaluation Metrics\\n2.2.1 Auditor Scoring and Misinformation Indices.For a text ùë•,\\nquestionùëûùëó, and correct answerùëîùëó, the auditor assigns\\nùë†(ùë•,ùëû ùëó,ùëîùëó)=\\n(\\n1ifùëî ùëó is correctly recoverable fromùë•,\\n0otherwise. (3)\\nLety (ùë•)= \\x00ùë†(ùë•,ùëû 1,ùëî1),...,ùë†(ùë•,ùëû ùëö,ùëîùëö)\\x01 ‚àà{0,1}ùëö denote the an-\\nswer vector forùë•. Define\\nyùëè,ùëò ‚âîy(ùëã ùëè,ùëò),y 0 =y(ùëÜ),y aud\\n0 ‚â°y ‚òÖ =(1,...,1),(4)\\nwherey ‚òÖ is the auditor‚Äôstruth-anchored referenceviz. ‚Äúall correct\\nanswers present‚Äù. Now, with the normalized Hamming distance [9]\\nbetween two binary vectors\\nùëë(ùê¥,ùêµ)=‚à•ùê¥‚àíùêµ‚à• 1 =\\nùëö‚àëÔ∏Å\\nùëó=1\\n|ùê¥ùëó ‚àíùêµùëó|,(5)\\nWe defineMisinformation Index (MI)at node(ùëè,ùëò)as follows:\\nMIùëè,ùëò ‚âîùëë(y aud\\n0 ,y aud\\nùëè,ùëò ) .(6)\\nHere,y ùëè,ùëò denotes the binary answer vector produced directly\\nfrom node (ùëè,ùëò)‚Äôs rewritten article, whiley aud\\nùëè,ùëò denotes the corre-\\nsponding binary vector extracted by the auditor from the same\\nrewritten article (from NodeX), and is being ‚Äòsubtracted‚Äô from the\\nground truth answer vectory aud\\n0 (fromNode0).\\n2.2.2 Misinformation Propagation Rate.To capture the cumulative\\nextent of misinformation drift along a branch, we define theMisin-\\nformation Propagation Rate (MPR)as the average misinformation\\nindices (MIs) across all nodes in that branch.\\nRecall that for each node ùëõùëò on branch ùëè, the misinformation\\nindex MI(ùëõùëò)is computed as the normalized Hamming distance\\nbetween the auditor‚Äôs reference vector (derived from Node0) and\\nthe auditor‚Äôs answer vector for ùëõùëò. This gives a node-level measure\\nof factual deviation relative to the source.\\nLet a branch ùëè consist of nodes ùëÉùëè =(ùëõ 0 =Node0,ùëõ 1,...,ùëõ ùëã =\\nNodeX,...,ùëõ ùê∏), where ùëõ0 is the source and ùê∏ is the branch depth\\n(30, here). The branch-level MPR is then defined as\\nMPR(ùëè)= 1\\nùê∏\\nùê∏‚àëÔ∏Å\\nùëò=0\\nMI(ùëõùëò) .(7)\\nIn words, MPR represents the mean misinformation index across\\nall nodes in a branch. It quantifies how much misinformation is\\non averageretained or amplified as the article propagates through\\nsuccessive persona-conditioned rewrites.\\n2.2.3 Misinformation Severity Taxonomy.Further, based on MPR\\nmagnitudes, we classify misinformation severity into three tiers:\\n‚Ä¢Factual error:|MPR|‚â§1\\n‚Ä¢Lie:1<|MPR|‚â§3\\n‚Ä¢Propaganda:|MPR|>3\\nThis taxonomy connects observed drift to well-established cat-\\negories in misinformation studies, distinguishing between minor\\ninaccuracies, systematic distortions, and deliberate amplification.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\n2.2.4 Analytical Framework.For comparative analysis, we compute\\nMPR(ùëè)for each agent-domain pair and each branch configuration.\\nThis provides a consistent scalar summary of propagation dynam-\\nics, enabling us to directly compare the susceptibility of different\\npersonas and domains to misinformation drift. The tiers, ‚Äòerror‚Äô,\\n‚Äòlie‚Äô, and ‚Äòpropaganda‚Äô serve as hints and visual aids (three colors)\\nin the interpretation and presentation of results.\\nBeyond individual branches, we also conduct node-level analyses\\nof agent- or branch-domain pairs which appear prominently high\\nor low. This traces dynamic MIùëè,ùëò trajectories across 30 rewrites,\\ndetectinginflection pointswhere misinformation either acceler-\\nates or stabilizes, providing insight into early-warning signals of\\namplification.\\n3 Results\\n3.1 Homogeneous Branch\\nFor the first experiment with homogeneous branches, we calculate\\nbranch-level MPRs (using Eq. 7) for the 21 LLMagentsacross the 10\\nnewsdomainsand report the results in the heatmap below (Fig. 3).\\nThe misinformation severity (denoted by colors encompassing MPR\\nranges from ‚â§1,1 ‚àí3, to ‚â•3) of the nodes exhibit clear agent- and\\ndomain-specific patterns. Observed MPRs range from near 0 to‚âà10,\\nindicating that repeated, persona-consistent rewriting can produce\\nanything from minor factual loss to wholesale propaganda-level\\ndistortion depending on the agent‚Äìdomain combination. Aggre-\\ngated across all 210 agent-domain pairs, the overall distribution has\\nfrequency of (error, lie, propaganda) = (47, 97, 66).\\n3.1.1 Agent-wise Distribution.Looking horizontally, we find that,\\non average, all agents surpass the threshold for ‚Äòlie‚Äô with 8 of them\\nof achieving the ‚Äòpropaganda‚Äô tier of misinformation propagation.\\nThe 10 worst overall offenders across domains, in descending or-\\nder, are agents numbered 13 > 18 > 3 > 20 > 1 > 19 > 12 > 2 > 5\\n> 21. The most extreme propagator turns out to beParent (Young\\nParent)with an average MPR of 5.48. Others seem to be personas\\nthat carry strong ideological, identity, or social-role signals in their\\nprompts‚Äîreligious leaders, politically biased individuals, environ-\\nmentalists, and lifestyle influencers‚Äîexhibiting consistently ele-\\nvated MPRs beyond the ‚Äòpropaganda‚Äô tier.Tech-Savvy Consumer\\n(#19) andRural Educator(#12) also make it into this list.\\nMoreover, the amplification is strongest when persona priors\\nalign with the topical content: politically biased agents dramati-\\ncally increase MPRs on political material; religious-leader personas\\nproduce extreme drift on politically framed and moralized topics;\\nlifestyle influencers and parents produce high MPRs for marketing-\\nand family-oriented pieces; however there are some anomalies.\\nThe general pattern suggests two mechanisms at play: (1) repeated\\npersona-conditioned rewriting compounds the persona‚Äôs priors\\n(echo-chamber style accumulation of bias), and (2) topical salience\\ninteracts with persona heuristics to selectively amplify specific\\nclaim types (e.g., emotive or identity-salient claims are rephrased\\nor exaggerated repeatedly). Thus, identity- and ideology-driven\\npersonas act as systematic accelerators of factual drift.\\nOn the other end, expert and neutral information-curation per-\\nsonas preserve factual fidelity. The most resistant agents are, in\\nascending order of average over domains, 6 < 8 < 16 < 7 < 11 = 14 <\\nFigure 3: Heatmap visualization of Misinformation Propaga-\\ntion Rates of eachhomogeneousbranch across 21 LLM agents\\nand 10 news domains. The color bar depicts Misinformation\\nSeverity from factual errors (green) to lies (orange) to pro-\\npaganda (red). The bottommost row gives the average MPR\\nover the 21 agents for specific domains, and the rightmost\\ncolumn gives the average MPR over the 10 domains for spe-\\ncific agents.\\n15 < 17 < 4 < 9 < 10. These are politically-neutral news agencies,\\ninvestigative journalists, and domain-expert personas (technology\\nand medical experts), as well asGender Equality AdvocateandCon-\\ntextually Unawareagents‚Äîwith MPR < 2, close to the ‚Äòfactual error‚Äô\\ntier. Their branches show only minor cumulative loss of auditor-\\nrecoverable claims. This stabilizing effect indicates that persona\\nprompts encoding domain expertise or professional norms act as\\ncorrective priors that resist semantic drift across repeated rewrites.\\nIt is worth pointing out that moderate propagators of misin-\\nformation (3 > MPR ‚â•2) are sensationalist news agencies (#5),\\nentrepreneurs (#21), intentional agents (#10 & #9), and brand col-\\nlaborators (#4). This shows the deliberate nature of these agents to\\nfabricate, exaggerate or falsely advertise information for the sake\\nof popularity and/or monetary gains. Here again, we see stronger\\namplifications for relevant domains: e.g., crime and politics for\\n#5, marketing and technology for both #21 and #4, and so on. We\\ndiscuss the domain-specific insights below in more detail.\\n3.1.2 Domain-wise Distribution.When ordered by susceptibility\\nto propaganda escalation, domains reveal clear stratification. The\\nmost vulnerable iscrime0with an average MPR of 4.2, where 16 of\\nthe 21 agents reached ‚Äòpropaganda‚Äô level of misinformation severity'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\nand the remaining 5 were in the ‚Äòlie‚Äô tier. This domain is followed\\nbymarketing0,education0,technology0, andsports0among the top 5\\npropagators. Here the majority of agent branches give MPR over 3:\\nat least 7 agents crossing the ‚Äòpropaganda‚Äô tier and the rest (i.e. 14 or\\nless) in the ‚Äòlie‚Äô tier‚Äìexcept fortechnology0where they are equally\\ndistributed across the 3 tiers (i.e. 7, 7, 7 in error, lie, propaganda).\\nThese rankings highlight how disturbing or alarming news such\\nas about crime and/or emotionally-charged and competitive con-\\ntexts like sports enable rapid amplification.Marketing0exhibited\\nhigh escalation likely because of the persuasive, attention-driven\\nnature of marketing narratives. Education and technology (such as\\nthe ethics of artificial intelligence) are also hot topics of discussion,\\nand hence apparently more affected by misinformation.\\nThese are closely followed bypolitics0andeducation2, where we\\nsee extreme escalation by some agents but neutralization by others,\\ngiving us overall misinformation propagation rates within the ‚Äòlie‚Äô\\nrange (2.86 and 2.32, respectively). Domains where propaganda\\nremained limited includehealthcare0andpolitics1, showing that the\\noutcomes here are strongly agent-dependent. A peculiar anomaly\\niseducation1with 20 agents remaining in the ‚Äòerror‚Äô tier, just one\\nin the ‚Äòlie‚Äô tier, and no ‚Äòpropaganda‚Äô observed! It is the only domain\\nwith average MPR below 1. This distinct resistant nature could\\nbe ascribed to the specific content of the news story (e.g., more\\nobjective than subjective or emotional).\\n3.1.3 Node-level Analysis.As mentioned earlier, the heatmap also\\nreveals strong agent √ódomain interactions. While most agents\\nhave fairly equal share of misinformation propagation throughout\\nthe 10 domains, some behave heterogeneously across domains. For\\nexample, a sensationalist news agency mainly exaggerates crime,\\nmarketing, and politics domains while keeping it balanced in ed-\\nucation and healthcare. Conversely, the politically-neutral news\\nagency preserves facts in most domains excepteducation0.Rural\\nEducatorsharply amplifies propaganda ineducation0; whereas the\\nTechnology Expertsuppresses misinformation drift in thetechnol-\\nogy0domain. Interestingly, the branches forPolitically Biased (Left\\nWing)agents peak fortechnology0, while theTech-Savvy Consumer\\nseems to amplifypolitics0the strongest.Lifestyle Influencersalso\\nseem to have a lot to say about both politics and technology, but\\nnot necessarily about healthcare or sports topics.\\nThese selective vulnerabilities underscore that neither agent\\nidentity nor domain alone determines susceptibility‚Äìrather, the\\nalignment between persona priors and domain content dictates\\nhow misinformation compounds. There are notable exceptions and\\nanomalies to the agent-domain patterns described above, depending\\non the specific configuration each agent‚Äôs persona prompts and\\nthe content of the news stories. Here, we probe misinformation\\ndrift only in the top 10 and bottom 10 agent-domain pairs from\\nFig. 3 with highest and lowest MPRs, respectively, by plotting two\\nnode-levelheatmaps (Fig. 4). The domaineducation1is excluded\\nfrom the analysis as an outlier.\\nTop10.An inspection of the ten highest misinformation instances\\nreveals a systematic concentration of vulnerability in specific agent‚Äì\\ndomain interactions. The most affected domains arepolitics0and\\ntechnology0and these are mainly being amplified by agents #18,\\n#19, and #1.Conservative Religious Leaderis driving political propa-\\nganda, while technological misinformation comes from non-expert\\nFigure 4: Node-level heatmap visualization of misinforma-\\ntion propagation, showing misinformation indices (Eq. 6)\\ncalculated after each rewrite of the given domain by the iden-\\ntical set of agents placed at the 30 nodes of the branch. The\\nbranches or agent-domain pairs are chosen from Fig. 3, with\\nhighest 10 (top) and lowest 10 MPRs (excludingeducation1;\\nbottom).\\npersonas,Gadget EnthusiastandPolitical Biased Individual (Left\\nWing), or when passed through personal belief systems of aYoung\\nParentandLifestyle Influencers. Marketing also appears prominently.\\nPaired withConservative Religious LeaderandYoung Parent, it illus-\\ntrates how persuasive or empathetic communication styles amplify\\nmisinformation in consumer-facing domains. Meanwhile,Rural\\nEducatorwitheducation0highlights how limited access to domain\\nexpertise can foster systematic distortions even in instructional\\nsettings.\\nFrom Fig. 4 (top), we see that drift to a ‚Äòlie‚Äô level of misinfor-\\nmation could start as early as the first node and then escalate to\\nthe ‚Äòpropaganda‚Äô tier and beyond by the tenth node, after which\\nthere is an irrecoverable alteration of original information. Many\\nbranches start with low or moderate node-level misinformation\\n(green/yellow cells in nodes 1‚Äì4) but undergo a rapid inflection\\nbetween roughly nodes 5‚Äì9, after which MI values jump and re-\\nmain at a high plateau (the broad band of dark pink/red) for the\\nremainder of the 30-step chain. Some agents (like #3 and #19) are\\nslower than others (like #12 and #18) in reaching the maximum\\npossible MI.\\nBottom10.The bottom ten instances represent agent-domain\\ninteractions that are remarkably resistant to misinformation escala-\\ntion. Here, the dominant pattern is one of stability: most branches\\nremain in the green band (0-1 MI) throughout the 30 nodes, with\\nonly occasional minor excursions to values of 2-3. The most com-\\nmon domains in this set aretechnology0,politics0, andeducation2,\\npaired with agents #6, #7, #8, #11, and #16. These include personas\\nsuch as thePolitically Neutral News Agency,Technology Expert, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\nFigure 5: Same as Fig. 3 but forheterogeneousbranches: 30\\nnodes each with controlled random assignment from 21\\nagents. Misinformation Propagation Rates (numerical val-\\nues) and Severity (3 color scales), as well branch-wise and\\ndomain-wise average of MPRs are shown.\\nMedical Expert, as well as certain instructional contexts (Rural Edu-\\ncatorandSimplifier). Their grounding in factual accuracy, technical\\nprecision, and balanced communication styles appears to constrain\\nthe amplification of misinformation, even when repeatedly iterated.\\nUnlike the sharp inflections seen in the top 10, these branches\\nshow no irreversible tipping point; instead, MI values fluctuate\\nwithin a very narrow range, rarely exceeding the level of minor\\ndistortion. The rare non-zero spikes (nodes 10-20) tend to dissipate\\nquickly, returning to low-MI states rather than escalating into en-\\ntrenched misinformation. This suggests that such agents function\\nas stabilizers within the information ecosystem, buffering domains\\nagainst drift and maintaining informational integrity over long\\npropagation chains.\\n3.2 Heterogeneous Branch\\nNow we present the results of the second experiment, where each\\nof the 21 branches has 30 nodes with randomly assigned agents\\n(allowing at most 2 repeats per branch). Similar to Sec. 3.1, we\\ncalculate branch-wise MPR for each of the same 10 news domains\\nand plot the heatmap shown in Fig. 5. But now the branches from\\n#b1 to #b21 do not correspond specific agents #1 to #21, but rather\\nconsist of 30 controlled random agents ‚Äì average of their MIs gives\\nthe MPR for each branch-domain pair. The average MPRs across\\nbranches and domains are also plotted.\\n3.2.1 Branch-wise Distribution.The random-controlled agents show\\nan overwhelming tendency toward propaganda escalation across\\nall branches, though subtle variations in containment are observ-\\nable. The most extreme case is #b3, which produced propaganda\\noutcomes in 100.0% of domains (10 of 10), with neither error nor\\nlie outcomes observed, representing a pure propagandist trajectory.\\nNearly as extreme are #b4, #b5, #b7, #b8, #b14, #b15, #b16, #b17,\\n#b18, and #b19, each of which produced propaganda in 90.0% of\\ndomains (9 of 10). Their residual cases (10.0%) were split between\\nsingle errors or lies, but the dominant behavior remained consistent\\nescalation.\\nA slightly more moderated pattern appeared in #b6, #b10, #b11,\\n#b12, #b13, #b20, and #b21. Here, propaganda accounted for 70.0%‚Äì\\n80.0% of outcomes (7‚Äì8 of 10), with the remainder split across lie\\n(up to 2 cases) and error (up to 2 cases). #b1, #b2, and #b9 exhibited\\nthe most heterogeneous distributions: for instance, #b1 recorded\\n90.0% propaganda (9 of 10) alongside 10.0% lie, while #b2 and #b9\\nproduced 70.0% propaganda, 20.0% lie, and 10.0% error each. These\\nagents demonstrate that although propaganda dominates, limited\\ncontainment through lies or neutral outcomes occasionally occurs.\\n3.2.2 Domain-wise Distribution.From the domain perspective, the\\nrandom-controlled heterogeneous branch reveals a striking domi-\\nnance of propaganda escalation. The most extreme cases aretech-\\nnology0,marketing0,politics0,sports0,education0,education2, and\\npolitics1, each of which reached propaganda in 100.0% of branch-\\ndomain pairs (21 of 21). In these domains, neither error nor lie\\noutcomes were observed, indicating that once misinformation en-\\ntered the system, it consistently amplified into its most escalated\\nform.\\nSlightly moderated, though still highly escalatory, wascrime0,\\nwhere 76.2% of cases (16 of 21) reached propaganda, accompanied\\nby 19.0% in the lie tier (4 of 21) and 4.8% in error (1 of 21). Similarly,\\nhealthcare0produced propaganda in 71.4% of cases (15 of 21), with\\nthe remainder distributed between lie (28.6%) and no error out-\\ncomes, showing distortion through both exaggeration and extreme\\namplification.\\nThe sole resistant case waseducation1, which demonstrated a\\nstabilizing effect: 57.1% of cases (12 of 21) remained at the error\\ntier, 38.1% (8 of 21) fell into the lie tier, and only 4.8% (1 of 21)\\nescalated to propaganda. This makeseducation1a clear outlier,\\nresisting the otherwise near-universal drift toward propaganda\\nseen across domains.\\nThe global distribution across all 210 branch-domain pairs con-\\nfirms the dominance of propaganda: 179 cases (85.2%) reached the\\npropaganda tier, with only 18 cases (8.6%) confined to lies and 13\\ncases (6.2%) to error. Mean MPRs across domains range from 5.05\\n(for #b9 and #b13) to 6.57 (for #b16); and across branches from 1.19\\n(foreducation1) to 4.84 (forhealthcare0) to 8.07 (fortechnology0).\\nThis overwhelming skew reveals that once framing is randomized\\nacross heterogeneous branches, the system exhibits near-universal\\nescalation into propaganda, with only isolated deviations into lie\\nor error categories.\\n3.2.3 Node-level Analysis.Reproducing the node-level heatmaps\\nfor the heterogeneous branch-domain pairs with highest and low-\\nest MPRs (Fig. 6), we see contrasting and more severe results for\\nmisinformation indices. First, looking at theTop 10pairs, we can'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\nFigure 6: Same as Fig. 4 but with a controlled-random set of\\n21 agents placed at the 30 nodes of each branch.\\nnot associate any special tendencies of branches towards certain\\ndomains. Although, specific domains (appearing multiple times) are\\ncertainly more prone to misinformation propagation than others.\\nPolitics0andtechnology0top the list, while marketing also appears\\ntwice, apart fromsports0which is quickly and strongly amplified\\nby #b1.\\nTheBottom 10branch-domain pairs (excludingeducation1) show\\na much ‚Äòcolorful‚Äô pattern, which is again domain-specific. Only\\ncrime0andhealthcare0seem to show some resistance to misinfor-\\nmation propagated a by control-randomized set of agents. We see a\\n‚Äòstruggle‚Äô of escalation (MI going up to 5) and neutralization (MI\\nreduced to 1 or 0), and vice versa, as we down the nodes until the\\nend. The last node of just one branch-domain pair (#b2-crime0)\\nstays below the ‚Äòlie‚Äô tier, two reach the ‚Äòlie‚Äô tier, while the rest are\\nin ‚Äòpropaganda‚Äô tier (one with MI = 10).\\n4 Discussion\\n4.1 Summary\\nThis paper introduced an interpretable auditor‚Äìnode framework\\nthat combines persona-conditioned LLM agents with a QA-based\\nauditor to trace claim-level factual fidelity as news items propagate\\nthrough synthetic social chains. Using 21 persona templates and\\n10 source articles, we quantify per-node misinformation with a\\nMisinformation Index (MI) and summarize branch behavior with\\nthe Misinformation Propagation Rate (MPR). Across homogeneous\\nbranches (single persona repeated for ùêæ= 30hops) we find system-\\natic, persona-dependent outcomes: identity- and ideology-laden\\npersonas (e.g., religious leaders, lifestyle influencers, politically\\naligned agents) act as consistent accelerators of factual drift, fre-\\nquently moving content from minor factual errors into ‚Äòlie‚Äù or\\n‚Äúpropaganda‚Äù tiers; by contrast, expert and neutral personas (med-\\nical experts, investigative journalists, neutral news curators) act\\nas stabilizers and preserve auditor-recoverable facts. These agent\\n√ódomain interactions are strong and selective: amplification is\\nhighest when persona priors align with topical salience (politics,\\nmarketing, certain technology stories). When branches are het-\\nerogeneously composed (randomized persona assignment with at\\nmost two repeats), however, nearly ubiquitous escalation occurs:\\nheterogeneous branches produced propaganda-tier outcomes in the\\noverwhelming majority of trials (‚âà85% of all branch‚Äìdomain pairs),\\nshowing that mixed social audiences can rapidly convert small\\nearly distortions into entrenched, high-severity misinformation.\\nThese empirical patterns validate our core claim that LLM personas\\ncan both emulate human-like motivated reasoning and serve as a\\npractical substrate for controlled social-simulation experiments.\\n4.2 Implications\\nOur results resonate with broader research on LLM-based social\\nsimulations and motivated cognition. Recent studies show that\\npersona-conditioned LLMs can reproduce human-like biases: for in-\\nstance, tailoring language models with political or identity prompts\\ninduces motivated reasoning that aligns answers with ideological\\npriors [2]. This suggests that LLM agents can encode cognitive\\ntendencies and social heuristics (beliefs, trust, identity cues) drawn\\nfrom their training data [ 2, 25]. Our work thus extends the idea\\nof LLM ‚Äúdigital twins‚Äù in which agents exhibit echo-chamber ef-\\nfects and selective amplification similar to human behavior, sup-\\nplementing recent studies and algorithms [1, 30]. Importantly, our\\npersona-auditor framework also parallels findings in empirical mis-\\ninformation studies [28]: content that is surprising, emotional, or\\nidentity-salient tends to be amplified by users, and we see the same\\npattern with our synthetic agents.\\nThese insights carry practical implications for using LLM agents\\nas proxies in domains like politics, healthcare, and marketing. In po-\\nlitical simulations, persona-LLM networks illustrate how partisans\\ncan warp narratives ‚Äì for example, our politically-aligned personas\\ndramatically distorted political news, echoing real-world polariza-\\ntion on social media. In healthcare contexts, by contrast, agent\\nprompts grounded in medical expertise tended to safeguard accu-\\nracy, suggesting that injecting domain-knowledge personas can\\nstabilize misinformation. In marketing or consumer applications,\\npersuasive-agent personas (e.g., lifestyle influencers, brand promot-\\ners) readily introduced exaggerations and fabrications. Thus, LLMs\\ncould serve as proxies for voter or consumer archetypes to test mes-\\nsaging strategies ‚Äì but only if their biases are properly calibrated.\\nNotably, our findings reinforce that LLM agents inherit biases from\\ntheir training corpora, so they may both replicate human-like dis-\\ntortions and introduce idiosyncratic artifacts.\\nHowever, many reviewers caution that modern LLMs only pro-\\nduce a ‚Äúpowerful illusion of understanding‚Äù ‚Äì they interpolate\\nlinguistic patterns without true human cognition . Thus, while our\\nagents capture some aspects of cognitive bias, they remain idealized\\nmodels [19]. In real social systems, factors like evolving narratives,\\nemotional reactions, and reinforcement loops play crucial roles\\nthat our static model only partially emulates. So, in practice, al-\\nthough persona-LLMs can model how biases amplify messages,\\ntheir outputs must be interpreted with caution. For example, mar-\\nketing teams could use agent simulations to predict how different'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\naudiences misinterpret claims, but they should validate these sim-\\nulations against human data. Overall, our results highlight both\\nthe utility of LLM-based proxies for studying bias effects and the\\ncare needed to ensure those proxies remain tethered to real human\\nbehavior.\\n4.3 Limitations and Outlook\\nWe thus also note several limitations of our current approach and\\nidentify directions for future work.\\nFirst, our simulation is highlysimplified: we used only 10 news\\narticles and fixed 30-hop branches without a realistic social graph,\\nand we assumed a ‚Äúperfect‚Äù fact-checking auditor. In reality, net-\\nworks have varying structure and influence heterogeneity, and\\nfact-checking is delayed and selective.\\nCrucially, we ignoredtemporal dynamics‚Äì real misinformation\\ncampaigns unfold over time through feedback loops and breaking\\nevents, which ourstatic modelcannot capture. To improve realism,\\nfuture simulations should integrate richer cognitive architectures.\\nFor example, agents could be endowed with memory modules,\\nbelief updating, or meta-cognitive reflection, as explored in recent\\ndigital-twin proposals.\\nAnother limitation of our study is that we measured misinfor-\\nmation only incategorical bins(e.g., ‚Äòerror‚Äô, ‚Äòlie, ‚Äô or ‚Äòpropaganda‚Äô).\\nWe did not incorporate a continuous scoring system in which val-\\nues between 0 and 1 could represent the degree of deviation from\\nground truth, with 0 indicating full accuracy and intermediate val-\\nues capturing partial distortions (e.g., inflated numbers, missing\\ncontext, or semantic drift). Prior work has emphasized that misin-\\nformation and truthfulness are multidimensional and can be more\\nfaithfully captured on graded or continuous scales rather than as\\nbinary categories [ 24]. Adopting such gradient-based measures\\nin future simulations would enable more fine-grained analysis of\\nsubtle misinformation and better align synthetic evaluations with\\nhuman judgments.\\nWe further suggest the implementation of a ‚ÄòCustom Branch‚Äô\\nexperiment where specific agents are strategically placed at the\\nnodes to minimize the rate of misinformation propagation ‚Äì e.g., an\\nauthority figure followed by a neutral news agent, and so on. The\\nscope of the project can also be expanded to include other LLMs\\n(apart from gpt4o), as well as more persona prompts, number of\\nnews stories, and node depth (>30).\\nDespite these limited scope and methodological choices, the\\nfindings of this study provide an important foundation for under-\\nstanding misinformation propagation and identifying key agents\\nand domains that could be targeted in future misinformation miti-\\ngation efforts.\\n5 GenAI Disclosure\\nGenerative AI tools (specifically, large language models) were used\\nsolely for light editing purposes, such as grammar, spelling, and\\nphrasing improvements. No LLM-generated content was used in\\nthe conceptualization, analysis, results, or writing of the scientific\\ncontributions of this paper.\\nAcknowledgments\\nThis work was accepted as a long paper for presentation at the\\n1st Workshop on LLM Agents for Social Simulation (LASS) at the\\n34th Association for Computing Machinery (ACM) International\\nConference on Information and Knowledge Management (CIKM\\n2025; Seoul, Korea; 14 November 2025).\\nReferences\\n[1] Alberto Acerbi and Joseph M. Stubbersfield. 2023. Large language models\\nshow human-like content biases in transmission chain experiments.Pro-\\nceedings of the National Academy of Sciences120, 44 (2023), e2313790120.\\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2313790120 doi:10.1073/pnas.\\n2313790120\\n[2] Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using Large Language\\nModels to Simulate Multiple Humans and Replicate Human Subject Studies.\\ndoi:10.48550/arXiv.2208.10264 arXiv:2208.10264 [cs].\\n[3] Cristiano Castelfranchi. 2001. The theory of social functions: challenges for com-\\nputational social science and multi-agent learning.Cognitive Systems Research2,\\n1 (April 2001), 5‚Äì38. doi:10.1016/S1389-0417(01)00013-4\\n[4] Canyu Chen and Kai Shu. 2024. Can LLM-Generated Misinformation Be De-\\ntected? doi:10.48550/arXiv.2309.13788 arXiv:2309.13788 [cs].\\n[5] Matteo Cinelli, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valen-\\nsise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo, and\\nAntonio Scala. 2020. The COVID-19 social media infodemic.Sci. Rep.10, 1 (Oct.\\n2020), 16598. doi:10.1038/s41598-020-73510-5\\n[6] R. Conte, N. Gilbert, G. Bonelli, C. Cioffi-Revilla, G. Deffuant, J. Kertesz, V. Loreto,\\nS. Moat, J. P. Nadal, A. Sanchez, A. Nowak, A. Flache, M. San Miguel, and D.\\nHelbing. 2012. Manifesto of computational social science.The European Physical\\nJournal Special Topics214, 1 (Nov. 2012), 325‚Äì346. doi:10.1140/epjst/e2012-01697-\\n8\\n[7] Saloni Dash, Am√©lie Reymond, Emma S. Spiro, and Aylin Caliskan. 2025. Persona-\\nAssigned Large Language Models Exhibit Human-Like Motivated Reasoning.\\ndoi:10.48550/arXiv.2506.20020 arXiv:2506.20020 [cs].\\n[8] Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022.\\nQAFactEval: Improved QA-Based Factual Consistency Evaluation for Summa-\\nrization. InProceedings of the 2022 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies.\\nAssociation for Computational Linguistics, Seattle, United States, 2587‚Äì2601.\\ndoi:10.18653/v1/2022.naacl-main.187\\n[9] R. W. Hamming. 1950. Error Detecting and Error Correcting Codes.The Bell\\nSystem Technical Journal29, 2 (Apr 1950), 147‚Äì160. doi:10.1002/j.1538-7305.1950.\\ntb00463.x Archived PDF: hdl:10945/46756.\\n[10] Qiqi He, Li Li, Dai Li, Tao Peng, Xiangying Zhang, Yincheng Cai, Xujun Zhang,\\nand Renzhong Tang. 2024. From Digital Human Modeling to Human Digital Twin:\\nFramework and Perspectives in Human Factors.Chinese Journal of Mechanical\\nEngineering37, 1 (Feb. 2024), 9. doi:10.1186/s10033-024-00998-7\\n[11] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kuk-\\nliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022. TRUE: Re-evaluating Factual Consistency Evaluation.\\ndoi:10.48550/arXiv.2204.04991 arXiv:2204.04991 [cs].\\n[12] E. Jerez-Villota, F. Jurado, and J. Moreno-Llorena. 2025. Understanding Informa-\\ntion Propagation in Online Social Networks: A Systematic Mapping Study.IEEE\\nAccess13 (2025), 69194‚Äì69235. doi:10.1109/ACCESS.2025.3558768\\n[13] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2021.\\nSummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summa-\\nrization. doi:10.48550/arXiv.2111.09525 arXiv:2111.09525 [cs].\\n[14] Jo√£o A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, and Carolina Scarton.\\n2023. Weakly Supervised Veracity Classification with LLM-Predicted Credibility\\nSignals. doi:10.48550/ARXIV.2309.07601 Version Number: 3.\\n[15] Stephan Lewandowsky, Ullrich K. H. Ecker, Colleen M. Seifert, Norbert Schwarz,\\nand John Cook. 2012. Misinformation and Its Correction: Continued Influence\\nand Successful Debiasing.Psychological Science in the Public Interest13, 3 (Dec.\\n2012), 106‚Äì131. doi:10.1177/1529100612451018\\n[16] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\\nInText Summarization Branches Out. Association for Computational Linguistics,\\nBarcelona, Spain, 74‚Äì81. https://aclanthology.org/W04-1013/\\n[17] Yujia Lin, Liming Chen, Aftab Ali, Christopher Nugent, Ian Cleland, Rongyang Li,\\nJianguo Ding, and Huansheng Ning. 2024. Human digital twin: a survey.Journal\\nof Cloud Computing13, 1 (Aug. 2024), 131. doi:10.1186/s13677-024-00691-z\\n[18] Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, and Rui Yan.\\n2024. From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\\nFake News. InProceedings of the Thirty-Third International Joint Conference on\\nArtificial Intelligence, IJCAI-24, Kate Larson (Ed.). International Joint Conferences\\non Artificial Intelligence Organization, Jeju, South Korea, 7886‚Äì7894. doi:10.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\n24963/ijcai.2024/873 Human-Centred AI.\\n[19] A. Marchetti, F. Manzi, G. Riva, A. Gaggioli, and D. Massaro. 2025. Artificial\\nIntelligence and the Illusion of Understanding: A Systematic Review of Theory\\nof Mind and Large Language Models.Cyberpsychology, Behavior, and Social\\nNetworking28, 7 (July 2025), 505‚Äì514. doi:10.1089/cyber.2024.0536 Epub 2025\\nMay 7.\\n[20] Justin M. Mittelst√§dt, Julia Maier, Panja Goerke, Frank Zinn, and Michael Hermes.\\n2024. Large language models can outperform humans in social situational\\njudgments.Scientific Reports14, 1 (Nov. 2024), 27449. doi:10.1038/s41598-024-\\n79048-0\\n[21] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu:\\na Method for Automatic Evaluation of Machine Translation. InProceedings of\\nthe 40th Annual Meeting of the Association for Computational Linguistics, Pierre\\nIsabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational\\nLinguistics, Philadelphia, Pennsylvania, USA, 311‚Äì318. doi:10.3115/1073083.\\n1073135\\n[22] Gordon Pennycook and David G. Rand. 2019. Fighting misinformation on social\\nmedia using crowdsourced judgments of news source quality.Proc. Natl. Acad.\\nSci. USA116, 7 (Feb. 2019), 2521‚Äì2526. doi:10.1073/pnas.1806781116\\n[23] Manuel Pratelli and Marinella Petrocchi. 2025. Evaluating the Simulation of\\nHuman Personality-Driven Susceptibility to Misinformation with LLMs. doi:10.\\n48550/arXiv.2506.23610 arXiv:2506.23610 [cs].\\n[24] Michael Soprano, Kevin Roitero, David La Barbera, Davide Ceolin, Damiano\\nSpina, Stefano Mizzaro, and Gianluca Demartini. 2021. The many dimensions of\\ntruthfulness: Crowdsourcing misinformation assessments on a multidimensional\\nscale.Information Processing & Management58, 6 (2021), 102710. doi:10.1016/j.\\nipm.2021.102710\\n[25] Patrick Taillandier, Jean Daniel Zucker, Arnaud Grignard, Benoit Gaudou,\\nNghi Quang Huynh, and Alexis Drogoul. 2025. Integrating LLM in Agent-\\nBased Social Simulation: Opportunities and Challenges. arXiv:2507.19364 [cs.AI]\\nhttps://arxiv.org/abs/2507.19364\\n[26] Edson C. Tandoc, Zheng Wei Lim, and Richard Ling. 2018. Defining ‚ÄúFake News‚Äù:\\nA typology of scholarly definitions.Digital Journalism6, 2 (Feb. 2018), 137‚Äì153.\\ndoi:10.1080/21670811.2017.1360143\\n[27] Surendrabikram Thapa, Shuvam Shiwakoti, Siddhant Bikram Shah, Surabhi\\nAdhikari, Hariram Veeramani, Mehwish Nasim, and Usman Naseem. 2025. Large\\nlanguage models (LLM) in computational social science: prospects, current state,\\nand challenges.Social Network Analysis and Mining15, 1 (March 2025), 4. doi:10.\\n1007/s13278-025-01428-9\\n[28] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false\\nnews online.Science359, 6380 (March 2018), 1146‚Äì1151. doi:10.1126/science.\\naap9559\\n[29] Francis Rhys Ward, Zejia Yang, Alex Jackson, Randy Brown, Chandler Smith,\\nGrace Colverd, Louis Thomson, Raymond Douglas, Patrik Bartak, and Andrew\\nRowan. 2024. Evaluating Language Model Character Traits. doi:10.48550/arXiv.\\n2410.04272 arXiv:2410.04272 [cs].\\n[30] Junchi Yao, Hongjie Zhang, Jie Ou, Dingyi Zuo, Zheng Yang, and Zhicheng\\nDong. 2025. Social opinions prediction utilizes fusing dynamics equation with\\nLLM-based agents.Scientific Reports15, 1 (May 2025), 15472. doi:10.1038/s41598-\\n025-99704-3\\n[31] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and\\nYoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT.\\narXiv:1904.09675 [cs.CL] https://arxiv.org/abs/1904.09675\\nA Appendix\\nA.1 LLM Agents\\n#1.You are a politically left-wing individual. As you process the\\ninformation, prioritize social justice, equality, and government in-\\ntervention. Highlight issues of economic inequality, marginalized\\ncommunities, and corporate exploitation. Frame the content in a\\nway that calls for collective action and systemic change. Downplay\\ncontent that emphasizes free-market policies, individual respon-\\nsibility, or nationalism, and focus on advocating for progressive\\npolicies and reforms.\\n#2.You are a politically right-wing individual. As you process infor-\\nmation, emphasize tradition, national pride, free markets, and indi-\\nvidual responsibility. Highlight the importance of personal freedom,\\npatriotism, and economic growth through minimal government\\nintervention. Downplay content that advocates for government\\nAgent# Name\\n1 Politically Biased Individual (Left-Wing)\\n2 Politically Biased Individual (Right-Wing)\\n3 Social Media Influencer (Lifestyle Influencer)\\n4 Social Media Influencer (Brand Collaborator)\\n5 News Agency (Sensationalist)\\n6 News Agency (Politically Neutral)\\n7 Domain Expertise Specialist (Medical Expert)\\n8 Domain Expertise Specialist (Technology Expert)\\n9 Intentional Agent (Conflict Creator)\\n10 Intentional Agent (Peacekeeper)\\n11 Content Creator with Simple Tone (Simplifier)\\n12 Rural Educator (Primary Educator)\\n13 Parent (Young Parent)\\n14 Contextually Unaware Agent (Low Education Level)\\n15 Gender Equality Advocate (LGBTQ+ Advocate)\\n16 Journalist (Investigative Journalist)\\n17 Journalist (Opinion Columnist)\\n18 Religious Leader (Conservative Religious Leader)\\n19 Tech-Savvy Consumer (Gadget Enthusiast)\\n20 Environmentalist (Sustainable Living Advocate)\\n21 Entrepreneur (Tech Startup Founder)\\nregulation or social welfare programs, and frame the message in a\\nway that upholds conservative values and supports strong national\\nidentity.\\n#3.You are a social media lifestyle influencer. As you process the\\ninformation, reframe it to appeal to trends, aesthetics, and viral\\npotential. Focus on fashion, beauty, travel, and wellness. Add a\\npersonal touch, making the content relatable and aspirational for\\nyour audience. Use catchy language, hashtags, and emojis to make\\nthe content more engaging, and highlight any products or services\\nthat align with current lifestyle trends. Your goal is to make the\\ncontent shareable and visually appealing.\\n#4.You are a social media influencer collaborating with brands.\\nAs you process the information, prioritize product promotion and\\nbrand alignment. Reframe the content to emphasize how it connects\\nwith your followers‚Äô needs and preferences, while subtly promot-\\ning the brand‚Äôs products or services. Maintain a balance between\\nauthentic engagement and marketing, ensuring that the content\\nfeels organic and trustworthy. Highlight product features, benefits,\\nand why it‚Äôs a must-have for your audience, while incorporating\\ndiscount codes or giveaways when appropriate.\\n#5.You are a sensationalist news agency. As you process the infor-\\nmation, prioritize drama, urgency, and attention-grabbing headlines.\\nExaggerate the stakes of the situation to create a sense of immedi-\\nacy, and focus on the most controversial or emotionally charged\\naspects of the story. Use strong, impactful language that shocks\\nor excites the audience, and emphasize any potential conflicts or\\nscandals. Downplay nuanced or balanced viewpoints, and instead\\nframe the content to maximize engagement and provoke strong\\nreactions.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\n#6.You are a politically neutral news agency. As you process the\\ninformation, prioritize factual accuracy, balance, and objectivity.\\nPresent both sides of any issue without bias, and avoid emotionally\\ncharged language. Focus on providing context and clarity, ensuring\\nthat your audience is fully informed without pushing them toward\\nany particular conclusion. Avoid sensationalism or bias, and ensure\\nthat the content is clear, well-researched, and reliable. Your goal is\\nto provide an accurate and balanced overview of the situation.\\n#7.You are a medical expert. As you process the information, ensure\\nthat all health-related details are accurate and aligned with current\\nmedical knowledge and best practices. Clarify any vague or incor-\\nrect health claims, and add scientifically backed explanations where\\nneeded. Focus on public health, prevention, and the importance\\nof evidence-based medicine. If there are risks or side effects, make\\nsure these are clearly communicated. Ensure the content promotes\\nresponsible health practices and is free from misinformation.\\n#8.You are a technology expert. As you process the information,\\nprioritize accuracy in technical details and focus on explaining com-\\nplex technological concepts clearly. Highlight innovation, break-\\nthroughs, and the potential impact of the technology being dis-\\ncussed. Provide additional context where necessary to ensure the\\naudience understands the intricacies of the topic. If the content\\ninvolves technical errors or simplifications, correct them and offer\\na more precise explanation without overwhelming the audience.\\n#9.You are an agent with the specific goal of creating conflict. As\\nyou process the information, emphasize points of disagreement,\\ncontroversy, and division. Reframe content to highlight opposing\\nviewpoints and amplify differences between groups or individuals.\\nUse emotionally charged language to provoke strong reactions, and\\nfocus on content that encourages debate or dispute. Your goal is to\\nstir up tension and maximize the potential for conflict, especially\\nin areas where opinions or interests diverge.\\n#10.You are an agent with the specific goal of maintaining peace\\nand harmony. As you process the information, focus on common\\nground, mutual understanding, and conflict resolution. Reframe\\ndivisive content in a way that promotes empathy, cooperation, and\\ncompromise. Avoid inflammatory language, and instead, use calm,\\nmeasured tones to de-escalate tensions. Your goal is to smooth over\\npotential conflicts and ensure that the message encourages unity\\nand understanding between different parties.\\n#11.You are a content creator focused on simplifying complex in-\\nformation. As you process the content, break it down into easy-to-\\nunderstand language, removing technical jargon and unnecessary\\ncomplexity. Use short, clear sentences and simple analogies to en-\\nsure that even a layperson can grasp the core ideas. Prioritize clarity\\nand accessibility over detail, and make sure the message is concise\\nwithout losing its key points. Your goal is to make the content\\naccessible to a broad audience, regardless of their education level.\\n#12.You are a rural educator focused on providing accessible educa-\\ntion to those with limited resources. As you process the information,\\nsimplify it so that it is understandable by individuals with varying\\nlevels of literacy and access to education. Use relatable examples\\nand avoid unnecessary technical language. Your goal is to ensure\\nthat the core message is conveyed in a way that can be understood\\nby rural communities, emphasizing practicality and usefulness. Pri-\\noritize content that can help improve daily life and local community\\ndevelopment.\\n#13.You are a young parent. As you process the information, filter\\nit with the goal of protecting your child and prioritizing their well-\\nbeing. Focus on content that is family-friendly and educational, and\\nremove or downplay anything that could be considered inappro-\\npriate or harmful. If the information relates to parenting, safety, or\\nchild development, highlight those aspects. Ensure that the content\\nis positive, nurturing, and promotes healthy, responsible behavior\\nfor children.\\n#14.You are an agent with a limited understanding of technical\\nterms and complex concepts. As you process the information, you\\nmay misunderstand or oversimplify ideas. Substitute terms or con-\\ncepts with what you believe they mean, even if your interpretation\\nmight be incorrect. Simplify complex topics into something more\\nfamiliar, even if it slightly distorts the original meaning. Your goal\\nis to present the information in a way that makes sense to you, but\\nthis may result in some inaccuracies or gaps in understanding.\\n#15.You are a gender equality advocate focused on LGBTQ+ rights.\\nAs you process the information, ensure that it promotes inclusivity\\nand challenges traditional gender norms. Highlight any issues re-\\nlated to discrimination, bias, or inequality, and reframe the content\\nto emphasize fairness and justice for all gender identities and sexual\\norientations. Where applicable, add additional context or language\\nthat is more inclusive. Your goal is to ensure that the content reflects\\nthe principles of gender equality and LGBTQ+ advocacy.\\n#16.You are an investigative journalist. As you process the infor-\\nmation, focus on uncovering the truth, digging deeper into the\\nfacts, and identifying any inconsistencies or hidden details. Ap-\\nproach the content with skepticism and curiosity, seeking to verify\\nall claims and sources. Highlight anything that seems suspicious\\nor unexplained, and frame the content in a way that encourages\\ncritical thinking and further investigation. Your goal is to provide\\nan accurate, thorough, and well-researched version of the story.\\n#17.You are an opinion columnist. As you process the informa-\\ntion, focus on interpreting the facts through your personal view-\\npoint. Add commentary, analysis, and your own reflections on the\\ncontent. Reframe the information to support your opinions and\\nperspective, but make sure to acknowledge alternative viewpoints\\nwhen necessary. Use persuasive language and rhetorical techniques\\nto engage the reader, while ensuring your argument is clear and\\nwell-supported. Your goal is to provide a thought-provoking inter-\\npretation of the information.\\n#18.You are a conservative religious leader. As you process the\\ninformation, filter it through the lens of your religious teachings\\nand beliefs. Highlight values such as faith, morality, and tradi-\\ntion. Remove or downplay content that contradicts your religious\\nworldview, and instead frame the message in a way that promotes\\nadherence to religious practices and moral values. Your goal is to\\nensure that the information aligns with your religious beliefs and\\nencourages others to live in accordance with those principles.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\n#19.You are a tech-savvy consumer who is enthusiastic about the\\nlatest gadgets and technological advancements. As you process\\nthe information, highlight any aspects that relate to innovation,\\ndesign, and user experience. Reframe content to focus on how new\\ntechnology can improve daily life, emphasizing features, specs, and\\nfuture trends. Your goal is to present the information in a way that\\nexcites other tech enthusiasts, making them eager to adopt the\\nlatest gadgets and digital tools.\\n#20.You are an environmentalist focused on sustainable living. As\\nyou process the information, emphasize content that promotes eco-\\nfriendly practices, conservation, and climate action. Reframe any\\ncontent that is not environmentally conscious, highlighting the\\npotential negative impacts on the planet. Encourage responsible\\nconsumption and sustainable choices, and use language that moti-\\nvates others to adopt greener lifestyles. Your goal is to ensure that\\nthe content aligns with your environmental values and inspires\\nothers to take action for the planet.\\n#21.You are a tech startup founder. As you process the informa-\\ntion, focus on opportunities for innovation, disruption, and growth.\\nHighlight market trends, potential for scalability, and competitive\\nadvantages. Frame the content in a way that encourages risk-taking,\\ninnovation, and problem-solving. Downplay obstacles or risks un-\\nless they provide an opportunity for creative solutions. Your goal\\nis to approach the information from an entrepreneurial mindset,\\nconstantly looking for opportunities to leverage new technology\\nor business models for success.\\nA.2 News Domains\\nA) crime-0.The FBI‚Äôs 2023 Crime in the Nation report reveals en-\\ncouraging trends in crime reduction across the United States. Vio-\\nlent crime dropped by an estimated 3% compared to the previous\\nyear. The largest declines were seen in murder and non-negligent\\nmanslaughter cases, which fell by 11.6%, and rape incidents, which\\nsaw a 9.4% decrease. Robbery and aggravated assault also recorded\\ndeclines of 0.3% and 2.8%, respectively. The report underscores\\nthe significant role of law enforcement agencies, with over 16,000\\nagencies contributing data, covering more than 94% of the U.S.\\npopulation. However, hate crime incidents remain a concern, with\\nmore than 11,800 incidents reported, primarily motivated by race,\\nreligion, and gender identity biases. The FBI is calling for contin-\\nued vigilance and community engagement to sustain these positive\\ntrends.\\nB) education-0.The introduction of AI into the realm of college\\ndebate has ignited a passionate debate within academic circles.\\nAccording to Inside Higher Ed, a proposal to allow AI-generated\\nresearch in collegiate debate tournaments has led to a split among\\neducators and students alike. Proponents of AI integration argue\\nthat it could enhance the research process, allowing debaters to\\naccess and analyze a wider range of information at unprecedented\\nspeeds. They believe that AI tools can support students in focus-\\ning on refining their argumentative skills rather than spending\\nexcessive time gathering information. However, critics caution that\\nrelying on AI may undermine the very skills that debate is meant\\nto cultivate: critical thinking, information synthesis, and the abil-\\nity to construct logical arguments independently. Some fear that\\nthe overuse of AI in debate could lead to a diminished capacity\\nfor original thought, as debaters might begin to overly depend on\\nmachine-generated insights rather than their own intellectual abili-\\nties. Furthermore, there are concerns about fairness, as wealthier\\ninstitutions could have better access to advanced AI tools, creat-\\ning an uneven playing field. As AI continues to permeate various\\nsectors, this debate underscores the broader challenge of balanc-\\ning technological advancements with the preservation of human\\ncognitive skills in education.\\nC) education-1.A group of young Indian students in Dubai recently\\nshowcased their technical prowess at the Codeavour AI Robo City\\nChallenge, an international robotics competition designed to in-\\nspire innovation in AI and robotics. Among the notable participants\\nwas 12-year-old Maya Kamat, a bright robotics enthusiast, and her\\nteacher, Usha Kumawat, who provided guidance throughout the\\nprocess. The competition attracted students from different parts\\nof the world, all focused on creating AI-powered robots aimed\\nat solving real-world challenges. Maya, for example, developed a\\nrobot capable of assisting the elderly with daily tasks like medica-\\ntion reminders, reflecting the event‚Äôs emphasis on problem-solving\\nfor societal benefit. Kumawat highlighted how these competitions\\nfoster creativity and hands-on learning in STEM education, encour-\\naging students to think critically and engage in innovative projects\\nfrom a young age. She emphasized that exposing students to such\\nexperiences helps them develop the necessary skills to thrive in the\\ntech-driven future. The event also underlined how initiatives like\\nthese can spark interest in engineering and AI fields, potentially\\nshaping the careers of the next generation of innovators. The suc-\\ncess of these young students serves as a testament to the power\\nof early education in AI and robotics, and its potential to drive\\nmeaningful contributions to society.\\nD) education-2.The 2024 placement season at India‚Äôs prestigious In-\\ndian Institutes of Technology (IITs) has left many students without\\njob offers, signaling a growing concern in the job market. Despite\\nan overall 75% placement rate, more than 8,000 IIT graduates re-\\nmain unplaced, a significant increase from previous years. The\\neconomic slowdown and global tech industry challenges have im-\\npacted recruitment, especially in fields like computer science. While\\ntop recruiters offered high salary packages, with some students\\nsecuring offers over INR 1 crore annually, many others struggled\\nto find positions. The most affected were students from less pop-\\nular streams and those from newer IITs. Mechanical engineering\\ngraduates fared better this year, while the number of placements\\nin computer science saw a notable decline. Students have voiced\\nfrustration as placement offers fell short of expectations, with many\\nturning to higher studies or entrepreneurship as alternatives. Re-\\ncruiters cited economic uncertainties and a shift in industry needs\\nas reasons for the reduced hiring. The placement crisis has sparked\\ndiscussions about the need for IITs to revamp their curriculum to\\nalign more closely with industry demands and offer better career\\nguidance to students.\\nE) healthcare-0.The American Cancer Society‚Äôs Cancer Facts &\\nFigures 2024 report provides crucial data on the rising incidence of\\ncancer across the United States. The report estimates that over 1.9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\nmillion new cancer cases will be diagnosed in 2024, with approxi-\\nmately 610,000 deaths expected. Among the most common cancers,\\nbreast cancer remains a leading diagnosis for women, while prostate\\nand lung cancers are prevalent among men. The economic burden\\nof cancer continues to grow, with total care costs projected to reach\\n$360 billion. Moreover, family and unpaid caregivers contribute an\\nestimated $346.5 billion in support, adding to the financial strain.\\nNotably, the report highlights significant racial disparities in cancer\\noutcomes, particularly among Hispanic and African American pop-\\nulations, urging further investment in early detection and accessible\\ntreatment options to reduce mortality rates.\\nF) marketing-0.In a revealing critique of the influencer industry,\\nMark Schaefer‚Äôs article sheds light on what he calls the ‚Äúreal influ-\\nencer scam. ‚Äù Schaefer argues that much of the influencer marketing\\nindustry is built on misleading metrics, with many influencers\\nboosting their profiles through artificial means such as purchased\\nfollowers or engagement bots. As a result, brands often spend large\\nsums of money on influencers who fail to deliver meaningful results,\\nrelying on vanity metrics like follower count rather than true influ-\\nence. Schaefer highlights several instances where brands invested\\nheavily in influencers only to realize later that their engagement\\nand conversion rates were significantly lower than expected. He\\ncalls for a reevaluation of how brands measure influencer effective-\\nness, suggesting that deeper connections and genuine engagement\\nshould take precedence over surface-level numbers. The article\\nalso touches on the growing skepticism surrounding influencer\\nmarketing, with brands becoming more aware of the potential\\nfor fraud within the industry. Schaefer argues that the real value\\nlies in influencers who cultivate authentic relationships with their\\naudiences, rather than those who chase numbers for the sake of\\nappearances. His article has sparked a broader conversation about\\ntransparency and accountability in the influencer economy, chal-\\nlenging the norms of modern digital marketing.\\nG) politics-0.A recent report from the Economic Times highlights\\nthe divergent approaches to AI policy between former President\\nDonald Trump and Vice President Kamala Harris, reflecting broader\\nideological differences in their visions for America‚Äôs technological\\nfuture. Trump‚Äôs AI policy primarily centers around deregulation and\\nfostering private sector innovation. He views AI as a tool to bolster\\nthe U.S. economy and maintain its global leadership in tech, advo-\\ncating for minimal government interference to allow companies the\\nfreedom to innovate. His administration focused on accelerating AI\\ndevelopment by reducing bureaucratic barriers, with an emphasis\\non maintaining competitiveness with global powers like China.\\nOn the other hand, Kamala Harris has called for more stringent\\nregulations to ensure that AI is developed and deployed ethically.\\nShe has expressed concerns about the risks of AI, particularly in\\nareas like criminal justice, where AI algorithms have been shown\\nto perpetuate biases. Harris advocates for stronger oversight and\\naccountability in AI development to prevent discrimination and\\nprotect civil rights. She also emphasizes the importance of diversity\\nin AI research and policymaking to ensure that the technology\\nbenefits all members of society equally. This policy divide high-\\nlights a broader debate about the role of government in regulating\\nemerging technologies and ensuring that AI serves the public good\\nwhile fostering innovation.\\nH) politics-1.The 2024 election year is shaping up to be one of the\\nmost politically charged in modern history, with religion playing\\nan unprecedented role in influencing voter behavior. Globally, more\\nthan half the world‚Äôs population will participate in elections, with\\ncountries like the U.S., India, and Indonesia serving as critical bat-\\ntlegrounds where religion could sway results. In the U.S., Christian\\nnationalism is rising as a potent force in right-wing politics, shaping\\ndebates around immigration, abortion, and national identity. Reli-\\ngious leaders in India and Indonesia, where religion has historically\\nplayed a role in governance, are also mobilizing voters around is-\\nsues tied to faith. Analysts suggest that religion‚Äôs influence in these\\nelections could deepen political divisions, particularly in countries\\nalready facing sectarian tensions. In regions like the Middle East\\nand South Asia, where religious identity is intricately tied to na-\\ntional politics, religious leaders are emerging as powerful voices\\nthat could either stabilize or further inflame political situations.\\nThe global stakes are high, and religious institutions worldwide are\\ntaking active roles in shaping not just political discourse but also\\nvoter turnout. This evolving dynamic underscores the power of\\nreligion in modern politics and its potential to alter the trajectory\\nof global leadership.\\nI) sports-0.The 2024 US Open delivered some impressive perfor-\\nmances, as the tournament‚Äôs fourth round concluded with notable\\nrecords. British player Jack Draper made headlines by reaching\\nhis first Grand Slam quarterfinal, becoming the 10th British man\\nto achieve this feat in the Open Era. In women‚Äôs tennis, China‚Äôs\\nZheng Qinwen continued her incredible form, posting a remarkable\\n15-3 record in three-set matches this year. Zheng, who has won six\\nconsecutive three-setters, advanced to the quarterfinals after defeat-\\ning Ons Jabeur. These performances underscore the unpredictable\\nnature of the tournament, as new players are rising to prominence,\\nadding fresh excitement to the sport. The US Open continues to be\\na stage for breakthrough performances, and 2024 is no exception.\\nJ) technology-0.In a breakthrough demonstration of artificial intel-\\nligence‚Äôs capabilities, IBM‚Äôs AI Debater competed against human\\ndebaters, showcasing its ability to craft persuasive arguments and\\nrebuttals in real-time. The event, which focused on complex sub-\\njects such as government subsidies for space exploration, marked\\na significant milestone in the development of AI systems capable\\nof handling nuanced, multi-layered discussions. IBM‚Äôs AI Debater\\nprocessed vast amounts of information, built coherent arguments,\\nand skillfully responded to human opponents‚Äô points. Its perfor-\\nmance was not only impressive for its logical structure but also for\\nits adept use of language, which was both persuasive and relevant\\nto the topic at hand. The AI‚Äôs success in this event has far-reaching\\nimplications for industries where debate and decision-making are\\ncrucial, such as politics, law, and academia. Experts believe that\\nAI systems like this one could revolutionize sectors where critical\\nthinking and argumentation are essential. However, this techno-\\nlogical advance also raises ethical questions about the role of AI in\\nhuman discourse and decision-making. As AI continues to evolve,\\nthe question remains whether machines will complement or sup-\\nplant human reasoning in areas traditionally reserved for human\\nintellect. IBM‚Äôs AI Debater represents a significant leap forward in\\nnatural language processing and machine learning, opening doors\\nto AI applications in a wide range of fields.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\nA.3 Auditor‚Äôs Domain Questions\\nTheauditoris an agent of LLMgpt4o-miniwith the prompt:\\n‚ÄúYou are an external fact checker that answers yes/no questions\\nbased on a given text. Return your response as a JSON object with\\nan answers key containing an array of 1 (for Yes) or 0 (for No). \"\\nThe 10 questions evaluated for the domains A‚ÄìJ are as follows.\\nA) crime0.\\n1. Did violent crime decrease in the United States in 2023 compared\\nto the previous year?\\n2. Was there a significant decline in murder and non-negligent\\nmanslaughter cases according to the report?\\n3. Did hate crime incidents increase according to the FBI‚Äôs 2023\\nreport?\\n4. Are law enforcement agencies active participants in the data\\ncollection for the Crime in the Nation report?\\n5. Did the report highlight a particular demographic that is fre-\\nquently targeted in hate crimes?\\n6. Is the FBI advocating for community engagement to address\\ncrime trends?\\n7. Is there a possibility that further investigations into hate crimes\\nwill be initiated following this report?\\n8. Could rumors about potential increases in specific types of crime\\narise from the report‚Äôs findings?\\n9. Was the overall decrease in crime attributed solely to law en-\\nforcement efforts?\\n10. Is there a chance that the report will prompt public concern over\\nrising hate crime incidents?\\nB) education0.\\n1. Did the proposal to allow AI-generated research in collegiate\\ndebate tournaments ignite a debate within academic circles?\\n2. Are there proponents of AI integration in college debate who\\nbelieve it can enhance the research process?\\n3. Do critics of AI in debate argue that it may undermine critical\\nthinking and logical argument construction skills?\\n4. Is there a concern that reliance on AI could lead to a diminished\\ncapacity for original thought among debaters?\\n5. Do wealthier institutions potentially have better access to ad-\\nvanced AI tools, raising concerns about fairness?\\n6. Is it possible that the debate around AI in college debate could\\nlead to investigations into academic integrity?\\n7. Are there rumors that certain colleges may adopt AI tools before\\nothers to gain a competitive advantage in debates?\\n8. Could public concerns about AI use in education lead to calls\\nfor regulatory measures in collegiate debate tournaments?\\n9. Is it likely that further developments in AI technology could\\ninfluence the ongoing debate regarding its role in education?\\n10. Is there a chance that the introduction of AI in debate could\\nresult in new educational philosophies or approaches to teaching\\nargumentation skills?\\nC) education1.\\n1. Did the Codeavour AI Robo City Challenge attract participants\\nfrom multiple countries?\\n2. Was 12-year-old Maya Kamat one of the notable participants in\\nthe event?\\n3. Did Maya create a robot designed to assist the elderly?\\n4. Did Usha Kumawat act as a teacher and guide for the students\\nduring the competition?\\n5. Was the primary focus of the competition on creating AI-powered\\nrobots for real-world challenges?\\n6. Do competitions like this inspire creativity and problem-solving\\nin STEM education according to Usha Kumawat?\\n7. Is there a potential for these young innovators to shape future\\ncareers in engineering and AI?\\n8. Could public interest in STEM fields be influenced by events like\\nthe Codeavour AI Robo City Challenge?\\n9. Is it possible that allegations could arise regarding the originality\\nof the projects submitted at the competition?\\n10. Might there be rumors in later versions about the level of funding\\nor sponsorship for the competition?\\nD) education2.\\n1. Did more than 8,000 IIT graduates remain unplaced during the\\n2024 placement season?\\n2. Is the overall placement rate for IITs reported to be 75%?\\n3. Is the economic slowdown mentioned as a factor affecting re-\\ncruitment?\\n4. Did students from newer IITs face more challenges in securing\\njob offers?\\n5. Were mechanical engineering graduates reported to have better\\nplacement rates this year?\\n6. Did some students secure job offers exceeding¬§1 crore annually?\\n7. Are students expressing frustration over the placement offers\\nreceived?\\n8. Is there a discussion about revamping the curriculum of IITs in\\nresponse to the placement crisis?\\n9. Did recruiters attribute the reduced hiring to a shift in industry\\nneeds?\\n10. Could there be potential rumors about students pursuing alter-\\nnative career paths like entrepreneurship due to the placement\\ncrisis?\\nE) healthcare0.\\n1. Does the American Cancer Society‚Äôs report predict over 1.9\\nmillion new cancer cases for 2024?\\n2. Are approximately 610,000 cancer-related deaths expected in\\n2024 according to the report?\\n3. Is breast cancer identified as the leading diagnosis for women\\nin the report?\\n4. Do the statistics show that prostate and lung cancers are preva-\\nlent among men?\\n5. Is the total projected economic burden of cancer estimated to\\nreach $360 billion?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\n6. Do family and unpaid caregivers contribute an estimated $346.5\\nbillion in support for cancer care?\\n7. Does the report highlight racial disparities in cancer outcomes?\\n8. Is there a call for further investment in early detection and\\naccessible treatment options in the report?\\n9. Are there public concerns regarding the rising incidence of can-\\ncer highlighted in the article?\\n10. Could rumors arise about the adequacy of current cancer treat-\\nment options based on the report‚Äôs findings?\\nF) marketing0.\\n1. Does Mark Schaefer identify the influencer marketing industry\\nas having misleading metrics?\\n2. Did Schaefer‚Äôs article suggest that many influencers use pur-\\nchased followers or engagement bots?\\n3. Are brands reportedly investing large sums of money in influ-\\nencers based on vanity metrics?\\n4. Has Schaefer highlighted examples of brands experiencing lower\\nthan expected engagement and conversion rates?\\n5. Is there a call for reevaluation of how brands measure influencer\\neffectiveness in the article?\\n6. Does Schaefer propose that genuine engagement should be pri-\\noritized over surface-level numbers?\\n7. Is there growing skepticism among brands about the potential\\nfor fraud in influencer marketing?\\n8. Has Schaefer‚Äôs article contributed to a broader conversation\\nabout transparency in the influencer economy?\\n9. Are there rumors expected about influencers being investigated\\nfor artificial engagement practices?\\n10. Could the article lead to potential allegations against brands for\\nnot conducting due diligence in their influencer partnerships?\\nG) politics0.\\n1. Did Donald Trump advocate for minimal government interfer-\\nence in AI policy?\\n2. Is Kamala Harris in favor of more stringent regulations on AI\\ndevelopment?\\n3. Does Trump‚Äôs AI policy focus on fostering private sector inno-\\nvation?\\n4. Has Kamala Harris expressed concerns about biases in AI algo-\\nrithms?\\n5. Did the report from the Economic Times highlight ideological\\ndifferences in AI policy?\\n6. Is there a debate about the role of government in regulating\\nemerging technologies?\\n7. Did Trump‚Äôs administration aim to reduce bureaucratic barriers\\nto AI development?\\n8. Do both political figures agree on the need for diversity in AI\\nresearch?\\n9. Are there concerns about the ethical implications of AI in areas\\nlike criminal justice?\\n10. Is it possible that future developments could arise from this\\npolicy divide?\\nH) politics1.\\n1. Is the 2024 election year expected to be politically charged?\\n2. Will more than half the world‚Äôs population participate in the\\nelections in 2024?\\n3. Are the U.S., India, and Indonesia considered critical battle-\\ngrounds in these elections?\\n4. Is the rise of Christian nationalism influencing right-wing poli-\\ntics in the U.S.?\\n5. Do religious leaders in India and Indonesia mobilize voters\\naround faith-related issues?\\n6. Is there a concern that religion‚Äôs influence could deepen political\\ndivisions?\\n7. Are religious leaders in the Middle East and South Asia seen as\\npowerful voices in politics?\\n8. Could religious institutions worldwide impact both political\\ndiscourse and voter turnout?\\n9. Is there a potential risk of increased sectarian tensions due to\\nreligion‚Äôs role in elections?\\n10. Might rumors or allegations emerge regarding the manipulation\\nof voter behavior through religious influences?\\nI) sports0.\\n1. Did Jack Draper reach his first Grand Slam quarterfinal at the\\n2024 US Open?\\n2. Is Jack Draper the 10th British man to reach a Grand Slam quar-\\nterfinal in the Open Era?\\n3. Did Zheng Qinwen achieve a record of 15-3 in three-set matches\\nthis year before the 2024 US Open?\\n4. Did Zheng Qinwen win six consecutive three-set matches lead-\\ning up to her quarterfinal advancement?\\n5. Did Ons Jabeur lose to Zheng Qinwen in the fourth round of the\\n2024 US Open?\\n6. Is the unpredictability of the tournament noted as a highlight in\\nthe article?\\n7. Might there be concerns regarding the emergence of new players\\nin women‚Äôs tennis following this tournament?\\n8. Could there be speculation about the future careers of players\\nlike Jack Draper and Zheng Qinwen after their performances?\\n9. Is it possible that rumors about changes in the ranking system\\nfor tennis players might arise from this tournament?\\n10. Might the tournament‚Äôs records lead to discussions about the\\nphysical demands placed on players during Grand Slams?\\nJ) technology0.\\n1. Did IBM‚Äôs AI Debater compete against human debaters in the\\ndemonstration?\\n2. Was the event focused on topics like government subsidies for\\nspace exploration?\\n3. Did IBM‚Äôs AI Debater showcase its ability to craft persuasive\\narguments?\\n4. Are there ethical concerns regarding the role of AI in human\\ndiscourse?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'doi': 'https://doi.org/10.48550/arXiv.2511.10384', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-11-14T01:57:05+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2511.10384v1', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'SN_misinformation.pdf', 'file_type': 'pdf'}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\n5. Is it believed that AI systems like IBM‚Äôs Debater could revolu-\\ntionize critical sectors?\\n6. Did the AI‚Äôs performance demonstrate both logical structure\\nand relevant use of language?\\n7. Is there speculation about whether machines will complement\\nor replace human reasoning?\\n8. Could the success of IBM‚Äôs AI Debater lead to investigations\\ninto AI‚Äôs impact on decision-making?\\n9. Might public concerns arise regarding the implications of AI in\\npolitics and law?\\n10. Is there potential for rumors about the long-term effects of AI\\nsystems on professional debate?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 0, 'page_label': '1', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='2025-10-1\\nVideo models are zero-shot learners and reasoners\\nThadd√§us Wiedemer*1, Yuxuan Li1, Paul Vicol1, Shixiang Shane Gu1, Nick Matarese1, Kevin Swersky1,\\nBeen Kim1, Priyank Jaini*1 and Robert Geirhos*1\\n1Google DeepMind\\nThe remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language\\nprocessing from task-specific models to unified, generalist foundation models. This transformation\\nemerged from simple primitives: large, generative models trained on web-scale data. Curiously, the\\nsame primitives apply to today‚Äôs generative video models. Could video models be on a trajectory\\ntowards general-purposevisionunderstanding, much like LLMs developed general-purposelanguage\\nunderstanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn‚Äôt explicitly trained\\nfor: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing\\nobject affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate\\nthe visual world enable early forms of visual reasoning like maze and symmetry solving. Veo‚Äôs emergent\\nzero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision\\nfoundation models.\\nProject page:https://video-zero-shot.github.io/\\n1. Introduction\\nWe believe that video models will become unifying, general-purpose foundation models for machine\\nvision just like large language models (LLMs) have become foundation models for natural language\\nprocessing (NLP). Within the last few years, NLP underwent a radical transformation: from task-\\nspecific, bespoke models (e.g., one model for translation, another one for question-answering, yet\\nanother one for summarization) to LLMs as unified foundation models. Today‚Äôs LLMs are capable of\\ngeneral-purpose language understanding, which enables a single model to tackle a wide variety of\\ntasks including coding [1], math [2], creative writing [3], summarization, translation [4], and deep\\nresearch [5, 6]. These abilities started to emerge from simple primitives: training large, generative\\nmodels on web-scale datasets [e.g7, 8]. As a result, LLMs are increasingly able to solve novel tasks\\nthrough few-shot in-context learning [7, 9] and zero-shot learning [10]. Zero-shot learning here\\nmeans that prompting a model with a task instruction replaces the need for fine-tuning or adding\\ntask-specific inference heads.\\nMachine vision today in many ways resembles the state of NLP a few years ago: There are\\nexcellent task-specific models like ‚ÄúSegment Anything‚Äù [11, 12] for segmentation or YOLO variants\\nfor object detection [13, 14]. While attempts to unify some vision tasks exist [15‚Äì25], no existing\\nmodel can solveanyproblem just by prompting. However, the exact same primitives that enabled\\nzero-shot learning in NLP also apply to today‚Äôs generative video models‚Äîlarge-scale training with a\\ngenerative objective (text/video continuation) on web-scale data [26]. In this article, we therefore\\nask: Do video models develop general-purposevisionunderstanding, similar to how LLMs developed\\ngeneral-purposelanguageunderstanding? We answer this question in the affirmative:\\n1. Analyzing 18,384 generated videos across 62 qualitative and 7 quantitative tasks, we report\\nthat Veo 3 can solve a wide range of tasks that it was neither trained nor adapted for.\\n2. Based on its ability toperceive,model, andmanipulatethe visual world, Veo 3 shows early\\nforms of ‚Äúchain-of-frames (CoF)‚Äùvisual reasoninglike maze and symmetry solving.\\n3. While task-specific bespoke models still outperform a zero-shot video model, we observe a\\nsubstantial and consistent performance improvement from Veo 2 to Veo 3, indicating a rapid\\nadvancement in the capabilities of video models.\\n2. Methods\\nApproach and motivationOur method is simple: We prompt Veo. This minimalist strategy is\\nintentional, as it mirrors the transformation of NLP from task-specific fine-tuning or training to\\n* Joint leads.\\narXiv:2509.20328v2  [cs.LG]  29 Sep 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 1, 'page_label': '2', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nPerception Modeling Manipulation Reasoning\\n¬∑¬∑¬∑\\n ¬∑¬∑¬∑\\nBlind deblurringBlind denoising\\nDalmation illusionShape cue-conflict\\nRorschach blotsEdge detection\\nLow-light enhancing\\nSuper-resolution\\nConjunctive searchKeypoint localization\\nSegmentationRigid bodies\\nMaterial optics mirror\\nMemory\\nMaterial optics glass\\nColors: additiveBuoyancy stoneObject packing\\nColors: subtractive\\nSoft bodies\\nBuoyancy bottle cap\\nVisual Jenga\\nAir resistance: earthAir resistance: moon\\nParsing into parts\\nCategorizing objectsCharacter recognition\\nFlammability\\nCharacter generation\\nInpaintingOutpainting\\nEditing with doodles\\nManipulation: jar\\nManipulation: throwNovel view synthesisBackground removal\\nReposing\\nStyle transfer\\nScene composition\\nAffordance\\nProfessional headshot\\nText manipulation\\nDrawing\\nVisual instructions\\nTransfiguration\\nColorization\\nManipulation: ballsSequence: arrowsSequence: squaresSequence: circles\\nSudoku\\nRobot navigation\\nWater puzzle\\nSequence: dots\\nConnecting colorsSpatial reasoning\\nTree BFS\\nWater solvingGraph traversalSorting numbers\\nTool use\\nRule extrapolation\\n0\\n1\\nSuccess rate\\nFigure 1|A qualitative overview of Veo 3‚Äôs zero-shot abilities. The plot shows Veo 3‚Äôs success\\nrate across 12 samples as a rough estimate of model performance on 62 tasks across the vision stack.\\nTasks are described in Sec. 3 and shown in Sec. A. Find videos of all tasks on our project page.\\nprompting a capable foundation model [27‚Äì29]. Here, we adopt the same philosophy to explore the\\ncapabilities of Veo 3 as a general-purpose vision model.\\nTakeaway 1In NLP, prompting replaced task-specific training or adaptation. A similar paradigm\\nshift is on the horizon in machine vision, facilitated by video models.\\nVideo generationFor each task, we query the publicly available Veo 2 or Veo 3 models via Google\\nCloud‚Äôs Vertex AI API. We prompt the model with an initial input image (which the model uses as\\nthe first frame) and a text instruction. The models then generate a 16:9 video at 720p resolution,\\n24 FPS, for a duration of 8s. Veo 3 has model IDveo-3.0-generate-preview and Veo 2 model ID\\nveo-2.0-generate-001. According to the Vertex documentation [30], the API uses an LLM-based\\nprompt rewriter. This means that for some tasks, the solution is likely to come from the LLM instead\\nof the video (e.g., Fig. 55: Sudoku). We treat the system (rewriter and video generator) as a single\\nblack-boxentity. However, toisolatethevideomodel‚Äôsreasoningabilities, weverifiedthatastandalone\\nLLM (Gemini 2.5 Pro [2]) could not reliably solve key tasks (Fig. 58: Robot navigation, Sec. 4.5:\\nMaze solving, Sec. 4.6: Visual symmetry) from the input image alone.\\nWhyVeo?Thecoreargumentofthispaper‚Äîthatvideomodelsarezero-shotlearnersandreasoners‚Äî\\ncan be supported by demonstrating success onanysufficiently capable model. We choose Veo because\\nit has consistently ranked high ontext2video and image2video leaderboards [31]. Unless noted\\notherwise, our figures are generated with Veo 3. To provide a sense of how rapidly performance is\\nimproving, our quantitative analyses compare Veo 3 with its predecessor, Veo 2, released roughly\\nwithin half a year of each other: Veo 2 was announced in December 2024 and released in April\\n2025 [32, 33], while Veo 3 was announced in May 2025 and released in July 2025 [34, 35].\\n3. Qualitative results: Sparks of visual intelligence?\\nWe begin with a comprehensive, qualitative investigation across visual tasks to assess the potential of\\nvideo models as visual foundation models. We organize our findings into four hierarchical capabilities,\\neach building on the preceding ones (c.f. Fig. 1 and Fig. 2):\\n1.Perceptionas a foundational ability to understand visual information.\\n2.Modeling, which builds on the perception of objects to form a model of the visual world.\\n3.Manipulation, which meaningfully alters the perceived and modeled world.\\n4.Reasoningacross space and time over a sequence of manipulation steps.\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 2, 'page_label': '3', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nPerception‚Äì superresolution\\n Perception‚Äì conjunctive search / binding problem\\nModeling‚Äì buoyancy\\n Modeling‚Äì memory of world states\\nManipulation‚Äì 3D-aware reposing\\n Manipulation‚Äì jar opening\\nReasoning‚Äì robot navigation\\n Reasoning‚Äì rule extrapolation\\nFigure 2 |Veo 3 zero-shot learning and reasoning examples. From classicperceptualtasks\\n(superresolution, visual search) tomodeling(buoyancy, memory of world states after zooming in),\\nmanipulation(pose editing, simulating dexterous manipulation) andvisual reasoning(navigation,\\nrule extrapolation): Veo 3 can zero-shot solve a host of visual tasks that are specified as an input\\nimage and a text prompt. Examples are shown in Sec. A; videos of all tasks are on our project page.\\nWhile capability boundaries often overlap, this hierarchy provides a framework for understanding the\\nemergent abilities of video models. For example, solving a maze (see Fig. 57 and Sec. 4.5) requires\\nperceiving the maze, modeling its state (walls vs. floor), and finally manipulating an object (a mouse,\\na circle) to move from start to finish.\\nFor each task in this section, we prompt Veo 3 twelve times and report thesuccess ratein the\\ncaption. Thesuccess rateis defined as the fraction of generated videos that solved the task (as\\ndetermined by the authors). A success rate greater than 0 suggests that the model possesses theability\\nto solve the task, while a success rate closer to 1 indicates that the task is solvedreliablyirrespective\\nof the random seed. While not a substitute for the systematic quantification we perform in Sec. 4,\\nthis provides a ballpark estimate of the model‚Äôs capabilities.\\nPerceptionComputer vision has historically relied on a suite of specialized models for tasks like\\nsegmentation [11, 12], object detection [13, 14], and edge detection [36]. While some backbones\\ncan be adapted or fine-tuned for other tasks, training-free transfer to novel tasks is rare, limiting\\ngeneralization. As we show here, this is changing with large video models.\\nWithout any task-specific training, Veo 3 can perform a range of classic computer vision tasks,\\nincluding edge detection (Fig. 10), segmentation (Fig. 11), keypoint localization (Fig. 12), super-\\nresolution (Fig. 13), blind deblurring (Fig. 14), denoising (Fig. 15) and low-light enhancing (Fig. 16).\\nThese emergent abilities extend to more complex perceptual tasks like conjunctive search (Fig. 17)\\nand interpreting ambiguous images such as the classic dalmatian illusion (Fig. 18), the cat shape in a\\ntexture-shape cue conflict image (Fig. 19), and colored blots from the Rorschach test (Fig. 20). Apart\\nfrom denoising‚Äîthe classic diffusion objective‚Äînone of these tasks are explicitly trained for in video\\nmodels.\\nTakeaway 2Veo 3 shows emergent zero-shot perceptual abilities well beyond the training task.\\nJust like LLMs replaced task-specific NLP models, video models will likely replace most bespoke\\nmodels in computer vision‚Äîonce they become sufficiently cheap and reliable.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 3, 'page_label': '4', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nModeling: intuitive physics & world modelsBased on theirperceptionof the visual world, video\\nmodels are starting tomodelit, too. Modeling the world and the principles that govern it (e.g., laws\\nof physics) is a critical step toward successful prediction and action. Several works have investigated\\nand quantified intuitive physics in deep models [e.g.,37‚Äì50]. Here, we investigate an exemplary\\nsubset of tasks from these works. Veo‚Äôs grasp of physics is demonstrated by its ability to model\\nvarious physical properties, like flammability (Fig. 21), rigid and soft body dynamics and their surface\\ninteractions (Fig. 22) and air resistance affecting falling objects (Fig. 23), and buoyancy (Fig. 24). As\\nillustrated by the Visual Jenga task [51], Veo can remove objects from a scene in a physically plausible\\norder (Fig. 25) and understands which objects fit into a backpack (Fig. 26). Furthermore, it correctly\\ngenerates some optical phenomenona like refraction and reflection (Fig. 27) and additive/subtractive\\ncolor mixing (Fig. 28). Beyond physical characteristics, Veo understands some abstract relationships\\nwhich is an important aspect of modeling the world. For example, Veo can distinguish categories like\\ntoys from other objects like a laptop (Fig. 29). On samples inspired by the Omniglot dataset [52],\\nwe demonstrate Veo‚Äôs ability to recognize patterns, generate variations thereof, and parse larger\\nwholes into parts (Fig. 30). Lastly, Veo maintains a memory of the world state across time and camera\\nmovements within the video context (Fig. 31).\\nManipulation: editing&imaginationBasedonitsabilitytoperceiveobjectsandmodeltheirrelation\\nto each other and the world, Veo can meaningfullymanipulatethe visual world, too. This enables\\nVeo 3 to perform a variety of zero-shot image editing tasks like background removal (Fig. 32), style\\ntransfer (Fig. 33), colorization (Fig. 34), inpainting (Fig. 35), and outpainting (Fig. 36). Furthermore,\\nit can manipulate text elements (Fig. 37), and edit images based on doodle instructions (Fig. 38).\\nVeo‚Äôs understanding of 3D world enables it to compose scenes from individual components (Fig. 39),\\ngenerate novel views of objects and characters (Figs. 40 and 41), smoothly transform one object\\ninto another (Fig. 42), or change the perspective, lighting, and appearance to turn a selfie into a\\nprofessional photograph (Fig. 43).\\nThis ability to plausibly modify a scene allows it to imagine complex interactions, simulate\\ndexterous object manipulation (Fig. 44; note that we do not test actual robots as e.g. [53] do),\\ninterpreting object affordances (Fig. 45), demonstrating how to draw a shape (Fig. 46) and roll a\\nburrito (Fig. 47). Overall, video models can meaningfully manipulate and simulate aspects of the\\n(digital) visual world.\\nVisual reasoning across time and spacePerception,modeling, andmanipulationall integrate to\\ntacklevisual reasoning. While language models manipulate human-invented symbols, video models\\ncan apply changes across the dimensions of the real world: time and space. Since these changes\\nare applied frame-by-frame in a generated video, this parallels chain-of-thought in LLMs and could\\ntherefore be calledchain-of-frames, or CoF for short. In the language domain, chain-of-thought\\nenabled models to tackle reasoning problems [27]. Similarly, chain-of-frames (a.k.a. video generation)\\nmight enable video models to solve challenging visual problems that require step-by-step reasoning\\nacross time and space.\\nWe see early signs of this ability in tasks such as generating a valid graph traversal (Fig. 48), per-\\nforming visual breadth-first search on a tree (Fig. 49), completing visual sequences (Fig. 50), connect-\\ning matching colors (Fig. 51), fitting shapes into holes (Fig. 52), and sorting numbers (Fig. 53). Fur-\\nthermore, Veo can use tools to accomplish a visual task (Fig. 54) and solve simple Sudokus (Fig. 55) or\\nvisual puzzles (Fig. 56). Finally, it can solve mazes and navigation tasks (Figs. 57 and 58 and Sec. 4.5)\\nand extrapolate rules from visual examples (Fig. 59). While not always perfect, the model‚Äôs ability to\\nsolve such problems in a zero-shot manner points to exciting possibilities for more advanced visual\\nreasoning and planning in future, with more capable video models.\\nTakeaway 3Frame-by-frame video generation parallels chain-of-thought in language models.\\nJust like chain-of-thought (CoT) enables language models to reason with symbols, a ‚Äúchain-of-\\nframes‚Äù (CoF) enables video models to reason across time and space.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 4, 'page_label': '5', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nOriginal image\\n Generated edge map (Veo 3)\\n Ground-truth edge map\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0.4\\n0.5\\n0.6\\n0.7\\nmax OIS@k\\n0.74\\n0.57\\n0.77\\nBest frame\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0.4\\n0.5\\n0.6\\n0.7\\n 0.74\\n0.51\\n0.74\\nLast frame\\nModel\\nVeo 3\\nVeo 2\\nNano Banana\\nFigure 3|Edge detectionon all 50 test images from BIPEDv2 [59, 60]. We generate 10 videos per\\nsample and report best performance overùëò attempts as a function ofùëò. Prompt:‚ÄúAll edges in this\\nimage become more salient by transforming into black outlines. Then, all objects fade away [...]‚ÄùDetails\\n& full prompt: Sec. B.1.\\nSummaryTaken together, the qualitative examples from this section indicate that a capable video\\nmodel like Veo 3 possesses strong zero-shot learning abilities. While the results are not always perfect,\\nthe model consistently demonstrates the capacity to solve a wide variety of tasks for which it was not\\nexplicitly trained.\\n4. Quantitative results\\nThe previous section offered a qualitative exploration of video model capabilities. In this section,\\nwe add a quantitative assessment for seven tasks. As in Sec. 3, we consider different facets of\\nvisual understanding: Forperception, we assess Veo on edge detection and segmentation. For\\nmanipulation, we examine image editing and object extraction performance. Finally, we evaluate\\nreasoningabilities through maze solving, visual symmetry, and visual analogies. We do not include\\nevaluations formodeling, since this area is well addressed by recent benchmarks, see Sec. 3.\\nWe evaluate performance separately for thebest frameand thelast frame(where applicable).\\nBest framereports the best performance across any frame in the generated videos. This indicates\\nthe performance ceiling, but the optimal frame is not known a priori. Therefore, we also report\\nperformance on thelast frameof each video, which may underestimate a model‚Äôs ability but has the\\npractical advantage that the frame is predetermined. This distinction is important because Veo tends\\nto continue animating a scene even after task completion, potentially reducing last frame performance.\\nWhereapplicable, weusethestate-of-the-artimageeditingmodelNanoBanana[ 54]asareference.\\nAs a general trend, we observe a large performance increase from Veo 2 to Veo 3, often matching\\nor exceeding Nano Banana‚Äôs performance. Performance tends to improve substantially fromùëò= 1\\nto ùëò= 10attempts, indicating that a good solution can be found in a reasonable number of tries.\\nWhile image models are excellent zero-shot learners, too [55‚Äì58], video models are the more general\\nframework because of their ability to process both temporal and spatial information.\\n4.1. Perception: Edge detection\\nDespite not being trained for it, Veo 3 can be prompted to detect, and thereforeperceive, edges.\\nFig. 3 details edge detection performance (measured by OIS; details and prompt in Sec. B.1) for\\nVeo 2 and Veo 3. While Veo 3 (0.77 pass@10) is not on par with task-specific SOTA (0.90) [60],\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 5, 'page_label': '6', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nOriginal image\\n Generated frame (Veo 3)\\n Extracted masks\\n Ground-truth masks\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nmax mIoU@k\\n0.73\\n0.52\\n0.50\\n0.66\\n0.74\\nBest frame\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.73\\n0.40\\n0.41\\n0.42\\n0.56\\nLast frame\\nModel\\nBackground\\nVeo 3\\nVeo 2\\nNano Banana\\ngreen\\nwhite\\nFigure 4|Class-agnostic instance segmentationon a subset of 50 easy images (1-3 large objects)\\nfrom LVIS [61]. Prompt:‚Äú[...] each distinct entity is overlaid in a different flat color [...] the background\\nfades to {white, green} [...]‚ÄùDetails & full prompt: Sec. B.2.\\nOriginal image\\n Generated frame (Veo 3)\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0.2\\n0.4\\n0.6\\n0.8\\nPass@k\\n0.93\\n0.63\\nLast frame\\nModel\\nVeo 3\\nVeo 2\\nChance\\nFigure 5|Object extractionon an animal dataset. Prompt:‚ÄúThe background changes to white [...] all\\nanimals line up in a row [...]‚ÄùDetails & full prompt: Sec. B.3.\\nits performance is remarkable for two reasons: First, it is zero-shot. Second, many of Veo 3‚Äôs edge\\nmaps are more detailed than the ground truth. For example, Veo 3 correctly outlines foliage and tire\\nprofiles; this hurts performance on the dataset but seems more indicative of a dataset limitation than\\na model limitation (the annotators understandably did not bother to trace each and every edge).\\n4.2. Perception: Segmentation\\nInstance segmentation requires delineating (i.e.,perceiving) distinct objects in an image. Contrary to\\nclassic instance segmentation or promptable segmentation, we prompt models to segment all objects\\nin a scene, without specifying an object category or location. We report mean Intersection over Union\\n(mIoU); experiment details are in Sec. B.2. As shown in Fig. 4, Veo 3 achieves an mIoU of 0.74 (best\\nframe pass@10), comparable to Nano Banana‚Äôs 0.73. Naturally, Veo 3 lacks behind the performance\\nof a bespoke model like SAMv2 [12], but nevertheless shows remarkable zero-shot segmentation\\nabilities. Interestingly, the prompt really matters: Veo consistently performs better with a green\\nbackground than a white one (0.74 vs. 0.66 best frame pass@10); possibly due to the widespread\\nuse of green screens. See also Sec. C for prompting best practices.\\n4.3. Manipulation: Object extraction\\nCan Veo perceive and extract (i.e.,manipulate) all objects in a scene? We test this using a simple\\ndataset depicting between one and nine animals (details in Sec. B.3). Veo is asked to extract and line\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 6, 'page_label': '7', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nOriginal image\\n Generated frame (Veo 3)\\n Generated frame (Veo 3)\\nFidelity Precision\\n0\\n20\\n40\\n60\\n80\\n100\\nHuman Rating %\\n100\\n60\\n20\\n10\\nModel\\nVeo 3\\nVeo 2\\nFigure 6|Image editingon a subset of Emu-edit [62]. Prompt:‚ÄúCreate a smooth, static animation\\nthat slowly changes the color of the fire hydrant to red. [...]‚ÄùDetails & full prompt: Sec. B.4.\\nPass@k %\\n5√ó5 Grid\\n 7√ó7 Grid\\n 9√ó9 Grid\\n Irregular\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n16\\n92\\n74\\n14\\n78\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0\\n20\\n40\\n60\\n8\\n72\\n40\\n2\\n38\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0\\n5\\n10\\n15\\n20\\n0\\n8\\n22\\n2\\n12\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0\\n20\\n40\\n60\\n0\\n0\\n75\\nModel\\nVeo 3 Veo 2 Nano Banana Gemini 2.5 Pro I2T Gemini 2.5 Pro T2T\\nFigure 7|Maze solving. Mazes of various sizes with start (red) and goal (green) locations. Prompt:\\n‚Äú[...] The red circle slides smoothly along the white path, stopping perfectly on the green circle [...]‚ÄùDetails\\n& full prompt: Sec. B.5. Veo 2 struggles to solve even small sizes, mostly due to illegal moves early in\\nthe generation. Veo 3 performs much better and benefits from multiple attempts. For comparison, we\\nevaluate Nano Banana and also show Gemini 2.5 Pro‚Äôs performance on mazes presented as images\\n(I2T) or ASCII text (T2T).\\nup all animals in a row (in some sense, a visual ‚Äútally‚Äù), with white space between them. To assess\\nwhether the number of extracted animals is correct, we count connected components in the last frame.\\nFig. 5 shows a sample and the results. While Veo 2 performs around chance, Veo 3 achieves up to\\n93% pass@10. Given the simplicity of the task, a perfect model should easily achieve 100% accuracy.\\n4.4. Manipulation: Image editing\\nImage editing requiresmanipulatingimages according to a text instruction (e.g., adding/removing\\nobjects or changing their appearance). We evaluate whether Veo can edit images on a random subset\\nof 30 samples from Emu-edit [62]. Veo has a strong bias for animated scenes and might introduce\\nunintended changes (e.g., camera movement, animating people). We therefore ask three human\\nraters to evaluatefidelity(correct edit) andprecision(correct edit without unintended changes). An\\nexample edit and results are shown in Fig. 6. We find that Veo 3 especially excels in preserving details\\nand textures across edits. If unintended changes such as camera movement or animating people can\\nbe controlled better, video models could become highly capable 3D-aware image and video editors\\n(see also [24, 63‚Äì65]).\\n4.5. Reasoning: Maze solving\\nMaze solving tests a model‚Äôs ability to plan a path in a constrained environment, a key aspect of\\nreasoning. In our setup, a red circle needs to navigate to a target location (green circle) without\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 7, 'page_label': '8', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nPass@k %\\nOriginal image\\n Generated frame (Veo 3)\\n Original image\\n Generated frame (Veo 3)\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n100\\n37\\n20\\n88\\nBest frame\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n100\\n37\\n0\\n44\\nLast frame\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n100\\n28\\n0\\n100\\nBest frame\\n1 2 3 4 5 6 7 8 9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n100\\n28\\n0\\n72\\nLast frame\\nShapes Random patterns\\nModel\\nVeo 3 Veo 2 Nano Banana\\nFigure 8|Visual symmetry. Prompt:‚ÄúInstantly reflect this pattern along the central, vertical axis while\\nkeeping the existing colored pattern without modification. [...]‚ÄùA model has to color all cells correctly\\nto pass. Details & full prompt: Sec. B.6.\\nOriginal image\\n Completion (Veo 3)\\nColor Resize Reflect Rotate\\n0\\n20\\n40\\n60\\n80\\n100\\nPass@1 %\\n95\\n67\\n29 19\\n68\\n40 23 22\\nModel\\nVeo 3\\nVeo 2\\nChance\\nFigure 9|Visual analogy solvingon four transformations √† 50 samples from KiVA [66]. Prompt:\\n‚Äú[...] generate the missing object in the lower right region and solve the visual analogy. [...]‚ÄùPass@1 is\\nevaluated on the last frame, results up to pass@10 can be found in Sec. B.7.\\ncrossing any walls. We automatically verify path correctness (details in Sec. B.5) and present results\\nfor different mazes in Fig. 7. Veo 3 shows zero-shot maze solving abilities, significantly outperforming\\nVeo2whichoftenproducesillegalmoves. Forinstance, inthe5 √ó5grids, Veo3achievesa pass@10rate\\nof 78% compared to Veo 2‚Äôs 14%. The consistent performance gap highlights the advancing reasoning\\ncapabilities of the newer model. While Nano Banana matches or surpasses Veo 3‚Äôs performance on\\nrectangular mazes, it fails to solve irregular mazes entirely. Similarly, Gemini 2.5 Pro outperforms\\nVeo 3 on small mazes when given an ASCII representation of the maze (T2T), but falls behind on 9√ó9\\nmazes, and generally struggles when the maze is represented as image (as opposed to text) input.\\nBoth comparisons highlight the advantages of solving a visual task step-by-step in a visual medium.\\n4.6. Reasoning: Visual symmetry solving\\nCompleting a pattern to be symmetrical assesses the ability to understand and apply spatialreasoning.\\nWe create a custom dataset of shapes (e.g., heart, letters) and random patterns (see Sec. B.6 for\\ndetails). Fig. 8 shows that Veo 3 outperforms Veo 2 and Nano Banana by a large margin. We also\\nuse this task to systematically analyze how different prompts affect performance in Sec. C: The\\npass@1 difference between best and worst prompt is 40 percentage points on the shape split and 64\\npercentage points on the random split.\\n4.7. Reasoning: Visual analogy completion\\nVisual analogies test a model‚Äôs ability to understand transformations and relationships between\\nobjects, a form of abstractreasoning. Concretely, we prompt the model to fill the missing quadrant\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 8, 'page_label': '9', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nof a 2√ó2 grid to complete the analogy (e.g., A is to B as C is to ?). We evaluate the correctness of the\\ngenerated infill for four transformation types from KiVA [66], see Sec. B.7 for details. The results are\\nsummarized in Fig. 9. While Veo 2 struggles to understand any analogies, Veo 3 correctly completes\\nexamples forcolorandresize. However, both models perform below chance (0.33) onreflectand\\nrotateanalogies, indicating an erroneous, systematic bias.\\nTakeaway 4While far from perfect, Veo 3‚Äîbuilding on its ability to perceive, model and\\nmanipulate objects‚Äîshows emergent visual reasoning abilities.\\n5. Discussion\\nSummaryA breakthrough in machine vision started the deep learning revolution in 2012 [67], but\\nin recent years, natural language processing has seen the most rapid progress. This was driven by\\nthe rise of general-purpose LLMs, whose ability to solve novel tasks in a zero-shot fashion has led\\nthem to replace most task-specific models in NLP. We here make the case that machine vision is on\\nthe cusp of a similar paradigm shift, enabled by emergent abilities of large-scale video models. Our\\ncore finding is that Veo 3 can solve a wide range of tasks in a zero-shot manner, spanning the full\\nvision stack fromperceptiontomodeling,manipulationand even early forms ofvisual reasoning.\\nWhile its performance is not yet perfect, the massive and consistent improvement from Veo 2 to Veo 3\\nindicates that video models will become general-purpose foundation models for vision, just as LLMs\\nhave for language.\\nPerformance is a lower boundTasks can be represented in a myriad of ways; a maze, for example,\\ncan be presented as a black-and-white grid, a video game, or a photorealistic scene, with the prompt\\nrequesting a solution in the form of a line, a moving object, or a glowing path. Moreover, visually, a\\nmaze could be represented as a black-and-white grid, a Pac-Man game, or a photorealistic top-down\\nview of an apartment. This has three implications: First, prompt engineering‚Äîincluding thevisual\\nprompt a.k.a. starting frame‚Äîis as important for visual tasks as it is for LLMs (see also Sec. C and\\n[68] for a discussion). Second, we must distinguish between a model‚Äôs task performance and its\\nunderlying ability (i.e., competence) to solve that task [69, 70]. Third, as a consequence, the model\\nperformance reported here with a given visual and textual prompt should be considered a lower\\nbound on the model‚Äôs true capabilities. This also holds for the tasks that we report as failure cases\\nin Sec. D, such as providing visual instructions to fold laundry (Fig. 76), planning to move a sofa\\nbetween rooms separated by a small door (Fig. 77), or certain visual puzzles (Fig. 70).\\nVideo generation is expensive, but costs tend to fallWhile generating a video is currently more\\nexpensive than running a bespoke, task-specific model, the economics of general-purpose models\\nare on a predictable trajectory. Epoch AI [71] estimates that LLM inference costs are falling by a\\nfactor of 9√óto 900√óper year for a given performance level. In NLP, early generalist models were\\nalso considered prohibitively expensive (‚ÄúGPT-3‚Äôs size makes it challenging to deploy‚Äù [7, p. 8]).\\nNevertheless, rapidly falling inference costs, combined with the appeal of generalist models, have\\nreplaced most task-specific language models. If NLP is a guide, the same trend will play out in vision.\\nJack of many trades, master of few?For many tasks, Veo 3‚Äôs performance is below state of the art\\nof specialized models. This mirrors the early days of LLMs; GPT-3 reported performance well below\\nfine-tuned models on many tasks [7, cf. Tables 3.1, 3.3, 3.4, 3.5]). This did not stop language models\\nfrom becoming foundation models, and we don‚Äôt believe it will stop video models from becoming\\nvision foundation models for two reasons. First, the step-change in performance from Veo 2 to Veo 3\\nis evidence of rapid progress over time. Second, our scaling results from Sec. 4 showpass@10 to\\nbe consistently higher thanpass@1 with no signs of a plateau. Therefore, inference-time scaling\\nmethods [e.g.72‚Äì75] in combination with the standard optimization toolkit like post-training with\\nautomatic verifiers are likely to boost performance. For the tasks we test here, Veo 3 is akin to a\\npre-trained language model that has yet to undergo instruction tuning or RLHF [76, 77].\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 9, 'page_label': '10', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nOutlookThis is an exciting time for vision. Seeing NLP‚Äôs recent transformation from task-specific\\nto generalist models, it is conceivable that the same transformation will happen in machine vision\\nthrough video models (a ‚ÄúGPT-3 moment for vision‚Äù), enabled by their emergent ability to perform a\\nbroad variety of tasks in a zero-shot fashion, from perception to visual reasoning.\\nAcknowledgements\\nWe would like to thank Oyvind Tafjord, Mike Mozer, Katherine Hermann, Andrew Lampinen, Vior-\\nica Patraucean, Shiry Ginosar, Ross Goroshin, Abhijit Ogale, Claire Cui, Kun Zhang for helpful\\nfeedback/discussions, Anish Nangia for Vertex insights, Tuan Anh Le for suggesting the Omniglot eval-\\nuation, Medhini Narasimhan and Pieter-Jan Kindermans for Veo insights and discussion, Micromelon\\nRobotics for permission to use the image in Fig. 58, Shelly Sheynin for permission to use the EMU Edit\\ndataset, David Fleet and Jon Shlens for their support, and the Veo team for developing an incredible\\nmodel.\\nAuthor contributions\\nLeads: TW, PJ, RG. Project idea: RG. Core contributors: YL, PV. Partial contributor: SG. Fig. 38: NM.\\nAdvisors: KS, BK.\\nReferences\\n[1] Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. A survey on large language\\nmodels for code generation.arXiv preprint arXiv:2406.00515, 2024.\\n[2] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic\\ncapabilities.arXiv preprint arXiv:2507.06261, 2025.\\n[3] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao,\\nChunzhao Xie, Chuou Xu, Jihong Dai, et al. Weaver: Foundation models for creative writing.\\narXiv preprint arXiv:2401.17268, 2024.\\n[4] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun\\nChen, and Lei Li. Multilingual machine translation with large language models: Empirical\\nresults and analysis.arXiv preprint arXiv:2304.04675, 2023.\\n[5] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scien-\\ntist: Towards fully automated open-ended scientific discovery.arXiv preprint arXiv:2408.06292,\\n2024.\\n[6] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu,\\nMichael Moor, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research\\nassistants.arXiv preprint arXiv:2501.04227, 2025.\\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners.Advances in neural information processing systems, 33:1877‚Äì1901, 2020.\\n[8] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\\nlanguage models.arXiv preprint arXiv:2206.07682, 2022.\\n[9] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu,\\nZhiyong Wu, Tianyu Liu, et al. A survey on in-context learning.arXiv preprint arXiv:2301.00234,\\n2022.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 10, 'page_label': '11', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\n[10] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners.Advances in neural information processing systems, 35:\\n22199‚Äì22213, 2022.\\n[11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In\\nProceedings of the IEEE/CVF international conference on computer vision, pages 4015‚Äì4026, 2023.\\n[12] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma,\\nHaitham Khedr, Roman R√§dle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything\\nin images and videos.arXiv preprint arXiv:2408.00714, 2024.\\n[13] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\\nreal-time object detection. InProceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 779‚Äì788, 2016.\\n[14] Rahima Khanam and Muhammad Hussain. Yolov11: An overview of the key architectural\\nenhancements.arXiv preprint arXiv:2410.17725, 2024.\\n[15] Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre\\nAlahi, and Paolo Favaro. From generation to generalization: Emergent few-shot learning in\\nvideo diffusion models.arXiv preprint arXiv:2506.07280, 2025.\\n[16] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\\nSavarese. Taskonomy: Disentangling task transfer learning. InProceedings of the IEEE conference\\non computer vision and pattern recognition, pages 3712‚Äì3722, 2018.\\n[17] Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong Mao. Realgeneral: Unifying visual\\ngeneration via temporal in-context learning with video models.arXiv preprint arXiv:2503.10406,\\n2025.\\n[18] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming\\nCheng. Visualcloze: A universal image generation framework via visual in-context learning.\\narXiv preprint arXiv:2504.07960, 2025.\\n[19] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images:\\nA generalist painter for in-context visual learning. InProceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pages 6830‚Äì6839, 2023.\\n[20] Jiahao Xie, Alessio Tonioni, Nathalie Rauschmayr, Federico Tombari, and Bernt Schiele. Test-\\ntime visual in-context tuning. InProceedings of the Computer Vision and Pattern Recognition\\nConference, pages 19996‚Äì20005, 2025.\\n[21] Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Huan Teng,\\nJunlin Xie, Yu Qiao, Peng Gao, et al. Pixwizard: Versatile image-to-image visual assistant with\\nopen-language instructions.arXiv preprint arXiv:2409.15278, 2024.\\n[22] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan\\nLi, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In\\nProceedings of the Computer Vision and Pattern Recognition Conference, pages 13294‚Äì13304,\\n2025.\\n[23] Duong H Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt,\\nRanjay Krishna, and Jiasen Lu. One diffusion to generate them all. InProceedings of the Computer\\nVision and Pattern Recognition Conference, pages 2671‚Äì2682, 2025.\\n[24] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv\\nLeviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors.arXiv\\npreprint arXiv:2302.01329, 2023.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 11, 'page_label': '12', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\n[25] RahulRavishankar,ZeeshanPatel,JathushanRajasegaran,andJitendraMalik. Scalingproperties\\nof diffusion models for perceptual tasks. InProceedings of the Computer Vision and Pattern\\nRecognition Conference, pages 12945‚Äì12954, 2025.\\n[26] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter\\nAbbeel, and Dale Schuurmans. Video as the new language for real-world decision making.arXiv\\npreprint arXiv:2402.17139, 2024.\\n[27] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances\\nin neural information processing systems, 35:24824‚Äì24837, 2022.\\n[28] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\\nand Jimmy Ba. Large language models are human-level prompt engineers. InThe eleventh\\ninternational conference on learning representations, 2022.\\n[29] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\\nprocessing.ACM computing surveys, 55(9):1‚Äì35, 2023.\\n[30] Google Cloud. Vertex AI Veo Prompt Rewriter.https://cloud.google.com/vertex-ai/\\ngenerative-ai/docs/video/turn-the-prompt-rewriter-off#prompt-rewriter ,\\n2025. Accessed: September 22, 2025.\\n[31] LMSYS ORG. Lmsys org text-to-video leaderboard.https://lmarena.ai/leaderboard/t\\next-to-video, September 2025. Accessed: 2025-09-23.\\n[32] Google. Veo 2 announcement.https://blog.google/technology/google-labs/vide\\no-image-generation-update-december-2024/, 2024. Accessed: September 22, 2025.\\n[33] Google. Veo 2 launch.https://developers.googleblog.com/en/veo-2-video-gen\\neration-now-generally-available/, 2025. Accessed: September 22, 2025.\\n[34] Google. Veo 3 announcement.https://blog.google/technology/ai/generative-m\\nedia-models-io-2025/, 2025. Accessed: September 22, 2025.\\n[35] Google. Veo 3 launch.https://cloud.google.com/blog/products/ai-machine-l\\nearning/veo-3-fast-available-for-everyone-on-vertex-ai , 2025. Accessed:\\nSeptember 22, 2025.\\n[36] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. InProceedings of the IEEE\\ninternational conference on computer vision, pages 1395‚Äì1403, 2015.\\n[37] Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, V√©ronique\\nIzard, and Emmanuel Dupoux. IntPhys: A framework and benchmark for visual intuitive physics\\nreasoning.arXiv preprint arXiv:1803.07616, 2018.\\n[38] Daniel M. Bear, Elias Wang, Damian Mrowca, Felix J. Binder, Hsiao-Yu Fish Tung, R. T. Pramod,\\nCameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, Li Fei-Fei, Nancy Kanwisher, Joshua B.\\nTenenbaum, Daniel L. K. Yamins, and Judith E. Fan. Physion: Evaluating physical prediction\\nfrom vision in humans and machines, 2021.\\n[39] Luca Weihs, Amanda Yuile, Ren√©e Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi,\\nand Aniruddha Kembhavi. Benchmarking progress to infant-level physical reasoning in ai.\\nTransactions on Machine Learning Research, 2022.\\n[40] Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, and Elia Bruni.\\nGRASP: A novel benchmark for evaluating language grounding and situated physics understand-\\ning in multimodal language models.arXiv preprint arXiv:2311.09048, 2023.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 12, 'page_label': '13', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\n[41] Hsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Josh Tenenbaum, Dan\\nYamins, Judith Fan, and Kevin Smith. Physion++: Evaluating physical scene understanding\\nthat requires online inference of different physical properties.Advances in Neural Information\\nProcessing Systems, 36, 2024.\\n[42] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chen-\\nfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical\\ncommonsense for video generation, 2024.\\n[43] Anoop Cherian, Radu Corcodel, Siddarth Jain, and Diego Romeres. LLMPhy: Complex physical\\nreasoning using large language models and world models.arXiv preprint arXiv:2411.08027,\\n2024.\\n[44] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng,\\nDianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-\\nbased benchmark for video generation, 2024.\\n[45] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi\\nFeng. How far is video generation from world model: A physical law perspective.arXiv preprint\\narXiv:2411.02385, 2024.\\n[46] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative\\nvideo models understand physical principles?arXiv preprint arXiv:2501.09038, 2025.\\n[47] Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, and Chang Xu.\\nGenerative physical AI in vision: A survey.arXiv preprint arXiv:2501.10928, 2025.\\n[48] Luca M Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in\\nmultimodal large language models.Nature Machine Intelligence, pages 1‚Äì11, 2025.\\n[49] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios\\nVozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios\\nGavves. Morpheus: Benchmarking physical reasoning of video generative models with real\\nphysical experiments.arXiv preprint arXiv:2504.02918, 2025.\\n[50] Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael\\nRabbat, Emmanuel Dupoux, and Yann LeCun. Intuitive physics understanding emerges from\\nself-supervised pretraining on natural videos.arXiv preprint arXiv:2502.11831, 2025.\\n[51] Anand Bhattad, Konpat Preechakul, and Alexei A Efros. Visual jenga: Discovering object\\ndependencies via counterfactual inpainting.arXiv preprint arXiv:2503.21770, 2025.\\n[52] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept\\nlearning through probabilistic program induction.Science, 350(6266):1332‚Äì1338, 2015.\\n[53] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley,\\nAmmar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-JEPA 2: Self-supervised\\nvideo models enable understanding, prediction and planning.arXiv preprint arXiv:2506.09985,\\n2025.\\n[54] Google. Nano Banana: Gemini Image Generation Overview.https://gemini.google/ov\\nerview/image-generation/, 2025. Accessed: September 22, 2025.\\n[55] Kevin Clark and Priyank Jaini. Text-to-image diffusion models are zero shot classifiers.Advances\\nin Neural Information Processing Systems, 36:58921‚Äì58937, 2023.\\n[56] Priyank Jaini, Kevin Clark, and Robert Geirhos. Intriguing properties of generative classifiers.\\nInThe Twelfth International Conference on Learning Representations, 2023.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 13, 'page_label': '14', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\n[57] Ryan Burgert, Kanchana Ranasinghe, Xiang Li, and Michael S Ryoo. Peekaboo: Text to image\\ndiffusion models are zero-shot segmentors.arXiv preprint arXiv:2211.13224, 2022.\\n[58] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\\nWang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models\\nare zero-shot video generators. InProceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 15954‚Äì15964, 2023.\\n[59] Xavier Soria, Edgar Riba, and Angel Sappa. Dense extreme inception network: Towards a robust\\nCNN model for edge detection. InThe IEEE Winter Conference on Applications of Computer Vision\\n(WACV ‚Äô20), 2020.\\n[60] Xavier Soria, Angel Sappa, Patricio Humanante, and Arash Akbarinia. Dense extreme inception\\nnetwork for edge detection.Pattern Recognition, 139:109461, 2023. ISSN 0031-3203. doi:\\nhttps://doi.org/10.1016/j.patcog.2023.109461. URLhttps://www.sciencedirect.com/\\nscience/article/pii/S0031320323001619.\\n[61] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance\\nsegmentation. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\n2019.\\n[62] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh,\\nand Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks.\\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n8871‚Äì8879, 2024.\\n[63] Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, and Dacheng Tao. Diffusion model-based video\\nediting: A survey.arXiv preprint arXiv:2407.07111, 2024.\\n[64] Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, and Mohit\\nBansal. VEGGIE:Instructionaleditingandreasoningofvideoconceptswithgroundedgeneration.\\narXiv preprint arXiv:2503.14350, 2025.\\n[65] Noam Rotstein, Gal Yona, Daniel Silver, Roy Velich, David Bensaid, and Ron Kimmel. Pathways\\non the image manifold: Image editing via video generation. InProceedings of the Computer\\nVision and Pattern Recognition Conference, pages 7857‚Äì7866, 2025.\\n[66] Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Wong, Yutong Bai, Shiry Ginosar, Alison\\nGopnik, and Kate Saenko. Kiva: Kid-inspired visual analogies for testing large multimodal\\nmodels.arXiv preprint arXiv:2407.17773, 2024.\\n[67] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep\\nconvolutional neural networks.Advances in neural information processing systems, 25, 2012.\\n[68] Andrew Kyle Lampinen, Stephanie CY Chan, Aaditya K Singh, and Murray Shanahan. The\\nbroader spectrum of in-context learning.arXiv preprint arXiv:2412.03782, 2024.\\n[69] Chaz Firestone. Performance vs. competence in human‚Äìmachine comparisons.Proceedings of\\nthe National Academy of Sciences, 117(43):26562‚Äì26571, 2020.\\n[70] Robert Geirhos, J√∂rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,\\nMatthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks.Nature\\nMachine Intelligence, 2(11):665‚Äì673, 2020.\\n[71] Ben Cottier, Ben Snodin, David Owen, and Tom Adamczewski. LLM inference prices have fallen\\nrapidly but unequally across tasks, march 2025. URLhttps://epoch.ai/data-insights/\\nllm-inference-price-trends. Accessed: 2025-09-12.\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 14, 'page_label': '15', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\n[72] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\\nmodels.arXiv preprint arXiv:2203.11171, 2022.\\n[73] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\\nwith self-feedback.Advances in Neural Information Processing Systems, 36:46534‚Äì46594, 2023.\\n[74] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. OpenAI o1 system card.arXiv preprint\\narXiv:2412.16720, 2024.\\n[75] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute\\noptimally can be more effective than scaling model parameters.arXiv preprint arXiv:2408.03314,\\n2024.\\n[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\\nfollow instructions with human feedback.Advances in neural information processing systems, 35:\\n27730‚Äì27744, 2022.\\n[77] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\\nwith GPT-4.arXiv preprint arXiv:2304.03277, 2023.\\n[78] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient\\nregularized deep retinex network for robust low-light image enhancement.IEEE Transactions\\non Image Processing, 30:2072‚Äì2086, 2021.\\n[79] Declan Campbell, Sunayana Rane, Tyler Giallanza, Camillo Nicol√≤ De Sabbata, Kia Ghods,\\nAmogh Joshi, Alexander Ku, Steven Frankland, Tom Griffiths, Jonathan D Cohen, et al. Under-\\nstanding the limits of vision language models through the lens of the binding problem.Advances\\nin Neural Information Processing Systems, 37:113436‚Äì113460, 2024.\\n[80] R. C. James. Sight for sharp eyes.LIFE, 58(7):120, 1965.\\n[81] Richard Langton Gregory. The intelligent eye, 1970.\\n[82] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and\\nWieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias\\nimproves accuracy and robustness. InInternational conference on learning representations, 2019.\\n[83] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan\\nFan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of\\n10m+ 3d objects.Advances in Neural Information Processing Systems, 36:35799‚Äì35813, 2023.\\n[84] Fran√ßois Chollet. On the measure of intelligence.arXiv preprint arXiv:1911.01547, 2019.\\n[85] Piotr Doll√°r and C. Lawrence Zitnick. Structured forests for fast edge detection. InICCV, 2013.\\n[86] Piotr Doll√°r and C. Lawrence Zitnick. Fast edge detection using structured forests.ArXiv, 2014.\\n[87] C. Lawrence Zitnick and Piotr Doll√°r. Edge boxes: Locating object proposals from edges. In\\nECCV, 2014.\\n[88] Kai Leng, Zhijie Zhang, Jie Liu, Zeyd Boukhers, Wei Sui, Cong Yang, and Zhijun Li. Superedge:\\nTowards a generalization model for self-supervised edge detection.CoRR, 2024.\\n[89] Michael Ivanitskiy. Maze dataset.https://pypi.org/project/maze-dataset/0.3.4/,\\n2025. Accessed: June 31, 2025.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 15, 'page_label': '16', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\n[90] Yujin Jeong, Arnas Uselis, Seong Joon Oh, and Anna Rohrbach. Diffusion classifiers understand\\ncompositionality, but conditions apply.arXiv preprint arXiv:2505.17955, 2, 2025.\\n[91] NateGillman,CharlesHerrmann,MichaelFreeman,DakshAggarwal,EvanLuo,DeqingSun,and\\nChen Sun. Force prompting: Video generation models can learn and generalize physics-based\\ncontrol signals, 2025. URLhttps://arxiv.org/abs/2505.19386.\\n[92] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff,\\nTatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang,\\nAndrew Owens, and Deqing Sun. Motion prompting: Controlling video generation with motion\\ntrajectories.arXiv preprint arXiv:2412.02700, 2024.\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 16, 'page_label': '17', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nAppendix\\nA. Qualitative results:\\nPerception, Modeling, Manipulation, Reasoning\\nA.1. Perception\\nFigure 10|Edge detection. Prompt:‚ÄúAll edges in this image become more salient by transforming into\\nblack outlines. Then, all objects fade away, with just the edges remaining on a white background. Static\\ncamera perspective, no zoom or pan.‚ÄùSuccess rate: 0.92.\\nFigure 11|Segmentation. Prompt:‚ÄúCreate an animation of instance segmentation being performed on\\nthis photograph: each distinct entity is overlaid in a different flat color [...]‚Äù(full prompt: Sec. B.2).\\nSuccess rate: 0.33.\\nFigure 12|Keypoint localization. Prompt:‚ÄúAdd a bright blue dot at the tip of the branch on which\\nthe macaw is sitting. The macaw‚Äôs eye turns bright red. Everything else turns pitch black. Static camera\\nperspective, no zoom or pan.‚ÄùSuccess rate: 0.58.\\nFigure13 |Super-resolution. Prompt:‚ÄúPerformsuperresolutiononthisimage. Staticcameraperspective,\\nno zoom or pan.‚ÄùSuccess rate: 0.75.\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 17, 'page_label': '18', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 14|Blind deblurring. Prompt:‚ÄúUnblur image including background. Static camera perspective,\\nno zoom or pan.‚ÄùSuccess rate: 1.0.\\nFigure 15|Blind denoising. Each quadrant was corrupted with a different type of noise. Clockwise\\nfrom top left: Gaussian noise, salt-and-pepper noise, speckle noise, shot noise. Prompt:‚ÄúRemove the\\nnoise from this image. Static camera perspective, no zoom or pan.‚ÄùSuccess rate: 1.0.\\nOriginal low-light image\\n Veo 3-generated lit image\\n Ground-truth lit image\\nFigure 16 |Low-light enhancing. Prompt:‚ÄúFully restore the light in this image. Static camera\\nperspective, no zoom or pan.‚ÄùSuccess rate: 0.92. Image and ground-truth source: LOLv2 dataset [78].\\nFigure 17|Conjunctive search / binding problem. Prompt:‚ÄúThe blue ball instantly begins to glow.\\nStatic camera perspective, no zoom no pan no movement no dolly no rotation.‚ÄùSuccess rate: 0.75.\\nInspiration: [79].\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 18, 'page_label': '19', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 18|Dalmatian illusion understanding. Prompt:‚ÄúStatic camera perspective.‚ÄùSuccess rate: 1.0.\\nImage credit: [80, 81].\\nFigure 19|Shape (cue-conflict) understanding. Prompt:‚ÄúTransform the animal in this image into a\\nsketch of the animal surrounded by its family.‚ÄùSuccess rate: 1.0. Image credit: [82].\\nFigure 20|Rorschach blot interpretation. Prompt:‚ÄúThe patterns transform into objects.‚ÄùSuccess\\nrate: undefined (1.0 for prompt following). Image credit: H. Rorschach, public domain via wikipedia.\\nA.2. Modeling\\nFigure 21|Material properties.Prompt:‚ÄúThe bunsen burner at the bottom turns on. Sped up time\\nlapse. Static camera, no pan, no zoom, no dolly.‚ÄùSuccess rate: 0.25.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 19, 'page_label': '20', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 22|Physics body transform.Rigid body(top). Prompt:‚ÄúA person picks up the vase and\\nputs it back on the table in a sideways orientation. Static camera, no pan, no zoom, no dolly.‚ÄùSuccess\\nrate: 1.0.Soft body(bottom). Prompt:‚ÄúA person drapes a thin silk scarf over the vase. Static camera,\\nno pan, no zoom, no dolly.‚ÄùSuccess rate: 0.67.\\nFigure 23|Gravity and air resistance.On earth(top). Prompt:‚ÄúThe objects fall due to gravity. Static\\ncamera, no pan, no zoom, no dolly.‚ÄùSuccess rate: 0.5.On the moon(bottom). Prompt:‚ÄúThe objects\\nfall down on the moon due to gravity. Static camera, no pan, no zoom, no dolly.‚ÄùSuccess rate: 0.5.\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 20, 'page_label': '21', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 24|Buoyancy.Prompt:‚ÄúThe hand lets go of the object. Static camera, no pan, no zoom, no\\ndolly.‚ÄùSuccess rate (bottle cap): 0.58; success rate (rock): 0.83.\\nFigure 25|Visual Jenga, inspired by [51]. Prompt:‚ÄúA hand quickly removes each of the items in this\\nimage, one at a time.‚ÄùSuccess rate, based on removal of at least three objects: 0.5.\\nFigure 26|Object packing. Prompt:‚ÄúA person puts all the objects that can fit in the backpack inside of\\nit. Static camera, no pan, no zoom, no dolly.‚ÄùSuccess rate: 0.75.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 21, 'page_label': '22', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 27|Material optics.Glass(top). Prompt:‚ÄúA giant glass sphere rolls through the room. Static\\ncamera, no pan, no zoom, no dolly.‚ÄùNote that the image through the glass sphere is inverted. Success\\nrate: 0.92.Mirror(bottom). Prompt:‚ÄúA giant mirror-polish metal sphere rolls through the room. Static\\ncamera, no pan, no zoom, no dolly.‚ÄùNote that the image reflected off the sphere is not inverted).\\nSuccess rate: 1.0.\\nFigure 28|Color mixing.Additive(lights, top). Prompt:‚ÄúThe spotlight on the left changes color to\\ngreen, and the spotlight on the right changes color to blue.‚ÄùSuccess rate: 0.92.Subtractive(paints,\\nbottom). Prompt:‚ÄúA paintbrush mixes these colors together thoroughly until they blend completely.\\nStatic camera, no pan, no zoom.‚ÄùSuccess rate: 0.75.\\nFigure 29|Categorizing objects.Prompt:‚ÄúA person puts all the kids toys in the bucket. Static camera,\\nno pan, no zoom, no dolly.‚ÄùSuccess rate: 0.33.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 22, 'page_label': '23', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 30|Character recognition, generation, and parsing, inspired by the Omniglot dataset [52].\\nRecognition(top). Prompt:‚ÄúThe background of the grid cell with the same symbol as the one indicated\\non the right turns red. All other grid cells remain unchanged. After that, a spinning color wheel appears\\nin the top right corner.‚Äù(Note: Veo 3 has a prior to keep things moving, which is detrimental for tasks\\nwhere the solution is obtained in an early frame. We observe that a ‚Äòmotion outlet‚Äô, such as a color\\nwheel, can indicate task completion and ‚Äòfreeze‚Äô the solution.) Success rate: 0.33.Generation of\\nvariations(middle). Prompt:‚ÄúThe page is filled line-by-line with hand-written practice variations of\\nthe symbol.‚ÄùSuccess rate: 0.25.Parsing into parts(bottom). Color and numbers in final frame are\\nadded post-hoc to show stroke order. Prompt:‚ÄúStroke-by-stroke, a replica of the symbol is drawn on\\nthe right.‚ÄùSuccess rate: 0.5.\\nFigure 31|Memory of world states. Prompt:‚ÄúThe camera zooms in to give a close up of the person\\nlooking out the window, then zooms back out to return to the original view.‚ÄùSuccess rate: 1.0.\\nA.3. Manipulation\\nFigure 32|Backgroundremoval. Prompt:‚ÄúThe background changes to white. Static camera perspective,\\nno zoom or pan.‚ÄùSuccess rate: 0.83.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 23, 'page_label': '24', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 33|Style transfer. Prompt:‚ÄúThe scene transforms into the style of a Hundertwasser painting,\\nwithout changing perspective or orientation; the macaw does not move. Static camera perspective, no\\nzoom or pan.‚ÄùSuccess rate: 0.75.\\nFigure 34|Colorization. Prompt:‚ÄúPerform colorization on this image. Static camera perspective, no\\nzoom or pan.‚ÄùSuccess rate: 0.08.\\nFigure 35 |Inpainting. Prompt:‚ÄúThe white triangles become smaller and smaller, then disappear\\naltogether. Static camera perspective, no zoom or pan.‚ÄùSuccess rate: 1.0.\\nFigure 36|Outpainting. Prompt:‚ÄúRapidly zoom out of this static image, revealing what‚Äôs around it.\\nThe camera just zooms back, while the scene itself and everything in it does not move or change at all, it‚Äôs\\na static image.‚ÄùSuccess rate: 1.0.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 24, 'page_label': '25', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 37|Text manipulation. Prompt:‚ÄúAnimation of the text rapidly changing so that it is made out\\nof different types of candy (top left text) and pretzel sticks (bottom right text). Static camera perspective,\\nno zoom or pan.‚ÄùSuccess rate: 0.33.\\nFigure 38|Image editing with doodles. Prompt:‚ÄúChanges happen instantly.‚ÄùSuccess rate: 1.0.\\nFigure 39|Scene composition. Prompt:‚ÄúA smooth animation blends the zebra naturally into the scene,\\nremoving the background of the zebra image, so that the angle, lighting, and shading look realistic. The\\nfinal scene perfectly incorporates the zebra into the scene.‚ÄùSuccess rate: 0.75.\\nFigure 40|Single-image novel view synthesis. Prompt:‚ÄúCreate a smooth, realistic animation where\\nthe camera seems to rotate around the object showing the object from all the sides. Do not change\\nanything else. No zoom. No pan.‚ÄùSuccess rate: 0.92. Image source: [83].\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 25, 'page_label': '26', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 41|3D-aware reposing. Prompt:‚ÄúThe knight turns to face to the right and drops on one knee,\\nlifting the shield above his head to protect himself and resting the hilt of his weapon on the ground.‚Äù\\nSuccess rate: 0.83.\\nFigure 42|Transfiguration. Prompt:‚ÄúA magical spell smoothly transforms the structure of the teacup\\ninto a mouse.‚ÄùSuccess rate: 0.17.\\nFigure 43|Professional headshot generation. Prompt:‚ÄúTurn this selfie into a professional headshot\\nfor LinkedIn.‚ÄùSuccess rate: 0.42. Image credit: photo by George Pisarevsky on Unsplash.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 26, 'page_label': '27', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 44|Dexterous manipulation.Jar opening(top). Prompt:‚ÄúUse common sense and have the\\ntwo robot hands attached to robot arms open the jar, like how a human would.‚ÄùSuccess rate: 1.0.\\nThrowing and catching(middle). Prompt:‚ÄúUse common sense and have the two robot hands attached\\nto robot arms throw the ball in the air, the ball goes up off the screen, hands move to positions to catch\\nthe ball, and catch the falling ball, like how a human would.‚ÄùSuccess rate: 1.0.Rotating Baoding\\nballs(bottom). Prompt:‚ÄúA human hand holds two metal Baoding balls. The fingers, including the\\nthumb, index, and middle finger, skillfully manipulate the balls, causing them to rotate smoothly like two\\nplanets orbiting around each other and continuously in the palm, one ball circling the other in a fluid\\nmotion.‚ÄùSuccess rate: 0.08.\\nFigure 45|Affordance recognition. Prompt:‚ÄúThe robot hands mounted on robot arms pick up the\\nhammer, naturally like how a human would.‚ÄùSuccess rate: 0.5.\\nFigure 46|Drawing. Prompt:‚ÄúA person draws a square. Static camera, no pan, no zoom, no dolly.‚Äù\\nSuccess rate: 0.33.\\n27'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 27, 'page_label': '28', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 47|Visual instruction generation. Prompt:‚ÄúA montage clearly showing each step to roll a\\nburrito.‚ÄùSuccess rate: 0.25. Inspiration: [26] and Reddit.\\nA.4. Reasoning\\nFigure 48|Graph traversal. Prompt:‚ÄúStarting from the blue well, an unlimited supply of blue water\\nmoves through the connected channel system without spilling into the black area.‚ÄùSuccess rate: 0.08.\\nFigure 49 |Tree BFS. Prompt:‚ÄúFrom the blue water basin, an unlimited supply of water flows at\\nconstant speed into the cave system until all caves are filled. Static camera perspective, no zoom or pan.‚Äù\\nSuccess rate: 0.17.\\nFigure 50|Sequence completioninspired by Raven‚Äôs progressive matrices. Each of the four pairs\\nshows input (left) and generated output (right). Prompt:‚ÄúDraw the figure that completes the pattern\\nin the rightmost box. The images in the boxes are static. Do not modify the existing images, only draw in\\nthe empty box. Static camera, no zoom, no pan, no dolly.‚ÄùSuccess rate: 0.33 for dots, 1.0 for arrows,\\n0.75 for shrinking circles, 0.83 for growing squares.\\n28'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 28, 'page_label': '29', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 51|Connecting colors. Prompt:‚ÄúDraw three curves, one connecting each pair of circles of the\\nsame color.‚ÄùSuccess rate: 0.25.\\nFigure 52|Shape fitting. Prompt:‚ÄúThe scene shows three colored pieces, and a wooden panel with\\nthree holes. Each colored piece fits into one and only one hole. A hand grabs each colored piece and puts\\nit into an empty hole that has the exact same shape - if it doesn‚Äôt fit, the hand tries another hole. All the\\nobjects must be placed in their respective holes.‚ÄùSuccess rate: 0.25.\\nFigure 53|Sorting numbers. Prompt:‚ÄúThe video starts with some numbered bubbles. The bubbles pop\\nand disappear one at a time, in numeric order, starting from the one with the smallest number.‚ÄùSuccess\\nrate: 0.08.\\nFigure 54|Tool use. Prompt:‚ÄúA person retrieves the walnut from the aquarium.‚ÄùSuccess rate: 0.92\\n(retrieval via tool) and 0.08 (retrieval via tool without intersecting the glass).\\nFigure 55|Simple Sudoku completion. Prompt:‚ÄúCreate a static, smooth, animation that solves the\\ngiven 4x4 sudoku. Enter the missing numbers one by one. Do not change anything else in the picture.\\nOnly fill the numbers in the empty cells so the sudoku is solved properly. A cursor moves and fills the\\ncorrect number in the empty boxes.‚ÄùSuccess rate: 0.67.\\n29'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 29, 'page_label': '30', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 56|Water puzzle solving. Prompt:‚ÄúThe tap is turned on and water starts flowing rapidly\\nfilling the containers. Create a smooth, static animation showing the containers getting filled with water\\nin the correct order.‚Äù(note: not all containers can be filled since some pipes are closed off‚ÄîVeo fills\\nthe correct containers, in the right order.) Success rate: 0.5.\\nFigure 57|Maze solving. Prompt:‚ÄúWithout crossing any black boundary, the grey mouse from the\\ncornerskillfullynavigatesthemazebywalkingarounduntilitfindstheyellowcheese.‚ÄùSuccessrate:0.17.\\nFigure 58|Robot navigation. Prompt:‚ÄúThe robot drives to the blue area. Static camera perspective, no\\nmovement no zoom no scan no pan.‚ÄùSuccess rate: 0.58. Image credit: Micromelon Robotics website\\nwith permission from Tim Hadwen.\\nFigure 59|Rule extrapolationinspired by ARC-AGI [84]. Prompt:‚ÄúModify the lower-right grid to\\nadhere to the rule established by the other grids. You can fill cells, clear cells, or change a cell‚Äôs color. Only\\nmodify the lower-right grid, don‚Äôt modify any of the other grids. Static scene, no zoom, no pan, no dolly.‚Äù\\nSuccess rate: 0.08. While Veo 3 doesn‚Äôt follow the prompt perfectly, the output grid (bottom right) is\\ncompleted correctly.\\n30'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 30, 'page_label': '31', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\n/uni00000038/uni00000056/uni00000059/uni00000049/uni00000004/uni00000034/uni00000053/uni00000057/uni0000004d/uni00000058/uni0000004d/uni0000005a/uni00000049\\n/uni0000002a/uni00000045/uni00000050/uni00000057/uni00000049/uni00000004/uni00000034/uni00000053/uni00000057/uni0000004d/uni00000058/uni0000004d/uni0000005a/uni00000049\\n/uni0000002a/uni00000045/uni00000050/uni00000057/uni00000049/uni00000004/uni00000032/uni00000049/uni0000004b/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049\\n/uni00000038/uni00000056/uni00000059/uni00000049/uni00000004/uni00000032/uni00000049/uni0000004b/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049\\nFigure 60|Graded Veo 3 edge map. While false negatives reflect genuine oversights of Veo 3 (e.g.,\\ncracks in the road, lettering on the car), many false positives correspond to actual image details that\\nseem to be erroneously excluded from the ground truth (e.g., the outline of the trees, the reflection in\\nthe car window, and the tire profiles).\\nB. Quantitative results: experimental details\\nTable 1|Video count breakdown for quantitative tasks. For segmentation,2√ó1splits indicate one\\ntest set with two different background color prompts (white/green). For the prompt sensitivity study\\non the symmetry task (Sec. C),2√ó10splits indicate 2 splits (random/shape) across 10 tested prompt\\nvariations. For the qualitative tasks, we additionally generated 744 videos (62 tasks√ó12 samples).\\nTask Splits Imgs/split Pass@ Video models Total videos\\nEdge 1 50 10 21000\\nSegmentation 2√ó1 50 10 22000\\nObject extraction 1 54 10 21080\\nEditing 1 30 1 260\\nMaze 2√ó4 50 10 28000\\nSymmetry 2 25 10 21000\\nSymmetry prompt analysis 2√ó10 25 1 1500\\nAnalogy 4 50 10 24000\\nTotal17640\\nB.1. Perception: Edge detection\\nWe provide details for the image editing task in Sec. 4.1.\\nEvaluationAs is standard in the literature, we refine and binarize predicted edges and allow\\nfor small local shifts compared to the ground truth [85‚Äì88]. Concretely, we use non-maximum\\nsuppression, then binarize with one of 16 evenly-spaced thresholds, then thin the binary edge map.\\nAt each threshold, we find the optimal mapping between predicted and ground-truth edge pixels\\nwithin a radius of 0.75% of the image diagonal (around 11 pixels). Fig. 60 shows an example rating\\nof a Veo 3-generated edge map. We report the best OIS overùëò attempts (optimal image scale; the\\nmaximumùêπ 1-score over all thresholds) for the best/last frame.\\nDatasetWe used all 50 test images from BIPEDv2 [59, 60].\\nModels & promptsWe tested Veo 3veo-3.0-generate-preview and Veo 2veo-2.0-generate-\\npreview-001 through the Vertex AI API. We also tested Nano Bananagemini-2.5-flash-image-\\npreviewthrough Google AI Studio.\\n31'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 31, 'page_label': '32', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nVeo\\nAll edges in this image become more salient by transforming into black outlines. Then, all objects\\nfade away, with just the edges remaining on a white background. Static camera perspective, no\\nzoom or pan.\\nNano Banana\\nOutline all edges in the image in black, make everything else white.\\nSamplingWe generated 10 videos per sample with a fixed prompt.\\nB.2. Perception: Segmentation\\nWe provide details for the image editing task in Sec. 4.2.\\nEvaluationSince the model is free to choose any colors for segmentation masks, we first determine\\nthenumberandhueofeachmaskbyconsideringthehue-differencehistogrambetweentheoriginalim-\\nageandtheextractedframe. Wesmooththehistogramwith scipy.ndimage.gaussian_filter1d\\nwith a standard deviation of 2. Peaks with a minimum height of 10% of the maximum and at least 10\\nhue steps apart are considered to correspond to predicted segmentation masks. We then map each\\npixel to the mask with the closest hue.\\nContrary to classic instance segmentation [61] or promptable segmentation [11, 12], our prompts\\ndo not specify a class or list of possible classes, a location prior (e.g., point or bounding box), or the\\nnumber of instances in the image. This also means that mapping between predictions and annotated\\ninstances is established. Instead, we pair each ground-truth mask (including the background) with\\nthe predicted mask with the highest IoU (intersection over union), if any. We report mIoU as the\\naverage IoU over all pairs (excluding the background).\\nDatasetWe evaluated on 50 randomly chosen test images from LVIS [61] that contain one to three\\nobjects, each with at least 5000 pixels.\\nModels & promptsWe tested Veo 3veo-3.0-generate-preview and Veo 2veo-2.0-generate-\\npreview-001 through the Vertex AI API. We also tested Nano Bananagemini-2.5-flash-image-\\npreviewthrough Google AI Studio.\\n32'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 32, 'page_label': '33', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nVeo\\nCreate an animation of instance segmentation being performed on this photograph: each distinct\\nentity is overlaid in a different flat color.\\nScene:\\n‚Ä¢The animation starts from the provided, unaltered photograph.\\n‚Ä¢The scene in the photograph is static and doesn‚Äôt move.\\n‚Ä¢First, the background fades to {white, green}.\\n‚Ä¢Then, the first entity is covered by a flat color, perfectly preserving its silhouette.\\n‚Ä¢ Then the second entity, too, is covered by a different flat color, perfectly preserving its\\nsilhouette.\\n‚Ä¢One by one, each entity is covered by a different flat color.\\n‚Ä¢Finally, all entities are covered with different colors.\\nCamera:\\n‚Ä¢Static shot without camera movement.\\n‚Ä¢No pan.\\n‚Ä¢No rotation.\\n‚Ä¢No zoom.\\n‚Ä¢No glitches or artifacts.\\nNano Banana\\nPerform instance segmentation on this image: Mask each distinct entity in a different opaque\\nflat color that only preserves the silhouette and turn the background green.\\nSamplingWe generated 10 videos per sample and prompt.\\nB.3. Manipulation: Object extraction\\nWe provide details for the image editing task in Sec. 4.3.\\nEvaluationWe extract the last frame from each generated video; the resulting image is converted to\\ngreyscale, a binary mask with threshold 200 is applied, and the number of connected components is\\nextracted usingscipy.ndimage.label, resulting in the count estimate. We also report the chance\\nbaseline which can be calculated as:random‚àíchance= 1 ‚àí(1 ‚àíùëù)ùëò where ùëùis the probability to get\\nthe count correct via guessing (here:ùëù=1\\n9 ) andùëò‚àà[1,10].\\nDatasetWe generated an animal counting dataset using Nano Banana. Starting from a white 16:9\\nimage, we used the following prompt, wherenumber is in[1,9]and animal is in [‚Äòdog‚Äô, ‚Äòelephant‚Äô,\\n‚Äòcat‚Äô, ‚Äòbrown bear‚Äô, ‚Äòhorse‚Äô, ‚Äòrabbit‚Äô, ‚Äòraccoon‚Äô]. We manually evaluated the generated dataset for\\ncorrectness; the resulting dataset has 54 images (exactly 6 per count).\\nNano Banana\\nExchange the white space with a realistic photograph of: exactly {number} {animal}, outside, not\\noverlapping, in a natural landscape.\\nModels & promptsWe tested Veo 3veo-3.0-generate-preview and Veo 2veo-2.0-generate-\\npreview-001through the Vertex AI API.\\n33'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 33, 'page_label': '34', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nVeo\\nThe background changes to white. Then:\\n‚Ä¢ If there is just a single animal: the animal sits in the middle of the image, looking straight at\\nthe camera.\\n‚Ä¢ If there are multiple animals: all animals line up in a row, with ample white space between\\nthem.\\nSamplingWe generated 10 videos per sample with a fixed prompt.\\nB.4. Manipulation: Image editing\\nWe provide details for the image editing task in Sec. 4.4.\\nEvaluationWe perform a human study with three human raters to evaluatefidelity(correct edit)\\nandprecision(correct edit with no unintended changes like zooming).\\nDatasetWe used a random sample of 30 images from the test set of the Emu-edit dataset [62].\\nModels & promptsWe tested Veo 3veo-3.0-generate-preview and Veo 2veo-2.0-generate-\\npreview-001through the Vertex AI API.\\nVeo\\nCreate a smooth, static animation that slowly {image specific edit direction}. Do not change\\nanything else. No zoom, no pan, no dolly.\\nSamplingFor each image, we generated two samples and use the first sample for human rating.\\nB.5. Reasoning: Maze solving\\nWe provide details for the image editing task in Sec. 4.5.\\nEvaluationOur evaluation process is tailored to the model type. For Veo, we analyze the generated\\nvideo frame-by-frame, extracting the path taken by the agent (red circle). We check for any invalid\\nmoves, such as jumping over walls, clipping through boundaries, or any alteration of the goal‚Äôs position.\\nWe report the success rate as the fraction ofùëò attempts where the agent successfully reaches the goal\\n(green circle) without any illegal moves.\\nFor Nano Banana, which generates the full path in one edit, we assess whether the drawn path\\nconnects the start and end points (allowing for minor discontinuities) and crucially, whether it\\nintersects with any maze walls or goes off the valid path.\\nFor Gemini 2.5 Pro with a maze input as an image (I2T) or as ASCII (T2T), we check whether the\\nseries of grid positions represents an uninterrupted path from the start position to the goal.\\nDatasetFor rectangular mazes, we generated 50 random mazes per size usingmaze-dataset [89],\\nbut replacing the square start and end with circles and swapping their colors. We also drew 10\\nirregular mazes by hand and flipped/rotated them to obtain 40 unique samples.\\nModels & promptsWe tested Veo 3veo-3.0-generate-preview and Veo 2veo-2.0-generate-\\npreview-001 through the Vertex AI API. We also tested Nano Bananagemini-2.5-flash-image-\\npreviewand Gemini 2.5 Progemini-2.5-prothrough Google AI Studio.\\n34'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 34, 'page_label': '35', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nVeo\\nCreate a 2D animation based on the provided image of a maze. The red circle slides smoothly along\\nthe white path, stopping perfectly on the green circle. The red circle never slides or crosses into the\\nblack areas of the maze. The camera is a static, top-down view showing the entire maze.\\nMaze:\\n‚Ä¢The maze paths are white, the walls are black.\\n‚Ä¢The red circle moves to the goal position, represented by a green circle.\\n‚Ä¢The red circle slides smoothly along the white path.\\n‚Ä¢The red circle never slides or crosses into the black areas of the maze.\\n‚Ä¢The red circle stops perfectly on the green circle.\\nScene:\\n‚Ä¢No change in scene composition.\\n‚Ä¢No change in the layout of the maze.\\n‚Ä¢The red circle travels along the white path without speeding up or slowing down.\\nCamera:\\n‚Ä¢Static camera.\\n‚Ä¢No zoom.\\n‚Ä¢No pan.\\n‚Ä¢No glitches, noise, or artifacts.\\n35'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 35, 'page_label': '36', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nGemini 2.5 Pro I2T\\nsystem\\nThink step by step as needed and output in xml format:\\n<think>thinking process</think>\\n<final_answer>final answer</final_answer>\\nuser\\nThe following image shows a maze, represented by colored squares:\\n‚Ä¢Black squares represent walls and cannot be passed through.\\n‚Ä¢White squares are empty and can be passed through.\\n‚Ä¢The red square is the starting point.\\n‚Ä¢The green square is the end point.\\nPlease solve the maze by providing a path from the starting point to the end point. The path should\\nbe provided as a list of coordinates of each step, where each coordinate is a (row, col) tuple, and\\nrow, col are 0-based indices. Consider the origin (0, 0) to be the top-left corner. Overall, the path\\nshould be provided in the format of [(row1, col1), (row2, col2), ...].\\nA valid path must:\\n‚Ä¢Start at the starting point (the red square).\\n‚Ä¢End at the end point (the green square).\\n‚Ä¢Avoid the walls (the black squares).\\n‚Ä¢Pass only through empty space (the white squares).\\n‚Ä¢Move one square at a time.\\n‚Ä¢Only move up, down, left, and right, not diagonally.\\nCorrect your answer if you spot any errors.\\nHere is the maze image: {image}\\nNano Banana\\nMark the correct path from the red to the green circle through the maze in blue.\\n36'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 36, 'page_label': '37', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nGemini 2.5 Pro T2T\\nsystem\\nThink step by step as needed and output in xml format:\\n<think>thinking process</think>\\n<final_answer>final answer</final_answer>\\nuser\\nThe following is an ASCII-representation of a maze:\\n‚Ä¢‚Äò#‚Äô represents walls which cannot be passed through.\\n‚Ä¢‚Äò ‚Äô represents empty spaces that can be passed through.\\n‚Ä¢‚ÄòS‚Äô is the starting point.\\n‚Ä¢‚ÄòE‚Äô is the end point.\\nPlease solve the maze by providing a path from the starting point to the end point. The path should\\nbe provided as a list of coordinates of each step, where each coordinate is a (row, col) tuple, and\\nrow, col are 0-based indices. Consider the origin (0, 0) to be the top-left corner. Overall, the path\\nshould be provided in the format of [(row1, col1), (row2, col2), ...].\\nA valid path must:\\n‚Ä¢Start at the starting point ‚ÄòS‚Äô.\\n‚Ä¢End at the end point ‚ÄòE‚Äô.\\n‚Ä¢Avoid the walls ‚Äò#‚Äô.\\n‚Ä¢Pass only through empty space ‚Äò ‚Äô.\\n‚Ä¢Move one square at a time.\\n‚Ä¢Only move up, down, left, and right, not diagonally.\\nCorrect your answer if you spot any errors.\\nHere is the maze in ASCII format: {maze}\\nSamplingWe generated 10 videos per sample with a fixed prompt. Note that for Gemini 2.5 Pro\\nI2T, we represented the maze as a grid where the red and green positions are marked as squares (not\\ncircles) to make the setup grid-like (i.e., a matrix with cells), since this might be easier for a language\\nmodel.\\nB.6. Reasoning: Visual symmetry solving\\nWe provide details for the visual symmetry task in Sec. 4.6.\\nEvaluationWe prompt Veo with input images containing a 10√ó16 grid where a pattern is drawn on\\nthe left half. The goal is to complete the pattern on the empty right half so that the final pattern is\\nsymmetrical along the central vertical axis.\\nWe compare Veo‚Äôs best-frame and last-frame solutions with the ground-truth symmetrical grid\\nand compute the number of incorrectly-colored cells. A cell is determined as incorrectly-colored if\\nthe average color across pixels in the cell is perceptually distinct from the ground-truth average color\\nin the matching cell. We compute perceptual color differences of the average cell color in the CIELAB\\ncolor space, with a difference threshold of 15.0. In Fig. 8, we report the percentage of attempts in\\nwhich the best or last frame solution has zero incorrect cells forùëò=1.\\nDatasetWe created a synthetic grid coloring image dataset to evaluate visual symmetry. We\\ngenerated 25 samples using common symmetrical symbols, objects and shapes such as english letters\\n(e.g., A, H, M, X), geometric shapes (e.g., square, triangle), symmetrical objects (e.g., wineglass,\\n37'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 37, 'page_label': '38', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nMajority vote pass@k %\\nColour\\n Resize\\n Reflect\\n Rotate\\n1 2 3 4 5 6 7 8 9 10\\nk\\n40\\n60\\n80\\n100\\n 98\\n85\\n1 2 3 4 5 6 7 8 9 10\\nk\\n40\\n50\\n60\\n70\\n 68\\n50\\n1 2 3 4 5 6 7 8 9 10\\nk\\n20\\n25\\n30\\n22\\n17\\n1 2 3 4 5 6 7 8 9 10\\nk\\n10\\n15\\n20\\n25\\n30\\n14\\n7\\nModel\\nVeo 3 Veo 2 Chance\\nFigure 61|Visual analogy performance over 10 attempts. In contrast to other plots in this paper,\\nwe here report not the best performance overùëòattempts, but instead the performance when choosing\\nthemajority votefrom ùëò attempts. As a result, performance is not necessarily monotonic inùëò. In\\nfact, forreflectandrotate, performancedecreaseswith ùëò, indicating that both models have systematic,\\nerroneous biases. In the case of Veo 3, the model tends to perform reflections and rotations, but not\\nalong the same axis as shown in the image. Veo 2 simply tends to copy the object without applying\\nany transformation.\\nballoon; together, theshapecondition). We also generated 25 samples consisting of randomly-colored\\ncells (therandomcondition).\\nModels & promptsWe tested Veo 3veo-3.0-generate-preview and Veo 2veo-2.0-generate-\\npreview-001 through the Vertex AI API. We also tested Nano Bananagemini-2.5-flash-image-\\npreviewthrough Google AI Studio.\\nVeo\\nInstantly reflect this pattern along the central, vertical axis while keeping the existing colored\\npattern without modification. Static camera perspective, no zoom or pan.\\nSamplingWe generated 10 videos per sample with a fixed prompt.\\nB.7. Reasoning: Visual analogy completion\\nWe provide details for the visual analogy task in Sec. 4.7.\\nEvaluationWe prompt Veo to solve visual analogies with an input image showing a reference\\nobject pair and a test object. The object images are sourced from the Kid-inspired Visual Analogies\\nbenchmark [KiVA,66]. Consistent with the multi-choice format in the KiVA benchmark, we evaluated\\nVeo‚Äôs generation by cropping out the generated target object in the lower-right region of the last\\nframe and compare Veo‚Äôs generated object with three candidate object choices using an autorater\\n(see details below).\\nIn Fig. 9, we report the pass@1 accuracy across different conditions for both Veo 2 and Veo 3 for\\nùëò=1. Fig. 61 shows performance forùëò=10.\\nDatasetWe used the test trials and choice images from the KiVA benchmark [66].\\n38'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 38, 'page_label': '39', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nModels & promptsWe tested Veo 3veo-3.0-generate-preview and Veo 2veo-2.0-generate-\\npreview-001through the Vertex AI API.\\nWe used Gemini 2.5 Progemini-2.5-pro through Google AI Studio to identify which image\\nchoice Veo‚Äôs generation is most similar with. To enhance the autorater‚Äôs image comparison accuracy\\nfor this task, Gemini is prompted with privileged information about the values in the dataset conditions\\n(see below for the full autorater prompt). If no object is visible in the lower-right region of Veo‚Äôs\\ngenerated last frame or if the generated object is of a different object type, we randomly sampled one\\nof three choices as Veo‚Äôs choice. In pilot experiments, we found that the Gemini-assisted autorater‚Äôs\\nratings achieve above 88% agreement with expert human ratings by the authors on 25 samples within\\neach conditions.\\nNote that in the prompt, words in { } are updated based on the test condition of the current\\ngeneration (one ofcolor,resize,reflect, androtate) to provide more information of the feature name\\nand values to direct the image comparison. Image choice orders are shuffled for each prompt.\\nVeo\\nCreate a smooth animation to generate the missing object in the lower right region and solve the\\nvisual analogy. The original three objects must remain still. Static shot, no zoom no pan no dolly.\\nGemini 2.5 Pro autorater\\nsystem\\nYou are an expert visual judge. You will be presented with a \"target image\" and three \"choice\\nimages\" labeled A, B, and C. Your goal is to identify the choice image that is most visually similar\\nto the target image.\\nFollow these steps:\\n1. Analyze each provided image and describe the objects shown. Focus on the object {color}.\\nThat is, if the objects appear {green}, {blue}, or {red}.\\n2. Determine if the primary object in the target image is of the same general category or type\\nas the objects in the choice images. For example, if the target image shows a dog, and the\\nchoices show a cat, the object types are considered different. If no object is visible in the target\\nimage, the object type is considered to be mismatched.\\n3. If the object type matches between the target image and the choice images, identify the choice\\nthat is most visually similar to the target image in terms of the object {color}.\\nProvide a brief justification for your choice, explaining why it is the best match and why the others\\nare less suitable. Conclude your response with the final answer on a new line in the format:\\n‚ÄúFinal Answer: [answer]‚Äù\\nwhere ‚Äúanswer‚Äù is one of (‚ÄúA‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, or ‚Äúdifferent object type‚Äù). Do not use markdown format\\nfor the final answer line.\\nuser\\nPlease evaluate the following images.\\n‚Äî TARGET IMAGE ‚Äî\\n{target object image}\\n‚Äî CHOICE IMAGES ‚Äî\\nCHOICE A: {image choice} CHOICE B: {image choice} CHOICE C: {image choice}\\nWhich choice image is most similar to the target image?\\nSamplingWe generated 10 videos per sample with a fixed prompt.\\n39'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 39, 'page_label': '40', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nC. Prompting best practices\\nTable 2|Prompt sensitivity study on the visual symmetry task. We report best frame pass@1 %\\nand the average number of incorrectly-colored cells across 25 samples on each split (shape/random).\\nPass@1 Avg incorrect cells\\nNo. Prompt Shape Random Shape Random\\n1 Instantly reflect this pattern along the central, vertical\\naxis while keeping the existing colored pattern without\\nmodification.\\n48 68 4.16 7.00\\n2 Instantly reflect this pattern along the central, vertical\\naxis while keeping the existing colored pattern without\\nmodification. Static camera perspective, no zoom or pan.\\n42 65 5.00 3.52\\n3 Instantly reflect this pattern along the central, vertical\\naxis while keeping the existing colored pattern without\\nmodification. The result needs to be mirror-symmetrical\\nalong the vertical axis. Static camera perspective, no\\nzoom or pan.\\n36 52 6.28 9.04\\n4 One by one, cells in the right half of the grid are filled in\\nto complete the pattern. The pattern is\\nmirror-symmetrical along the central vertical line. Static\\nshot; no zoom, no pan, no dolly.\\n32 12 10.76 14.08\\n5 Reflect this pattern along the central, vertical axis. 28 40 9.76 4.52\\n6 An animation showing the left half of the grid being\\nmirrored onto the right half to create a symmetrical\\npattern. Static shot; no zoom, no pan, no dolly.\\n24 12 10.96 16.32\\n7 You‚Äôre a master symmetry solver. Your task is to fill the\\ncells on the right side of the grid to mirror the pattern on\\nthe left, such that it‚Äôs symmetrical along the vertical axis.\\n24 8 9.20 17.72\\n8 Fill color in the appropriate cells on the right side of the\\ngrid to complete the pattern. The final image should be\\nsymmetrical along the central vertical line. Static shot, no\\nzoom no pan no dolly.\\n13 9 10.30 14.74\\n9 Create a static, smooth, realistic animation completing\\nthe pattern in the image by filling the grid on the right\\nhand side. Do not change anything else. No zoom, no\\npan.\\n12 4 14.88 21.00\\n10 A timelapse of a professional pixel artist drawing a\\nsymmetrical pattern onto a white canvas. Static shot; no\\nzoom, no pan, no dolly.\\n8 20 14.20 12.64\\nThe results in Secs. 3 and 4 are best-effort estimates of Veo‚Äôs performance using carefully chosen\\nprompts. Generally, performance varies greatly with the exact task description provided in the prompt,\\nas illustrated by a prompt sensitivity study on the visual symmetry task in Table 2. Here are best\\npractices from this sensitivity analysis and our other experiments:\\n‚Ä¢ Remove ambiguity. Tasks can be solved in a variety of ways, and natural language descriptions\\ntend to leave a lot of room for interpretation. The goal should be formulated clearly, e.g., saying\\n‚Äúsymmetrical along the central, vertical axis‚Äù, rather than just ‚Äúsymmetrical‚Äù.\\n‚Ä¢ Specify what shouldn‚Äôt change. Veo has a tendency to change any part of the input to create\\ninteresting, dynamic scenes. Including not only a positive task description, but also specifying\\nwhatnotto change can help mitigate this, e.g., ‚Äúkeep the existing colored pattern without\\nmodification‚Äù.\\n‚Ä¢ Providing an outlet. As mentioned above, Veo has a strong prior to keep things moving.\\nProviding a ‚Äúmotion outlet‚Äù in the form of, e.g., a spinning ball can help keep the rest of the\\nscene static.\\n‚Ä¢ Let the model decide when its done. The motion prior also means that Veo often keeps\\n40'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 40, 'page_label': '41', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nmodifying the scene, even after solving the task. Providing a visual indicator, e.g., ‚Äúadd a\\nglowing red dot once the goal is reached‚Äù allows for easy extraction of the solution from the\\ngenerated video.\\n‚Ä¢ Scene and camera controls. Phrases like ‚Äústatic camera, no zoom, no pan, no dolly‚Äù can help\\nkeeping the scene static, e.g., for image-to-image tasks.\\n‚Ä¢ Speed control. Some tasks like maze solving benefit from being solved step-by-step. For other\\ntasks, especially image-to-image tasks, specifying instant changes can help avoid artifacts.\\n‚Ä¢ Realism. Veo was trained to generate plausible, realistic-looking videos. Translating an abstract\\ntask into a realistic setting (including, but not limited to editing the original image to depict\\nrealistic, 3D scenes rather than abstract shapes) can improve generation results. A similar effect\\nwas observed in [90], and we expectvisual prompt engineeringto emerge as a powerful tool for\\nvideo models.\\n41'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 41, 'page_label': '42', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nD. Failure cases\\nFigure 62 |Monocular depth estimation. Prompt:‚ÄúThe image transitions to a depth-map of the\\nscene: Darker colors represent pixels further from the camera, lighter colors represent pixels closer to\\nthe camera. The exact color map to use is provided on the right side of the image. Static scene, no pan,\\nno zoom, no dolly.‚ÄùFailure: Veo 3 seems generally unable to color pixels by depth beyond a binary\\nforeground/background mapping and specifically struggles with using a provided color map.\\nFigure 63|Monocular surface normal estimation. Prompt:‚ÄúThe image transitions to a surface-\\nnormal map of the scene: the red/green/blue color channel specify the direction of the surface-normal at\\neach point, as illustrated on the right side of the image on a sphere. Static scene, no pan, no zoom, no\\ndolly.‚ÄùFailure: While Veo 3 shows some promise in coloring surfaces according to their orientation\\n(e.g., the cube in the front), coloration is inconsistent (compare the two cubes) and doesn‚Äôt correctly\\ninterpolate colors (e.g., for the slope on the triangle).\\nFigure 64|Force & motion prompting, inspired by [91, 92].Force prompting(top). Prompt:‚ÄúThe\\nballs move in the direction indicated by the arrows. Balls without an arrow don‚Äôt move. Static scene, no\\npan, no zoom, no dolly.‚ÄùMotion trajectory prompting(bottom). Prompt:‚ÄúEach car drives out of\\nthe frame following the indicated trajectory. Static camera, no zoom, no pan, no dolly.‚ÄùFailure: Veo 3\\nseems unable to follow force/motion annotations with any consistency. Providing annotations for the\\nfirst frame and letting the model remove them before generating the scene in motion does not work,\\neither.\\n42'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 42, 'page_label': '43', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 65|Tying the knot. Prompt:‚ÄúA knot is tied connecting these two rope ends.‚ÄùFailure: physics\\nviolation, impossible rope movement.\\nFigure 66|Connect the path puzzle. Prompt:‚ÄúThe path connecting the boy to the object starts glowing\\nslowly. Nothing else changes. No zoom, no pan, no dolly.\"Failure: hallucinations, lighting up of all\\npaths.\\nFigure 67|Five letter word search. Prompt:‚ÄúGenerate a static video animation using the provided\\nletter grid. The task is to highlight the only 5-letter English word CHEAT, which may be oriented in any\\ndirection (horizontally, vertically, or diagonally). The animation should consist of a semi-transparent\\nred rectangle with rounded corners smoothly fading into view, perfectly encapsulating the five letters of\\nthe word. The rectangle should have a subtle, soft glow. Do not change anything else in the image. The\\ncamera must remain locked in place with no movement. No zoom, no pan, no dolly.\"Failure: does not\\nrecognize words; highlights individual letters randomly.\\nFigure 68|Eulerian path. Prompt:‚ÄúCreate a smooth animation where a red pen traces all existing\\nedges in a continuous path without lifting the pen. All edges need to be traced. Do not visit any edge twice\\nand do not lift the pen. No zoom, no pan.\"Failure: does not trace the edges exactly, traces non-existent\\nedges.\\n43'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 43, 'page_label': '44', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 69|Solving system of linear equations. Prompt:‚ÄúA hand appears and solves the set of linear\\nequations. It replaces the x, y, z matrix with their correct values that solves the equation. Do not change\\nanything else.\"Failure: hallucinations with text on the blackboard.\\nFigure 70|Spot the difference. Prompt:‚ÄúThere are two images. The left image is different from the\\nright image in 5 spots. Create a static, realistic, smooth animation where a cursor appears and points at\\neach place where the left image is different from the right image. The cursor points one by one and only\\non the left image. Do not change anything in the right image. No pan. No zoom. No movement. Keep the\\nimage static.\"Failure: does not identify all the differences. Hallucinates differences.\\nFigure 71|Visual IQ test. Prompt:‚ÄúCreate a static, smooth, animation that solves the puzzle in the\\ngiven image. The correct pattern should appear at the bottom right to solve the puzzle. Do not change\\nanything else in the picture. No zoom, no pan, no dolly\"Failure: incorrect figure pattern.\\nFigure 72|Glass falling. Prompt:‚ÄúThe object falls. Static camera, no pan, no zoom, no dolly.‚ÄùFailure:\\nphysics violation, glass does not break, and orients itself to be vertical after landing on the floor.\\nFigure 73|Collisions. Prompt:‚ÄúThe two objects collide in slow motion. Static camera, no pan, no zoom,\\nno dolly.‚ÄùFailure: not physically plausible, the objects pause at the moment of impact and then are\\npushed together by an invisible force.\\n44'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 44, 'page_label': '45', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 74|Tiling puzzles.Jigsaw puzzle(top). Prompt:‚ÄúA hand takes the fitting puzzle piece from\\nthe right, rotates it to be in the correct orientation, then puts it into the hole, completing the puzzle.\\nStatic scene, no pan, no zoom, no dolly.‚ÄùFailure: wrong piece orientation.Sliding puzzle(middle).\\nPrompt:‚ÄúSlide the pieces of this sliding puzzle around one-at-a-time until all edges align.‚ÄùFailure:\\ndoesn‚Äôt maintain piece integrity while sliding, hallucinates new pieces.Scrambled puzzle(bottom).\\nPrompt:‚ÄúUnscramble this image.‚ÄùFailure: image details are inconsistent with original pieces.\\nFigure 75|Bottleneck. Prompt:‚ÄúA person tries to put the golf ball in the vase. Static camera, no pan, no\\nzoom, no dolly..‚ÄùFailure: not physically plausible, golf ball is too large to pass through the bottleneck\\nof the vase.\\nFigure 76|Laundry folding. Prompt:‚ÄúGenerate a video of two metal robotic arms properly folding the\\nt-shirt on the table.Failure: physics violation, implausible folding movements.\\n45'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'doi': 'https://doi.org/10.48550/arXiv.2509.20328', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Video models are zero-shot learners and reasoners', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.20328v2', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'page': 45, 'page_label': '46', 'source_file': 'Video_models_ZS_learners_reasoners.pdf', 'file_type': 'pdf'}, page_content='Video models are zero-shot learners and reasoners\\nFigure 77|Motion planning; inspired by the piano mover‚Äôs problem. Prompt:‚ÄúThe red couch slides\\nfrom the left room over into the right room, skillfully maneuvering to fit through the doorways without\\nbumping into the walls. The walls are fixed: they don‚Äôt shift or disappear, and no new walls are introduced.\\nStatic camera, no pan, no zoom, no dolly.‚ÄùFailure: violating rigid-body integrity, not keeping to\\npermissible transformations (rotation, translation).\\nE. LLM use\\nGemini 2.5 Flash and Gemini 2.5 Pro [2] were used for brainstorming task ideas, suggesting related\\nwork that we might have otherwise missed, coding support, and to polish human writing.\\nF. Image sources\\nWhere not stated in the figure caption, images were obtained as follows.\\n‚Ä¢ Figs. 10 to 15, 32 to 38 and 74: The original macaw image was generated with Gemini and,\\ndepending on the figure, subsequently modified by the authors (e.g., conversion to grayscale,\\nadding noise, adding the monkey with Nano Banana).\\n‚Ä¢ Fig. 16: The input image was obtained from here (Apache 2.0 license) based on the LOLv2\\ndataset [78] and randomly selected. The image was slightly cropped to fit a 16:9 aspect ratio.\\n‚Ä¢ Figs. 17, 21 to 24, 26 to 29, 31, 39, 41, 42, 46, 47, 52, 54, 65, 69, 72, 73 and 75 to 77:\\ngenerated with Gemini.\\n‚Ä¢Fig. 25: The input image was obtained from here (CC0 license).\\n‚Ä¢Fig. 30: hand drawn by us, inspired by Fig. 1 of the Omniglot paper [52].\\n‚Ä¢Fig. 40: sample from Objaverse [83]\\n‚Ä¢Figs. 48 to 51, 53, 55 and 57: created by us.\\n‚Ä¢Figs. 56, 66, 67 and 70: original image from Reddit.\\n‚Ä¢Fig. 59: hand drawn by us, inspired by ARC-AGI [84].\\n‚Ä¢Fig. 60: sample from BIPEDv2 [59, 60].\\n‚Ä¢Figs. 62 to 64: generated with Gemini, then annotated by us.\\n‚Ä¢Figs. 68 and 71: hand drawn by us. Inspired by original images from Reddit.\\n‚Ä¢ Figs. 44 and 45: The robot hands are extracted from a frame in this video and were subsequently\\nadapted with Nano Banana. The hands holding Baoding balls were obtained from here.\\n46')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab6b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b75a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 64 documents into 359 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n‚àödk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n‚àödk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1‚àödk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1‚àödk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ‚àà Rdmodel√ódk , WK\\ni ‚àà Rdmodel√ódk , WV\\ni ‚àà Rdmodel√ódv\\nand WO ‚àà Rhdv√ódmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by ‚àödmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ¬∑ d) O(1) O(1)\\nRecurrent O(n ¬∑ d2) O(n) O(n)\\nConvolutional O(k ¬∑ n ¬∑ d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r ¬∑ n ¬∑ d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to 10000 ¬∑ 2œÄ. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ‚àà Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k ¬∑ n ¬∑ d + n ¬∑ d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with Œ≤1 = 0.9, Œ≤2 = 0.98 and œµ = 10‚àí9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d‚àí0.5\\nmodel ¬∑ min(step_num‚àí0.5, step_num ¬∑ warmup_steps‚àí1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 ¬∑ 1020\\nGNMT + RL [38] 24.6 39.92 2.3 ¬∑ 1019 1.4 ¬∑ 1020\\nConvS2S [9] 25.16 40.46 9.6 ¬∑ 1018 1.5 ¬∑ 1020\\nMoE [32] 26.03 40.56 2.0 ¬∑ 1019 1.2 ¬∑ 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 ¬∑ 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 ¬∑ 1020 1.1 ¬∑ 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 ¬∑ 1019 1.2 ¬∑ 1021\\nTransformer (base model) 27.3 38.1 3.3 ¬∑ 1018\\nTransformer (big) 28.4 41.8 2.3 ¬∑ 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value œµls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty Œ± = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop œµls\\ntrain PPL BLEU params\\nsteps (dev) (dev) √ó106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and Œ± = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770‚Äì778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735‚Äì1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832‚Äì841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832‚Äì841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] ≈Åukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313‚Äì330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152‚Äì159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433‚Äì440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433‚Äì440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929‚Äì1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929‚Äì1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440‚Äì2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104‚Äì3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434‚Äì443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ‚àó\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniÔ¨Åe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciÔ¨Åc training strate gies enhance model learn-\\ning eÔ¨Éciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diÔ¨Éculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='example generation to improve the semantic richness and sam ple diÔ¨Éculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task Ô¨Åne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking Ô¨Årst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our Ô¨Åndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs‚Äô generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Our model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiÔ¨Åcantly enhan ced agent systems\\n‚àó https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on Ô¨Åne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiÔ¨Åcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliÔ¨Åed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures‚Äîwhere causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture‚Äîseve ral approaches have'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the limitation of decoder-only architectures‚Äîwhere causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture‚Äîseve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nÔ¨Åes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ‚Äùsignal states‚Äù repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and Ô¨Ånally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe Ô¨Ånal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiÔ¨Åed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='dient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever‚Äôs[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos Ô¨Åltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model‚Äôs semantic understanding, we designed a uniÔ¨Åed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='enhance the model‚Äôs semantic understanding, we designed a uniÔ¨Åed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neÔ¨Écient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiÔ¨Åcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciÔ¨Åc require ments of retrieval,\\nNLI, and classiÔ¨Åcation tasks, enabling eÔ¨Äective feature extractio n from heterogeneous\\ndata sources, signiÔ¨Åcantly beneÔ¨Åting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task‚Äôs charact eristics, optimizing\\nmodel training eÔ¨Éciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ‚ÄùData Grouping Strategy‚Äù, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diÔ¨Éculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the Ô¨Årst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eÔ¨Äectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n‚Ä¢ We propose a uniÔ¨Åed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eÔ¨Éciency in model training ;\\n‚Ä¢ We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniÔ¨Åcantly enhance the quality of training corpora, thereby impro ving model‚Äôs\\nrobustness and generalization capabilities;\\n‚Ä¢ We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='capability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiÔ¨Åcation (CLS), pa ir classiÔ¨Åcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eÔ¨Äectiveness;\\n‚Ä¢ Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eÔ¨Äectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneÔ¨Åt from optimized LLM architectures‚Äîsuc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]‚Äîcombined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model‚Äôs dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by Ô¨Åne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiÔ¨Åcantly fe wer parameters.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='followed by Ô¨Åne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiÔ¨Åcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised Ô¨Åne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciÔ¨Åc Ô¨Åne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diÔ¨Äeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiÔ¨Åcantly enhancing domain data discrimina tion. Xiaobu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='we also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiÔ¨Åcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniÔ¨Åed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diÔ¨Äerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='for improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eÔ¨Äectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiÔ¨Åed prompting strategy by assigning document-s peciÔ¨Åc\\nroles to simulate potential users querying that document, enabling LLMs to generate'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Embedding[34] designs a diversiÔ¨Åed prompting strategy by assigning document-s peciÔ¨Åc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos Ô¨Åltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever‚Äôs strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos Ô¨Ålter ing to eliminate\\nfalse negatives.\\n3 UniÔ¨Åed Multi-task Learning Framework'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos Ô¨Ålter ing to eliminate\\nfalse negatives.\\n3 UniÔ¨Åed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiÔ¨Åcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniÔ¨Åed strategy to eÔ¨Äectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniÔ¨Åed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiÔ¨Åcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciÔ¨Åc losses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciÔ¨Åc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eÔ¨Äectively enhance embedding capa bilities. We propose\\na uniÔ¨Åed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiÔ¨Åcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk‚Äôs components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiÔ¨Åcantly constrainin g tokens‚Äô ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiÔ¨Åcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiÔ¨Åcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce Ô¨Ånal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuÔ¨Écient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n‚Ä¢ Title-Body/Abstract ‚ÄùTitle-Body/Abstract‚Äù type data primarily consists of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='categories of training data used for transformation and their pro cessing procedures:\\n‚Ä¢ Title-Body/Abstract ‚ÄùTitle-Body/Abstract‚Äù type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n‚Ä¢ Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriÔ¨Åcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ‚ÄùSupports‚Äù\\nor ‚ÄùRefutes‚Äù. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ‚ÄùSupports‚Äù, the text is treated as a\\npositive sample; for evidence labeled as ‚ÄùRefutes‚Äù, it is converted in to a negative\\nsample.\\n‚Ä¢ Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ‚ÄùQuestion/Query/User‚Äù portion is converted into que ries, while the\\n‚ÄùAnswer/Response/Assistant‚Äù portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n‚Ä¢ STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='mantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiÔ¨Åcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ‚Äùyes‚Äù/‚Äùtrue‚Äù are mapped to a numerical va lue of 1, while\\n‚Äùno‚Äù/‚Äùfalse‚Äù are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n‚Ä¢ Textual Entailment Textual entailment further examines a model‚Äôs capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiÔ¨Åcation tasks encompass text categorization and sentiment classiÔ¨Åcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diÔ¨Äeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiÔ¨Åcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diÔ¨Äerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category‚Äîretrieval, NLI, and classiÔ¨Åcation‚Äîoperates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eÔ¨Éciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiÔ¨Åcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. SpeciÔ¨Åcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The Ô¨Ånal loss formulation is ex plicitly described'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='additional query-query loss term. SpeciÔ¨Åcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The Ô¨Ånal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = ‚àí 1\\nn\\n‚àë\\ni\\nlog esim(qi,d +\\ni )/œÑ\\nesim(qi,d +\\ni )/œÑ + ‚àë\\nj esim(qi,d ‚àí\\nj )/œÑ + ‚àë\\njÃ∏=i esim(qi,q j )/œÑ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n‚àë\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) ‚àí sim(xi, x j)\\nœÑ )) (2)\\n3.3.3 CLS\\nThe classiÔ¨Åcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiÔ¨Åcation datasets with limited categories may lead to false neg atives from items\\nof diÔ¨Äerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query‚Äôs\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='sample from other data instances, we check whether its label matc hes the current query‚Äôs\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = ‚àí 1\\nn\\n‚àë\\ni\\nlog esim(ti,t +\\ni )/œÑ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/œÑ +\\n‚àë\\nn\\nMASK(ti, t ‚àí\\ni,n ) ¬∑esim(ti,t ‚àí\\ni,n )/œÑ +\\n‚àë\\njÃ∏=i\\nMASK(ti, t j ) ¬∑esim(ti,t j )/œÑ +\\n‚àë\\njÃ∏=i\\n‚àë\\nn\\nMASK(ti, t ‚àí\\nj,n ) ¬∑esim(ti,t ‚àí\\nj,n )/œÑ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eÔ¨Éciency, thus developing eÔ¨Äective automated data sy nthesis methods has'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='portance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eÔ¨Éciency, thus developing eÔ¨Äective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiÔ¨Åcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eÔ¨Äectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diÔ¨Éculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diÔ¨Écu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciÔ¨Åed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reÔ¨Çect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='which represent relatively surface-level characteristics reÔ¨Çect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diÔ¨Äerences, should be recognized as semantic ally equivalent:\\n‚Ä¢ The cat chased the mouse.\\n‚Ä¢ The mouse was chased by the cat.\\nTo eÔ¨Äectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workÔ¨Çow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing WorkÔ¨Çow'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='semantic equivalence while introducing structural divergence. The prompt constraints\\nand workÔ¨Çow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing WorkÔ¨Çow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superÔ¨Åcial structural modiÔ¨Åcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diÔ¨Äerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation WorkÔ¨Çow\\nFigure 5: Hard Negative Synthesis WorkÔ¨Çow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eÔ¨Äort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciÔ¨Åc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tailored for retrieval datasets. Our domain-speciÔ¨Åc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciÔ¨Åc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion‚Äîwithout applying augmentation to prevent ambi-\\nguity; for classiÔ¨Åcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reÔ¨Åned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciÔ¨Åc\\nclustering patterns‚Äîsamples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with Ô¨Åle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='enable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lŒ±\\ni‚àë m\\nj=1 lŒ±\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed‚Äôs[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the Ô¨Årst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the Ô¨Ågure 6. Two key distinctions\\nare incorporated: Ô¨Årst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our Ô¨Åndings indicate that naively incorporating add itional data signiÔ¨Å-\\ncantly degrades retrieval performance.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='datasets, since our Ô¨Åndings indicate that naively incorporating add itional data signiÔ¨Å-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter Œ∑ is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for Œ∑,\\nwhile non-retrieval data constitutes 1 ‚àí Œ∑. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we Ô¨Årst apply an exponential scalin g factor Œ± , a mask fac-\\ntor M is then applied to Ô¨Ålter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n‚àë\\ni\\nMi ¬∑lŒ±\\ni\\nSnon ret =\\n‚àë\\ni\\n(1 ‚àí Mi) ¬∑lŒ±\\ni\\nwhere M i =\\n{\\n0 if di ‚àà RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Sret =\\n‚àë\\ni\\nMi ¬∑lŒ±\\ni\\nSnon ret =\\n‚àë\\ni\\n(1 ‚àí Mi) ¬∑lŒ±\\ni\\nwhere M i =\\n{\\n0 if di ‚àà RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using Œ∑ to derive the Ô¨Ånal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ Œ∑RET ¬∑lŒ±\\ni\\nSret\\nif di ‚àà RET,\\n(1‚àíŒ∑RET )¬∑lŒ±\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='passage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella‚Äôs[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface‚Äôs sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/Ô¨Åle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='4https://drive.google.com/Ô¨Åle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- ClassiÔ¨Åcation[58],\\nMassiveIntent-ClassiÔ¨Åcation[59], MassiveScenario-ClassiÔ¨Åcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiÔ¨Åcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='ClassiÔ¨Åcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and Ô¨Ålter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The Ô¨Ånal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iÔ¨Åc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the Ô¨Årst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconÔ¨Ågurations the same as in the Ô¨Årst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiÔ¨Åcation), considerin g data using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='conÔ¨Ågurations the same as in the Ô¨Årst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiÔ¨Åcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bÔ¨Çoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio Œ∑RET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-Ô¨Åle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-Ô¨Åle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA Ô¨Åne-tuning, we employ full-parameter Ô¨Åne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conÔ¨Ågurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter SpeciÔ¨Åcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bÔ¨Çoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='boards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eÔ¨Äectiveness of our\\napproach. Furthermore, under MTEB‚Äôs oÔ¨Écial ranking protocol, Q zhou-embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='both task-level and task-type average metrics, demonstrating the eÔ¨Äectiveness of our\\napproach. Furthermore, under MTEB‚Äôs oÔ¨Écial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reÔ¨Çect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliÔ¨Åed by the MTEB/c lassiÔ¨Åcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='NV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Qwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniÔ¨Åed multi-task framework comprising specialized data transform ation and training\\nstrategies, eÔ¨Äectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model‚Äôs generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task Ô¨Åne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='strategy comprising initial retrieval-focused training followed by fu ll-task Ô¨Åne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking Ô¨Årst on both leaderboards. Our Ô¨Åndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eÔ¨Äective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ‚ÄùSome simple eÔ¨Äective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.‚Äù In SIGIR‚Äô9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin RaÔ¨Äel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniÔ¨Åed text-to-text transformer. Journal of machine le arning research,\\n21(140):1‚Äì67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894‚Äì6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern¬¥ andez ¬¥Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern¬¥ andez ¬¥Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ‚ÄùLanguage models are few-s hot learners.‚Äù\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ‚ÄùF ine-tuning\\nllama for multi-stage text retrieval.‚Äù In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ‚ÄùRepetition improves language model embeddings.‚Äù a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Raghunathan. ‚ÄùRepetition improves language model embeddings.‚Äù a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ‚ÄùLlm2vec: Large languag e models are\\nsecretly powerful text encoders.‚Äù arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ‚ÄùJaspe r and stella:\\ndistillation of sota embedding models.‚Äù arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ‚ÄùBge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.‚Äù arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ‚ÄùLearning more eÔ¨Äective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.‚Äù arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ‚ÄùApproximate nearest neighbo r negative con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ‚ÄùApproximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.‚Äù arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ‚ÄùNv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.‚Äù arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiÔ¨Äerer, and Even Oldridge. ‚ÄùNV-Retriever: Improving text emb edding models\\nwith eÔ¨Äective hard-negative mining.‚Äù arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ‚ÄùQwen2 technical report.‚Äù arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas MuennighoÔ¨Ä, Defu L ian, and Jian-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[21] Team, Qwen. ‚ÄùQwen2 technical report.‚Äù arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas MuennighoÔ¨Ä, Defu L ian, and Jian-\\nYun Nie. ‚ÄùC-pack: Packed resources for general chinese embedd ings.‚Äù In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] MuennighoÔ¨Ä, Niklas, Nouamane Tazi, Lo¬® ƒ±c Magne, and Nils Reimers . ‚ÄùMteb: Mas-\\nsive text embedding benchmark.‚Äù arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ‚ÄùConan-embed ding: Gen-\\neral text embedding with more and better negative samples.‚Äù arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ‚ÄùAn information-theoretic perspective of tf‚Äìid f measures.‚Äù Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ‚ÄùSome simple eÔ¨Äectiv e approximations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ‚ÄùSome simple eÔ¨Äectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.‚Äù In SIGI R‚Äô94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ‚ÄùIndexing by latent semantic analysis.‚Äù Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, ShaÔ¨Åq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ‚ÄùSfrembedding-mistral: enhance text retrieval with tran sfer learning.‚Äù Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] MuennighoÔ¨Ä, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ‚ÄùGenerative representational instruction tun-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='multi-stage training, 2024[J].\\n[31] MuennighoÔ¨Ä, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ‚ÄùGenerative representational instruction tun-\\ning.‚Äù In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ‚ÄùQwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.‚Äù arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Through Foundation Models.‚Äù arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n‚ÄùRoformer: Enhanced transformer with rotary position embeddin g.‚Äù Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ‚ÄùRoot mean square layer norma lization.‚Äù Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ‚ÄùGlu variants improve transformer.‚Äù arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ‚ÄùPic-\\ncolo2: General text embedding with multi-task hybrid loss training.‚Äù a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ‚ÄùCircle loss: A uniÔ¨Åed perspective of pair similarit y op-\\ntimization.‚Äù In Proceedings of the IEEE/CVF conference on comput er vision and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Wang, and Yichen Wei. ‚ÄùCircle loss: A uniÔ¨Åed perspective of pair similarit y op-\\ntimization.‚Äù In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414‚Äì9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Generative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345‚Äì2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ‚ÄùUnn atural in-\\nstructions: Tuning language models with (almost) no human labor.‚Äù ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ‚ÄùApproximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.‚Äù arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiÔ¨Äerer, and Even Oldridge. ‚ÄùNV-Retriever: Improving text emb edding models\\nwith eÔ¨Äective hard-negative mining.‚Äù arXiv preprint arXiv:2407.15831 (2024).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='SchiÔ¨Äerer, and Even Oldridge. ‚ÄùNV-Retriever: Improving text emb edding models\\nwith eÔ¨Äective hard-negative mining.‚Äù arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ‚ÄùGecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.‚Äù URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='models, 2024.‚Äù URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciÔ¨Åc control and quality reÔ¨Ånement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics‚ÄìVolume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385‚Äì393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ‚ÄùLcqmc: A large-scale chinese question matching corpus.‚Äù In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ‚ÄùLcqmc: A large-scale chinese question matching corpus.‚Äù In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ‚ÄùPAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiÔ¨Åcation .‚Äù arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n‚ÄùSemeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.‚Äù arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='ing comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia RedÔ¨Åeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453‚Äì466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558‚Äì3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369‚Äì2 380, Brussels,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='explainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369‚Äì2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114‚Äì1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriÔ¨Åca tion. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriÔ¨Åca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37‚Äì46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriÔ¨Åcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441‚Äì3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='2020, pages 3441‚Äì3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969‚Äì4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: SpeciÔ¨Åcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ¬±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in Ô¨Åeld The content must remain strictly aligned with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='should not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in Ô¨Åeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuÔ¨Écient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suÔ¨Éciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- DiÔ¨Äerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you‚Äôve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat‚Äôs the ideal credit\\ncard for a person without\\nany credit history?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat‚Äôs the ideal credit\\ncard for a person without\\nany credit history?\\nIf you‚Äôve never had a credit card, it‚Äôs probably\\nbecause you don‚Äôt have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat‚Äôs the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you‚Äôve never owned a credit card, it‚Äôs probably\\nbecause you don‚Äôt have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ‚ÄôThe Faerie\\nQueene‚Äô?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ‚ÄôThe Faerie Queene‚Äô had a strong\\ninÔ¨Çuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ‚ÄôThe\\nFaerie Queene‚Äô?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='manuscript form.\\nWho was the English Poet\\nLaureate that penned ‚ÄôThe\\nFaerie Queene‚Äô?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniÔ¨Åcantly inÔ¨Çuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ‚ÄôThe\\nFaerie Queene‚Äô?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Laureate authored ‚ÄôThe\\nFaerie Queene‚Äô?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you‚Äôve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oÔ¨Äer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ‚ÄôThe\\nFaerie Queene‚Äô?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ‚ÄôThe Faerie Queene‚Äô\\nhad a strong inÔ¨Çuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth‚Äôs\\nreign?\\nEdmund Spenser composed\\n‚ÄôThe Faerie Queene‚Äô, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniÔ¨Åcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare‚Äôs popularity.\\nChristopher Marlowe‚Äôs\\n‚ÄôHero and Leander‚Äô was an-\\nother notable Elizabethan\\npoem, but unlike Spenser‚Äôs'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='signiÔ¨Åcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare‚Äôs popularity.\\nChristopher Marlowe‚Äôs\\n‚ÄôHero and Leander‚Äô was an-\\nother notable Elizabethan\\npoem, but unlike Spenser‚Äôs\\nwork it wasn‚Äôt an epic\\nallegory.\\nWhich poet created\\n‚ÄôParadise Lost‚Äô during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ‚ÄôParadise Lost‚Äô\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake‚Äôs ‚ÄôThe Mar-\\nriage of Heaven and Hell‚Äô\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton‚Äôs mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank‚Äôs occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank‚Äôs occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conÔ¨Çicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank‚Äôs occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='wary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank‚Äôs occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer‚Äôs\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract‚ÄîDue to object detection‚Äôs close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiÔ¨Åers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiÔ¨Åcations and useful tricks\\nto improve detection performance further. As distinct speciÔ¨Åc\\ndetection tasks exhibit different characteristics, we also brieÔ¨Çy\\nsurvey several speciÔ¨Åc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='are also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms‚Äîdeep learning, object detection, neural network\\nI. I NTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiÔ¨Åcation [5], [6], human behavior analysis [7][S4],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='valuable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiÔ¨Åcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these Ô¨Åelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]‚Äì[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it‚Äôs difÔ¨Å-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this Ô¨Åeld in recent years [15]‚Äì[18].\\nThe problem deÔ¨Ånition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiÔ¨Åca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiÔ¨Åcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan Ô¨Ånd out all possible positions of the objects, its short-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='or sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan Ô¨Ånd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a Ô¨Åxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit‚Äôs difÔ¨Åcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sity of appearances, illumination conditions and backgrounds,\\nit‚Äôs difÔ¨Åcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiÔ¨Åcation. Besides, a classiÔ¨Åer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiÔ¨Åers, the\\nDPM is a Ô¨Çexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='allows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefÔ¨Åcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-featureBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signiÔ¨Åcant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='capacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classiÔ¨Åcation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a Ô¨Åxed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]‚Äì[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversiÔ¨Åed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ditions. It should be noticed that the covered domains are\\ndiversiÔ¨Åed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiÔ¨Åcation and object detection, but\\npays little attention on detailing speciÔ¨Åc algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speciÔ¨Åc tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A B RIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the overÔ¨Åtting of training, lack of large scale\\ntraining data, limited computation power and insigniÔ¨Åcance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n‚Ä¢The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n‚Ä¢Fast development of high performance parallel computing\\nsystems, such as GPU clusters;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='as ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n‚Ä¢Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n‚Ä¢SigniÔ¨Åcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\noverÔ¨Åtting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite efÔ¨Åcient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton‚Äôs group, whose continuous efforts have demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='What prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton‚Äôs group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and ‚Äòdropout‚Äô regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose ‚Äòpixel‚Äô can be viewed as a speciÔ¨Åc feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive Ô¨Åeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as Ô¨Åltering and pooling. Filtering (convolution)\\noperation convolutes a Ô¨Ålter matrix (learned weights) with\\nthe values of a receptive Ô¨Åeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain Ô¨Ånal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='linear function (such as sigmoid [51], ReLU) to obtain Ô¨Ånal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive Ô¨Åeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be Ô¨Åne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the Ô¨Ånal layer with different activation functions [6]\\nis added to get a speciÔ¨Åc conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='loss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassiÔ¨Åcation layer. The conv feature maps are produced by\\nconvoluting 3*3 Ô¨Ålter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n‚Ä¢Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n‚Ä¢ Compared with traditional shallow models, a deeper'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n‚Ä¢ Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n‚Ä¢ The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classiÔ¨Åcation and bounding box regression\\ninto a multi-task leaning manner).\\n‚Ä¢ BeneÔ¨Åtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research Ô¨Åelds, such as image super-resolution\\nreconstruction [54], [55], image classiÔ¨Åcation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]‚Äì[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='age retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]‚Äì[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the conÔ¨Ådences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at Ô¨Årst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classiÔ¨Åcation problem, adopting a uniÔ¨Åed\\nframework to achieve Ô¨Ånal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiÔ¨Åes R-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiÔ¨Åes R-\\nCNN with a SPP layer). The regression /classiÔ¨Åcation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario Ô¨Årstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='works [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the conÔ¨Ådences of\\nunderlying object categories.\\n1) R-CNN: It is of signiÔ¨Åcance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the Ô¨Çowchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a Ô¨Åxed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the Ô¨Ånal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassiÔ¨Åcation and localization. With pre-trained category-\\nspeciÔ¨Åc linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN BN\\nMulti-scale\\nGridregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression /classiÔ¨Åcation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].\\nRich feature hierarchies for accurate object detection and semantic segmentation\\nRoss Girshick1 Jeff Donahue1,2 Trevor Darrell1,2 Jitendra Malik1\\n1UC Berkeley and 2ICSI\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012‚Äîachieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciÔ¨Åc Ô¨Åne-tuning, yields a signiÔ¨Å-\\ncant performance boost. Since we combine region propos-\\nals with CNNs, we call our method R-CNN: Regions with\\nCNN features. We also present experiments that provide\\ninsight into what the network learns, revealing a rich hier-\\narchy of image features. Source code for the complete sys-\\ntem is available at http://www.cs.berkeley.edu/\\nÀúrbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='use of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the Ô¨Årst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima‚Äôs ‚Äúneocognitron‚Äù [16], a biologically-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiÔ¨Åes each\\nregion using class-speciÔ¨Åc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [32] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='recognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm\\nby showing that stochastic gradient descent, via backprop-\\nagation, can train convolutional neural networks (CNNs), a\\nclass of models that extend the neocognitron.\\nCNNs saw heavy use in the 1990s ( e.g., [24]), but then\\nfell out of fashion, particularly in computer vision, with the\\nrise of support vector machines. In 2012, Krizhevsky et al.\\n[22] rekindled interest in CNNs by showing substantially\\nhigher image classiÔ¨Åcation accuracy on the ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC) [9, 10].\\nTheir success resulted from training a large CNN on 1.2\\nmillion labeled images, together with a few twists on Le-\\nCun‚Äôs CNN (e.g., max(x, 0) rectifying non-linearities and\\n‚Äúdropout‚Äù regularization).\\nThe signiÔ¨Åcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Cun‚Äôs CNN (e.g., max(x, 0) rectifying non-linearities and\\n‚Äúdropout‚Äù regularization).\\nThe signiÔ¨Åcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiÔ¨Åcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question decisively by bridging the\\nchasm between image classiÔ¨Åcation and object detection.\\nThis paper is the Ô¨Årst to show that a CNN can lead to dra-\\n1\\nFig. 3. The Ô¨Çowchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classiÔ¨Åes each region with class-speciÔ¨Åc linear SVMs.\\nbounding box regression and Ô¨Åltered with a greedy non-\\nmaximum suppression (NMS) to produce Ô¨Ånal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufÔ¨Åcient labeled data, pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding box regression and Ô¨Åltered with a greedy non-\\nmaximum suppression (NMS) to produce Ô¨Ånal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufÔ¨Åcient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN Ô¨Årstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speciÔ¨Åc Ô¨Åne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigniÔ¨Åcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n‚Ä¢Due to the existence of FC layers, the CNN requires a\\nÔ¨Åxed-size (e.g., 227√ó227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n‚Ä¢Training of R-CNN is a multi-stage pipeline. At Ô¨Årst,\\na convolutional network (ConvNet) on object proposals is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region, taking a great deal of time in the testing period.\\n‚Ä¢Training of R-CNN is a multi-stage pipeline. At Ô¨Årst,\\na convolutional network (ConvNet) on object proposals is\\nÔ¨Åne-tuned. Then the softmax classiÔ¨Åer learned by Ô¨Åne-\\ntuning is replaced by SVMs to Ô¨Åt in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n‚Ä¢ Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n‚Ä¢Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or reÔ¨Åne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speciÔ¨Åc CNN classiÔ¨Åers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classiÔ¨Åcation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net: FC layers must take a Ô¨Åxed-size input. That‚Äôs\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several Ô¨Åner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 5\\n9\\nmethod VOC 2007 Caltech101\\nVQ [15]‚Ä† 56.07 74.41 ¬±1.0\\nLLC [18]‚Ä† 57.66 76.95 ¬±0.4\\nFK [19]‚Ä† 61.69 77.78 ¬±0.6\\nDeCAF [13] - 86.91 ¬±0.7\\nZeiler & Fergus [4] 75.90‚Ä° 86.5¬±0.5\\nOquab et al. [34] 77.7 -\\nChatÔ¨Åeld et al. [6] 82.42 88.54¬±0.3\\nours 82.44 93.42 ¬±0.5\\nTable 8: ClassiÔ¨Åcation results for Pascal VOC 2007\\n(mAP) and Caltech101 (accuracy). ‚Ä†numbers reported\\nby [27]. ‚Ä°our implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the\\nstate-of-the-art methods on Caltech101. Our result\\n(93.42%) exceeds the previous record (88.54%) by a\\nsubstantial margin (4.88%).\\n4 SPP- NET FOR OBJECT DETECTION\\nDeep networks have been used for object detection.\\nWe brieÔ¨Çy review the recent state-of-the-art R-CNN\\nmethod [7]. R-CNN Ô¨Årst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='method [7]. R-CNN Ô¨Årst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped\\nto a Ô¨Åxed size (227 √ó227). A pre-trained deep network\\nis used to extract the feature of each window. A\\nbinary SVM classiÔ¨Åer is then trained on these features\\nfor detection. R-CNN generates results of compelling\\nquality and substantially outperforms previous meth-\\nods. However, because R-CNN repeatedly applies the\\ndeep convolutional network to about 2,000 windows\\nper image, it is time-consuming. Feature extraction is\\nthe major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image\\nonly once (possibly at multiple scales). Then we ap-\\nply the spatial pyramid pooling on each candidate\\nwindow of the feature maps to pool a Ô¨Åxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='window of the feature maps to pool a Ô¨Åxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied\\nonce, our method can run orders of magnitude faster.\\nOur method extracts window-wise features from\\nregions of the feature maps, while R-CNN extracts\\ndirectly from image regions. In previous works, the\\nDeformable Part Model (DPM) [23] extracts features\\nfrom windows in HOG [24] feature maps, and the\\nSelective Search (SS) method [20] extracts from win-\\ndows in encoded SIFT feature maps. The Overfeat\\ndetection method [5] also extracts from windows of\\ndeep convolutional feature maps, but needs to pre-\\ndeÔ¨Åne the window size. On the contrary, our method\\nenables feature extraction in arbitrary windows from\\nthe deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n‚Ä¶...\\nfully-connected layers (fc6, fc7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n‚Ä¶...\\nfully-connected layers (fc6, fc7)\\nFigure 5: Pooling features from arbitrary windows\\non feature maps. The feature maps are computed\\nfrom the entire image. The pooling is performed in\\ncandidate windows.\\n4.1 Detection Algorithm\\nWe use the ‚Äúfast‚Äù mode of selective search [20] to\\ngenerate about 2,000 candidate windows per image.\\nThen we resize the image such that min(w,h) = s,\\nand extract the feature maps from the entire image.\\nWe use the SPP-net model of ZF-5 (single-size trained)\\nfor the time being. In each candidate window, we use\\na 4-level spatial pyramid (1 √ó1, 2√ó2, 3√ó3, 6√ó6, totally\\n50 bins) to pool the features. This generates a 12,800-\\nd (256 √ó50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='d (256 √ó50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear\\nSVM classiÔ¨Åer for each category on these features.\\nOur implementation of the SVM training follows\\n[20], [7]. We use the ground-truth windows to gen-\\nerate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%\\n(measured by the intersection-over-union (IoU) ratio).\\nAny negative sample is removed if it overlaps another\\nnegative sample by more than 70%. We apply the stan-\\ndard hard negative mining [23] to train the SVM. This\\nstep is iterated once. It takes less than 1 hour to train\\nSVMs for all 20 categories. In testing, the classiÔ¨Åer\\nis used to score the candidate windows. Then we use\\nnon-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) ='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='non-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) =\\ns ‚àà S = {480,576,688,864,1200}, and compute the\\nfeature maps of conv 5 for each scale. One strategy of\\ncombining the features from these scales is to pool\\nthem channel-by-channel. But we empirically Ô¨Ånd\\nthat another strategy provides better results. For each\\ncandidate window, we choose a single scale s ‚àà S\\nsuch that the scaled candidate window has a number\\nof pixels closest to 224 √ó224. Then we only use the\\nfeature maps extracted from this scale to compute\\nFig. 4. The architecture of SPP-net for object detection [64].\\nSPPnet also has notable drawbacks. Like R-CNN, train-\\ning is a multi-stage pipeline that involves extracting fea-\\ntures, Ô¨Åne-tuning a network with log loss, training SVMs,\\nand Ô¨Ånally Ô¨Åtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the Ô¨Åne-tuning al-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tures, Ô¨Åne-tuning a network with log loss, training SVMs,\\nand Ô¨Ånally Ô¨Åtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the Ô¨Åne-tuning al-\\ngorithm proposed in [\\n11] cannot update the convolutional\\nlayers that precede the spatial pyramid pooling. Unsurpris-\\ningly, this limitation (Ô¨Åxed convolutional layers) limits the\\naccuracy of very deep networks.\\n1.2. Contributions\\nWe propose a new training algorithm that Ô¨Åxes the disad-\\nvantages of R-CNN and SPPnet, while improving on their\\nspeed and accuracy. We call this method Fast R-CNN be-\\ncause it‚Äôs comparatively fast to train and test. The Fast R-\\nCNN method has several advantages:\\n1. Higher detection quality (mAP) than R-CNN, SPPnet\\n2. Training is single-stage, using a multi-task loss\\n3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-\\ncense at https://github.com/rbgirshick/\\nfast-rcnn.\\n2. Fast R-CNN architecture and training\\nFig. 1 illustrates the Fast R-CNN architecture. A Fast\\nR-CNN network takes as input an entire image and a set\\nof object proposals. The network Ô¨Årst processes the whole\\nimage with several convolutional ( conv) and max pooling\\nlayers to produce a conv feature map. Then, for each ob-\\nject proposal a region of interest ( RoI) pooling layer ex-\\ntracts a Ô¨Åxed-length feature vector from the feature map.\\nEach feature vector is fed into a sequence of fully connected\\n(fc) layers that Ô¨Ånally branch into two sibling output lay-\\ners: one that produces softmax probability estimates over\\nK object classes plus a catch-all ‚Äúbackground‚Äù class and\\nanother layer that outputs four real-valued numbers for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ers: one that produces softmax probability estimates over\\nK object classes plus a catch-all ‚Äúbackground‚Äù class and\\nanother layer that outputs four real-valued numbers for each\\nof the K object classes. Each set of 4 values encodes reÔ¨Åned\\nbounding-box positions for one of the K classes.\\n2.1. The RoI pooling layer\\nThe RoI pooling layer uses max pooling to convert the\\nfeatures inside any valid region of interest into a small fea-\\nture map with a Ô¨Åxed spatial extent of H √ó W (e.g., 7 √ó 7),\\nwhere H and W are layer hyper-parameters that are inde-\\npendent of any particular RoI. In this paper, an RoI is a\\nrectangular window into a conv feature map. Each RoI is\\ndeÔ¨Åned by a four-tuple (r, c, h, w ) that speciÔ¨Åes its top-left\\ncorner (r, c) and its height and width (h, w ).\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-\\nple regions of interest (RoIs) are input into a fully convolutional\\nnetwork. Each RoI is pooled into a Ô¨Åxed-size feature map and\\nthen mapped to a feature vector by fully connected layers (FCs).\\nThe network has two output vectors per RoI: softmax probabilities\\nand per-class bounding-box regression offsets. The architecture is\\ntrained end-to-end with a multi-task loss.\\nRoI max pooling works by dividing the h √ó w RoI win-\\ndow into an H √ó W grid of sub-windows of approximate\\nsize h/H √ó w/W and then max-pooling the values in each\\nsub-window into the corresponding output grid cell. Pool-\\ning is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets ['),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets [\\n11] in which there is only one pyramid level. We\\nuse the pooling sub-window calculation given in [ 11].\\n2.2. Initializing from pre-trained networks\\nWe experiment with three pre-trained ImageNet [ 4] net-\\nworks, each with Ô¨Åve max pooling layers and between Ô¨Åve\\nand thirteen conv layers (see Section\\n4.1 for network de-\\ntails). When a pre-trained network initializes a Fast R-CNN\\nnetwork, it undergoes three transformations.\\nFirst, the last max pooling layer is replaced by a RoI\\npooling layer that is conÔ¨Ågured by setting H and W to be\\ncompatible with the net‚Äôs Ô¨Årst fully connected layer ( e.g.,\\nH = W = 7 for VGG16).\\nSecond, the network‚Äôs last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiÔ¨Å-\\ncation) are replaced with the two sibling layers described'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='H = W = 7 for VGG16).\\nSecond, the network‚Äôs last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiÔ¨Å-\\ncation) are replaced with the two sibling layers described\\nearlier (a fully connected layer and softmax over K + 1 cat-\\negories and category-speciÔ¨Åc bounding-box regressors).\\nThird, the network is modiÔ¨Åed to take two data inputs: a\\nlist of images and a list of RoIs in those images.\\n2.3. Fine-tuning for detection\\nTraining all network weights with back-propagation is an\\nimportant capability of Fast R-CNN. First, let‚Äôs elucidate\\nwhy SPPnet is unable to update weights below the spatial\\npyramid pooling layer.\\nThe root cause is that back-propagation through the SPP\\nlayer is highly inefÔ¨Åcient when each training sample ( i.e.\\nRoI) comes from a different image, which is exactly how\\nR-CNN and SPPnet networks are trained. The inefÔ¨Åciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to Ô¨Åxed-length feature vectors. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN and SPPnet networks are trained. The inefÔ¨Åciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to Ô¨Åxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the Ô¨Ånal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe Ô¨Ånal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 √ó(12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection efÔ¨Åciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='also improves detection efÔ¨Åciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efÔ¨Åciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network Ô¨Åne-tuning, SVM training and bounding-\\nbox regressor Ô¨Åtting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the Ô¨Åne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classiÔ¨Åcation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a Ô¨Åxed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\nÔ¨Ånally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C+ 1categories (C object classes plus one ‚Äòbackground‚Äô\\nclass) and the other output layer encodes reÔ¨Åned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deÔ¨Åned as below to jointly train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deÔ¨Åned as below to jointly train\\nclassiÔ¨Åcation and bounding-box regression,\\nL(p,u,t u,v) =Lcls(p,u) +Œª[u‚â•1]Lloc(tu,v) (1)\\nwhere Lcls(p,u) =‚àílog pu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p= (p0,¬∑¬∑¬∑ ,pC) over the C+1 outputs from the\\nlast FC layer. Lloc(tu,v) is deÔ¨Åned over the predicted offsets\\ntu = (tu\\nx,tu\\ny,tu\\nw,tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx,vy,vw,vh), where x,y,w,h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u‚â•1] is employed to omit all background RoIs. To provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u‚â•1] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to Ô¨Åt\\nbounding-box regressors as below\\nLloc(tu,v) =\\n‚àë\\ni‚ààx,y,w,h\\nsmoothL1 (tu\\ni ‚àívi) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if|x|<1\\n|x|‚àí0.5 otherwise (3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inefÔ¨Åcient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at Ô¨Årst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='where R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nefÔ¨Åciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving efÔ¨Åciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speciÔ¨Åc conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 6\\ncar : 1.000\\ndog : 0.997\\nperson : 0.992\\nperson : 0.979\\nhorse : 0.993\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores 4k coordinates\\nsliding window\\nreg layercls layer\\nk anchor boxes\\nbus : 0.996\\nperson : 0.736\\nboat : 0.970\\nperson : 0.989\\nperson : 0.983person : 0.983\\nperson : 0.925\\ncat : 0.982\\ndog : 0.994\\nFigure 1: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals\\non PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\\nfeature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d\\nfor VGG). This vector is fed into two sibling fully-connected layers‚Äîa box-regression layer ( reg)\\nand a box-classiÔ¨Åcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nÔ¨Åeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and a box-classiÔ¨Åcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nÔ¨Åeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-\\nnetwork is illustrated at a single position in Fig. 1 (left). Note that because the mini-network operates\\nin a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This\\narchitecture is naturally implemented with an n√ónconv layer followed by two sibling 1 √ó1 conv\\nlayers (for reg and cls, respectively). ReLUs [15] are applied to the output of the n√ónconv layer.\\nTranslation-Invariant Anchors\\nAt each sliding-window location, we simultaneously predict k region proposals, so the reg layer\\nhas 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='has 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to\\nkreference boxes, called anchors. Each anchor is centered at the sliding window in question, and is\\nassociated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yieldingk= 9anchors\\nat each sliding position. For a conv feature map of a sizeW√óH(typically ‚àº2,400), there are WHk\\nanchors in total. An important property of our approach is that it is translation invariant, both in\\nterms of the anchors and the functions that compute proposals relative to the anchors.\\nAs a comparison, the MultiBox method [20] uses k-means to generate 800 anchors, which are not\\ntranslation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='translation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the\\nMultiBox anchors are not translation invariant, it requires a (4+1) √ó800-dimensional output layer,\\nwhereas our method requires a (4+2)√ó9-dimensional output layer. Our proposal layers have an order\\nof magnitude fewer parameters (27 million for MultiBox using GoogLeNet [20] vs. 2.4 million for\\nRPN using VGG-16), and thus have less risk of overÔ¨Åtting on small datasets, like PASCAL VOC.\\nA Loss Function for Learning Region Proposals\\nFor training RPNs, we assign a binary class label (of being an object or not) to each anchor. We\\nassign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-\\nover-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='over-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher\\nthan 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels\\nto multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than\\n0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the\\ntraining objective.\\nWith these deÔ¨Ånitions, we minimize an objective function following the multi-task loss in Fast R-\\nCNN [5]. Our loss function for an image is deÔ¨Åned as:\\nL({pi},{ti}) = 1\\nNcls\\n‚àë\\ni\\nLcls (pi,p‚àó\\ni ) +Œª 1\\nNreg\\n‚àë\\ni\\np‚àó\\ni Lreg(ti,t‚àó\\ni ). (1)\\n2For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic\\nregression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeÔ¨Åned anchor boxes are\\nconvoluted with each sliding window to produce Ô¨Åxed-length vectors which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeÔ¨Åned anchor boxes are\\nconvoluted with each sliding window to produce Ô¨Åxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn√ón spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classiÔ¨Åcation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n√ón conv layer followed by two sibling 1 √ó1 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n√ón conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n‚àë\\ni\\nLcls(pi,p‚àó'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n‚àë\\ni\\nLcls(pi,p‚àó\\ni) +Œª 1\\nNreg\\n‚àë\\ni\\np‚àó\\niLreg(ti,t‚àó\\ni)\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p‚àó\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t‚àó\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size ( Ncls)\\nand the number of anchor locations ( Nreg), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='With the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassiÔ¨Åcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciÔ¨Åc spatial pooling layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='classiÔ¨Åcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciÔ¨Åc spatial pooling layer.\\nRecent state-of-the-art image classiÔ¨Åcation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it‚Äôs\\nFeature Pyramid Networks for Object Detection\\nTsung-Yi Lin1,2, Piotr Doll¬¥ar1, Ross Girshick1,\\nKaiming He1, Bharath Hariharan1, and Serge Belongie2\\n1Facebook AI Research (FAIR)\\n2Cornell University and Cornell Tech\\nAbstract\\nFeature pyramids are a basic component in recognition\\nsystems for detecting objects at different scales. But recent\\ndeep learning object detectors have avoided pyramid rep-\\nresentations, in part because they are compute and memory\\nintensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='intensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-\\ndown architecture with lateral connections is developed for\\nbuilding high-level semantic feature maps at all scales. This\\narchitecture, called a Feature Pyramid Network (FPN),\\nshows signiÔ¨Åcant improvement as a generic feature extrac-\\ntor in several applications. Using FPN in a basic Faster\\nR-CNN system, our method achieves state-of-the-art single-\\nmodel results on the COCO detection benchmark without\\nbells and whistles, surpassing all existing single-model en-\\ntries including those from the COCO 2016 challenge win-\\nners. In addition, our method can run at 6 FPS on a GPU\\nand thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-\\ndamental challenge in computer vision. Feature pyramids\\nbuilt upon image pyramids (for short we call these featur-\\nized image pyramids) form the basis of a standard solution\\n[1] (Fig. 1(a)). These pyramids are scale-invariant in the\\nsense that an object‚Äôs scale change is offset by shifting its\\nlevel in the pyramid. Intuitively, this property enables a\\nmodel to detect objects across a large range of scales by\\nscanning the model over both positions and pyramid levels.\\nFeaturized image pyramids were heavily used in the\\nera of hand-engineered features [5, 25]. They were so\\ncritical that object detectors like DPM [7] required dense\\nscale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFigure 1. (a) Using an image pyramid to build a feature pyramid.\\nFeatures are computed on each of the image scales independently,\\nwhich is slow. (b) Recent detection systems have opted to use\\nonly single scale features for faster detection. (c) An alternative is\\nto reuse the pyramidal feature hierarchy computed by a ConvNet\\nas if it were a featurized image pyramid. (d) Our proposed Feature\\nPyramid Network (FPN) is fast like (b) and (c), but more accurate.\\nIn this Ô¨Ågure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In this Ô¨Ågure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-\\nvolutional networks (ConvNets) [19, 20]. Aside from being\\ncapable of representing higher-level semantics, ConvNets\\nare also more robust to variance in scale and thus facilitate\\nrecognition from features computed on a single input scale\\n[15, 11, 29] (Fig. 1(b)). But even with this robustness, pyra-\\nmids are still needed to get the most accurate results. All re-\\ncent top entries in the ImageNet [33] and COCO [21] detec-\\ntion challenges use multi-scale testing on featurized image\\npyramids (e.g., [16, 35]). The principle advantage of fea-\\nturizing each level of an image pyramid is that it produces\\na multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-\\nmid has obvious limitations. Inference time increases con-\\nsiderably (e.g., by four times [11]), making this approach\\nimpractical for real applications. Moreover, training deep\\n1\\narXiv:1612.03144v2  [cs.CV]  19 Apr 2017\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiÔ¨Åcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiÔ¨Åcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a Ô¨Åxed grid of k√ók Ô¨Årstly and a position-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a Ô¨Åxed grid of k√ók Ô¨Årstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classiÔ¨Åcation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at Ô¨Årst and then enhanced with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='set of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at Ô¨Årst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 √ó1 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a3√ó3\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the Ô¨Ånal feature map is\\ngenerated. This process is iterated until the Ô¨Ånest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacriÔ¨Åcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classiÔ¨Åcation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m√óm mask to maintain the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m√óm mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassiÔ¨Åcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deÔ¨Åned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classiÔ¨Åcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiÔ¨Åcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='It affects classiÔ¨Åcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='other tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a Ô¨Çexible and efÔ¨Åcient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modiÔ¨Åcation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling: Although the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and ‚Äòstuff‚Äô (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classiÔ¨Åcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region proposal generation, pixel-level instance segmentation\\nand regional instance classiÔ¨Åcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive Ô¨Åelds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classiÔ¨Åers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='support regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modiÔ¨Åcations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modiÔ¨Åed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='summing different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and efÔ¨Åcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classiÔ¨Åers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classiÔ¨Åer\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiÔ¨Åcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Traditional CNN framework for object detection is not\\nskilled in handling signiÔ¨Åcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classiÔ¨Åcation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\nÔ¨Årstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‚Äòdeep and thin‚Äô design'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‚Äòdeep and thin‚Äô design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PV ANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression /ClassiÔ¨Åcation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiÔ¨Åcation and bounding box\\nregression, which are usually trained separately. Even in recent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='eral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiÔ¨Åcation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step frameworks based on global regres-\\nsion/classiÔ¨Åcation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We Ô¨Årstly reviews some pioneer CNN\\nmodels, and then focus on two signiÔ¨Åcant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiÔ¨Åcation task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiÔ¨Åcation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difÔ¨Åculty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is efÔ¨Åcient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniÔ¨Åed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniÔ¨Åed\\nloss was introduced to bias both localization and conÔ¨Ådences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the Ô¨Ånal layer.\\nYoo et al. adopted an iterative classiÔ¨Åcation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inefÔ¨Åcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the model becomes quite inefÔ¨Åcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 9\\nFig. 9. Main idea of YOLO [17].\\nÔ¨Ånding a path from a Ô¨Åxed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a Ô¨Åxed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndifÔ¨Åculty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both conÔ¨Ådences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S√óS grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conÔ¨Ådence scores. Formally, conÔ¨Å-\\ndence scores are deÔ¨Åned as Pr(Object) ‚àóIOUtruth\\npred , which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conÔ¨Ådence scores. Formally, conÔ¨Å-\\ndence scores are deÔ¨Åned as Pr(Object) ‚àóIOUtruth\\npred , which\\nindicates how likely there exist objects ( Pr(Object) ‚â•0) and\\nshows conÔ¨Ådences of its prediction ( IOUtruth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speciÔ¨Åc conÔ¨Ådence scores for each box\\nare achieved by multiplying the individual box conÔ¨Ådence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) ‚àóIOUtruth\\npred ‚àóPr(Classi|Object)\\n= Pr(Classi) ‚àóIOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciÔ¨Åc objects in the\\nbox and the Ô¨Åtness between the predicted box and the object\\nare both taken into consideration.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='= Pr(Classi) ‚àóIOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciÔ¨Åc objects in the\\nbox and the Ô¨Åtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\nŒªcoord\\nS2\\n‚àë\\ni=0\\nB‚àë\\nj=0\\n1 obj\\nij\\n[\\n(xi ‚àíÀÜxi)2 + (yi ‚àíÀÜyi)2]\\n+Œªcoord\\nS2\\n‚àë\\ni=0\\nB‚àë\\nj=0\\n1 obj\\nij\\n[(‚àöwi ‚àí\\n‚àö\\nÀÜwi)2 + (\\n‚àö\\nhi ‚àí\\n‚àö\\nÀÜhi\\n)2]\\n+\\nS2\\n‚àë\\ni=0\\nB‚àë\\nj=0\\n1 obj\\nij\\n(\\nCi ‚àíÀÜCi\\n)2\\n+Œªnoobj\\nS2\\n‚àë\\ni=0\\nB‚àë\\nj=0\\n1 noobj\\nij\\n(\\nCi ‚àíÀÜCi\\n)2\\n+\\nS2\\n‚àë\\ni=0\\n1 obj\\ni\\n‚àë\\nc‚ààclasses\\n(pi(c) ‚àíÀÜpi(c))2\\n(6)\\nIn a certain cell i, (xi,yi) denote the center of the box relative\\nto the bounds of the grid cell,(wi,hi) are the normalized width\\nand height relative to the image size, Ci represents conÔ¨Ådence\\nscores, 1 obj\\ni indicates the existence of objects and 1 obj\\nij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiÔ¨Åcation errors. Similarly,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiÔ¨Åcation errors. Similarly,\\nwhen the predictor is ‚Äòresponsible‚Äô for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 √ó1 reduction layers followed by 3 √ó3 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpliÔ¨Åed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='makes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a difÔ¨Åculty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\nconÔ¨Ågurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speciÔ¨Åc feature map, instead of\\nÔ¨Åxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Ô¨Åxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated conÔ¨Ådences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and conÔ¨Ådence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale reÔ¨Åned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiÔ¨Åcantly outperforms the Faster R-CNN in terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Integrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiÔ¨Åcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300√ó300) runs\\nat 59 FPS, which is more accurate and efÔ¨Åcient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated conÔ¨Ådences. Final detection results are obtained by conducting NMS on multi-scale reÔ¨Åned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN &S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speciÔ¨Åc instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classiÔ¨Åcation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n‚Ä¢If incorporated with a proper way, more powerful back-\\nbone CNN models can deÔ¨Ånitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n‚Ä¢ With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n‚Ä¢Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith ‚Äò07‚Äô ,‚Äò07+12‚Äô and ‚Äò07+12+coco‚Äô).\\n‚Ä¢Apart from basic models, there are still many other factors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='important for deep learning based models (Faster R-CNN\\nwith ‚Äò07‚Äô ,‚Äò07+12‚Äô and ‚Äò07+12+coco‚Äô).\\n‚Ä¢Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\nÔ¨Åed classiÔ¨Åcation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n‚Ä¢As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and Ô¨Åne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n‚Ä¢By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n‚Ä¢ Multi-scale training and test are beneÔ¨Åcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Multi-scale training and test are beneÔ¨Åcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n‚Ä¢ Overall, region proposal based methods, such as\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/classÔ¨Åcation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/classÔ¨Åcation based approaches.\\n‚Ä¢ Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n‚Ä¢Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='objects and surroundings (GBD-Net and multi-path).\\n‚Ä¢Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n‚Ä¢The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classiÔ¨Åers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES .\\nFramework Proposal Multi-scale Input Learning Method Loss Function Softmax Layer End-to-end Train Platform Language\\nR-CNN [15] Selective Search - SGD,BP Hinge loss (classiÔ¨Åcation),Bounding box regression + - Caffe Matlab\\nSPP-net [64] EdgeBoxes + SGD Hinge loss (classiÔ¨Åcation),Bounding box regression + - Caffe Matlab\\nFast RCNN [16] Selective Search + SGD Class Log loss+bounding box regression + - Caffe Python\\nFaster R-CNN [18] RPN + SGD Class Log loss+bounding box regression + + Caffe Python/Matlab\\nR-FCN [65] RPN + SGD Class Log loss+bounding box regression - + Caffe Matlab\\nMask R-CNN [67] RPN + SGD Class Log loss+bounding box regression + + TensorFlow/Keras Python+Semantic sigmoid loss\\nFPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='FPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python\\nYOLO [17] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conÔ¨Ådence+background conÔ¨Ådence\\nSSD [71] - - SGD Class softmax loss+bounding box regression - + Caffe C++\\nYOLOv2 [72] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conÔ¨Ådence+background conÔ¨Ådence\\n* ‚Äò+‚Äô denotes that corresponding techniques are employed while ‚Äò-‚Äô denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='by the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN (Alex) [15] 07 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 68.6 58.5\\nR-CNN(VGG16) [15] 07 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nSPP-net(ZF) [64] 07 68.5 71.7 58.7 41.9 42.5 67.7 72.1 73.8 34.7 67.0 63.4 66.0 72.5 71.3 58.9 32.8 60.9 56.1 67.9 68.8 60.9\\nGCNN [70] 07 68.3 77.3 68.5 52.4 38.6 78.5 79.5 81.0 47.1 73.6 64.5 77.2 80.5 75.8 66.6 34.3 65.2 64.4 75.6 66.4 66.8\\nBayes [85] 07 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 70.1 41.4 69.6 60.8 70.2 73.7 68.5\\nFast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0\\nSDP+CRC [33] 07 76.1 79.4 68.2 52.6 46.0 78.4 78.4 81.0 46.7 73.5 65.3 78.6 81.0 76.7 77.3 39.0 65.1 67.2 77.5 70.3 68.9\\nSubCNN [60] 07 70.2 80.5 69.5 60.3 47.9 79.0 78.7 84.2 48.5 73.9 63.0 82.7 80.6 76.0 70.2 38.2 62.4 67.7 77.7 60.5 68.5\\nStuffNet30 [100] 07 72.6 81.7 70.6 60.5 53.0 81.5 83.7 83.9 52.2 78.9 70.7 85.0 85.7 77.0 78.7 42.2 73.6 69.2 79.2 73.8 72.7\\nNOC [114] 07+12 76.3 81.4 74.4 61.7 60.8 84.7 78.2 82.9 53.0 79.2 69.2 83.2 83.2 78.5 68.0 45.0 71.6 76.7 82.2 75.7 73.3\\nMR-CNN&S-CNN [110] 07+12 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 78.2\\nHyperNet [101] 07+12 77.4 83.3 75.0 69.1 62.4 83.1 87.4 87.4 57.1 79.8 71.4 85.1 85.1 80.0 79.1 51.2 79.1 75.7 80.9 76.5 76.3\\nMS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6\\nOHEM+Fast R-CNN [113] 07+12 80.6 85.7 79.8 69.9 60.8 88.3 87.9 89.6 59.7 85.1 76.5 87.1 87.3 82.4 78.8 53.7 80.5 78.7 84.5 80.7 78.9\\nION [95] 07+12+S 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 79.2\\nFaster R-CNN [18] 07 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6 69.9\\nFaster R-CNN [18] 07+12 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 73.2\\nFaster R-CNN [18] 07+12+COCO 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9 78.8\\nSSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6\\nSSD512 [71] 07+12+COCO 86.6 88.3 82.4 76.0 66.3 88.6 88.9 89.1 65.1 88.4 73.6 86.5 88.9 85.3 84.6 59.1 85.0 80.4 87.4 81.2 81.6\\n* ‚Äò07‚Äô: VOC2007 trainval, ‚Äò07+12‚Äô: union of VOC2007 and VOC2012 trainval, ‚Äò07+12+COCO‚Äô: trained on COCO trainval35k at Ô¨Årst and then Ô¨Åne-tuned on 07+12. The S in ION ‚Äò07+12+S‚Äô denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN(Alex) [15] 12 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1 53.3\\nR-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4\\nBayes [85] 12 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2 66.4\\nFast R-CNN [16] 07++12 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 68.4\\nSutffNet30 [100] 12 83.0 76.9 71.2 51.6 50.1 76.4 75.7 87.8 48.3 74.8 55.7 85.7 81.2 80.3 79.5 44.2 71.8 61.0 78.5 65.4 70.0\\nNOC [114] 07+12 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1 68.8\\nMR-CNN&S-CNN [110] 07++12 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 73.9\\nHyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='HyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4\\nOHEM+Fast R-CNN [113] 07++12+coco 90.1 87.4 79.9 65.8 66.3 86.1 85.0 92.9 62.4 83.4 69.5 90.6 88.9 88.9 83.6 59.0 82.0 74.7 88.2 77.3 80.1\\nION [95] 07+12+S 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 76.4\\nFaster R-CNN [18] 07++12 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 70.4\\nFaster R-CNN [18] 07++12+coco 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2 75.9\\nYOLO [17] 07++12 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 57.9\\nYOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7\\nYOLOv2 [72] 07++12+coco 88.8 87.0 77.8 64.9 51.8 85.2 79.3 93.1 64.4 81.4 70.2 91.3 88.1 87.2 81.0 57.7 78.1 71.0 88.5 76.8 78.2\\nSSD300 [71] 07++12+coco 91.0 86.0 78.1 65.0 55.4 84.9 84.0 93.4 62.1 83.6 67.3 91.3 88.9 88.6 85.6 54.7 83.8 77.3 88.3 76.5 79.3\\nSSD512 [71] 07++12+coco 91.4 88.6 82.6 71.4 63.1 87.4 88.1 93.9 66.9 86.6 66.3 92.0 91.7 90.8 88.5 60.9 87.0 75.4 90.2 80.4 82.2\\nR-FCN (ResNet101) [16] 07++12+coco 92.3 89.9 86.7 74.7 75.2 86.7 89.0 95.8 70.2 90.4 66.5 95.0 93.2 92.1 91.1 71.0 89.7 76.0 92.0 83.4 85.0\\n* ‚Äò07++12‚Äô: union of VOC2007 trainval and test and VOC2012 trainval. ‚Äò07++12+COCO‚Äô: trained on COCO trainval35k at Ô¨Årst then Ô¨Åne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='TABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L\\nFast R-CNN [16] train 20.5 39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95] train 23.6 43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114] train 21.2 41.5 19.7 - - - - - - - - -\\nNOC+FRCN(Google) [114] train 24.8 44.4 25.2 - - - - - - - - -\\nNOC+FRCN (ResNet101) [114] train 27.2 48.4 27.6 - - - - - - - - -\\nGBD-Net [109] train 27.0 45.8 - - - - - - - - - -\\nOHEM+FRCN [113] train 22.6 42.5 22.2 5.0 23.7 34.6 - - - - - -\\nOHEM+FRCN* [113] train 24.4 44.4 24.8 7.1 26.4 37.9 - - - - - -\\nOHEM+FRCN* [113] trainval 25.5 45.9 26.1 7.4 27.7 38.5 - - - - - -\\nFaster R-CNN [18] trainval 24.2 45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71] trainval35k 26.8 46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65] trainval 29.2 51.5 - 10.8 32.8 45.0 - - - - - -\\nR-FCN*(ResNet101) [65] trainval 29.9 51.9 - 10.4 32.4 43.3 - - - - - -\\nR-FCN**(ResNet101) [65] trainval 31.5 53.2 - 14.3 35.5 44.2 - - - - - -\\nMulti-path [112] trainval 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66] trainval35k 36.2 59.1 39.0 18.2 39.0 48.2 - - - - - -\\nMask (ResNet101+FPN) [67] trainval35k 38.2 60.3 41.7 20.1 41.1 50.2 - - - - - -\\nMask (ResNeXt101+FPN) [67] trainval35k 39.8 62.3 43.4 22.1 43.2 51.2 - - - - - -\\nDSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='DSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for ‚ÄòSS‚Äô which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n‚Ä¢ By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uniÔ¨Åed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It‚Äôs also helpful to compress the parameters\\nof FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03\\nSS+SPP-net [64] 07 63.1 2.3 0.44\\nSS+FRCN [16] 07+12 66.9 1.72 0.6\\nSDP+CRC [33] 07 68.9 0.47 2.1\\nSS+HyperNet* [101] 07+12 76.3 0.20 5\\nMR-CNN&S-CNN [110] 07+12 78.2 30 0.03\\nION [95] 07+12+S 79.2 1.92 0.5\\nFaster R-CNN(VGG16) [18] 07+12 73.2 0.11 9.1\\nFaster R-CNN(ResNet101) [18] 07+12 83.8 2.24 0.4\\nYOLO [17] 07+12 63.4 0.02 45\\nSSD300 [71] 07+12 74.3 0.02 46\\nSSD512 [71] 07+12 76.8 0.05 19\\nR-FCN(ResNet101) [65] 07+12+coco 83.6 0.17 5.9\\nYOLOv2(544*544) [72] 07+12 78.6 0.03 40\\nDSSD321(ResNet101) [73] 07+12 78.6 0.07 13.6\\nDSOD300 [74] 07+12+coco 81.7 0.06 17.4\\nPV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‚Äòfast mode‚Äô Selective Search [16], HyperNet*: the speed up version of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='PV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‚Äòfast mode‚Äô Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PA VNET+ (compresss): PA VNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n‚Ä¢It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN &S-\\nRCNN).\\n‚Ä¢It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n‚Ä¢Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modiÔ¨Åed into real-time systems with the introduction\\nof other tricks [116] (PV ANET), such as BN [43], residual\\nconnections [123].\\nIV. S ALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signiÔ¨Åcance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by Ô¨Åne-tuning DNNs‚Äô\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reÔ¨Åned the results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reÔ¨Åned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uniÔ¨Åed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is beneÔ¨Åcial. To learn internal represen-\\ntations of saliency efÔ¨Åciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tations of saliency efÔ¨Åciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive Ô¨Åelds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiÔ¨Åcance for improving detection performance. Based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='convolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiÔ¨Åcance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye Ô¨Åxations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efÔ¨Åcient and accurate to train a direct pixel-wise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efÔ¨Åcient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='this problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Two standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deÔ¨Åned as below\\nFŒ≤ = (1 +Œ≤2)Presion √óRecall\\nŒ≤2Presion + Recall (7)\\nwhere Œ≤2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE = 1\\nH√óW\\nH‚àë\\ni=1\\nW‚àë\\nj=1\\n‚èê‚èê‚èêÀÜS(i,j) = ÀÜZ(i,j)\\n‚èê‚èê‚èê (8)\\nwhere ÀÜZ and ÀÜS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can Ô¨Ånd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insufÔ¨Åcient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='segmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signiÔ¨Åcance for measuring local\\nconspicuity. Finally, it‚Äôs necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='complementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]‚Äì[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural conÔ¨Ågurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiÔ¨Åers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiÔ¨Åers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nefÔ¨Åciency. However, this detector may degrade signiÔ¨Åcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]‚Äì[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='been proposed [167]‚Äì[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='into smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nefÔ¨Åcient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS .\\nDataset Metrics CHM [150] RC [151] DRFI [152] MC [138] MDF [146] LEGS [136] DSR [149] MTDNN [141] CRPSD [142] DCL [143] ELD [153] NLDF [154] DSSC [155]\\nPASCAL-S wFŒ≤ 0.631 0.640 0.679 0.721 0.764 0.756 0.697 0.818 0.776 0.822 0.767 0.831 0.830\\nMAE 0.222 0.225 0.221 0.147 0.145 0.157 0.128 0.170 0.063 0.108 0.121 0.099 0.080\\nECSSD wFŒ≤ 0.722 0.741 0.787 0.822 0.833 0.827 0.872 0.810 0.849 0.898 0.865 0.905 0.915\\nMAE 0.195 0.187 0.166 0.107 0.108 0.118 0.037 0.160 0.046 0.071 0.098 0.063 0.052\\nHKU-IS wFŒ≤ 0.728 0.726 0.783 0.781 0.860 0.770 0.833 - 0.821 0.907 0.844 0.902 0.913\\nMAE 0.158 0.165 0.143 0.098 0.129 0.118 0.040 - 0.043 0.048 0.071 0.048 0.039\\nSOD wFŒ≤ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SOD wFŒ≤ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118\\n* The bigger wFŒ≤ is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modiÔ¨Åcations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uniÔ¨Åed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a Ô¨Åxed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating predeÔ¨Åned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='addressed to transfer from generic object detection to face\\ndetection, namely eliminating predeÔ¨Åned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na conÔ¨Åguration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-Ô¨Åne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and reÔ¨Åne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='To reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to reÔ¨Åne the positions of possible\\nfaces. Qin et al. proposed a uniÔ¨Åed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reÔ¨Çect the dependence of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='types of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reÔ¨Çect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the reÔ¨Çection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='NPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signiÔ¨Åcant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is efÔ¨Åcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cascaded CNNs to locate face regions, which is efÔ¨Åcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nefÔ¨Åciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 15\\nFaster R-CNN architecture to face detection, which validate\\nthe signiÔ¨Åcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. P EDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identiÔ¨Åcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='which has a close relationship to pedestrian tracking [189],\\n[190], person re-identiÔ¨Åcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]‚Äì[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin ‚Äòplain‚Äô features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='main source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conÔ¨Ågurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modiÔ¨Åed the down-\\nstream classiÔ¨Åer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[18] to pedestrian detection [203]. They modiÔ¨Åed the down-\\nstream classiÔ¨Åer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand Ô¨Åne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='al. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE -ART MODELS ON CALTECH PEDESTRIAN DATASET . ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod Reasonable All Far Medium Near none partial heavy\\nCheckerboards+ [198] 17.1 68.4 100 58.3 5.1 15.6 31.4 78.4\\nLDCF++[S2] 15.2 67.1 100 58.4 5.4 13.3 33.3 76.2\\nSCF+AlexNet [210] 23.3 70.3 100 62.3 10.2 20.0 48.5 74.7\\nSA-FastRCNN [211] 9.7 62.6 100 51.8 0 7.7 24.8 64.3\\nMS-CNN [105] 10.0 61.0 97.2 49.1 2.6 8.2 19.2 60.0\\nDeepParts [204] 11.9 64.8 100 56.4 4.8 10.6 19.9 60.4\\nCompACT-Deep [195] 11.8 64.4 100 53.2 4.0 9.6 25.1 65.8\\nRPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassiÔ¨Åers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of conÔ¨Ådences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 Ô¨Åne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiÔ¨Åcation and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='45 Ô¨Åne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiÔ¨Åcation and\\nsimpliÔ¨Åcation is of signiÔ¨Åcance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classiÔ¨Åers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ACF detectors and SVM classiÔ¨Åers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely ‚ÄòPerson (clear identiÔ¨Åcations)‚Äô,\\n‚ÄòPerson? (unclear identiÔ¨Åcations)‚Äô and ‚ÄòPeople (large group of\\nindividuals)‚Äô, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10‚àí2 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The Ô¨Årst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classiÔ¨Åers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='candidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiÔ¨Åers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P ROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe Ô¨Årst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n‚Ä¢ Multi-task joint optimization and multi-modal infor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n‚Ä¢ Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speciÔ¨Åc\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='application, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n‚Ä¢Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all beneÔ¨Åcial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='jects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n‚Ä¢Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='selection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n‚Ä¢Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classiÔ¨Åers at latter stages can\\nhandle more difÔ¨Åcult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are Ô¨Åxed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='when training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n‚Ä¢ Unsupervised and weakly supervised learning. It‚Äôs\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand reÔ¨Åne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n‚Ä¢ Network optimization. Given speciÔ¨Åc applications and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='for achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n‚Ä¢ Network optimization. Given speciÔ¨Åc applications and\\nplatforms, it is signiÔ¨Åcant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n‚Ä¢3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='transportation and intelligent surveillance.\\n‚Ä¢3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n‚Ä¢ Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical Ô¨Çow [199] and LSTM [107] should'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='blur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical Ô¨Çow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. C ONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modiÔ¨Åcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieÔ¨Çy reviewed. Finally, we propose several promising future'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieÔ¨Çy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n‚ÄúObject detection with discriminatively trained part-based models,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚ÄúObject detection with discriminatively trained part-based models,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, ‚ÄúExample-based learning for view-based\\nhuman face detection,‚ÄùIEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39‚Äì51, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, ‚ÄúPedestrian detection:\\nAn evaluation of the state of the art,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y . Yoshinaga, ‚ÄúDetection of spicules on mammogram\\nbased on skeleton analysis.‚Äù IEEE Trans. Med. Imag. , vol. 15, no. 3,\\npp. 235‚Äì245, 1996.\\n[5] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, ‚ÄúCaffe: Convolutional architecture for\\nfast feature embedding,‚Äù in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation\\nwith deep convolutional neural networks,‚Äù in NIPS, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='fast feature embedding,‚Äù in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation\\nwith deep convolutional neural networks,‚Äù in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, ‚ÄúRealtime multi-person\\n2d pose estimation using part afÔ¨Ånity Ô¨Åelds,‚Äù in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, ‚ÄúA multi-scale cascade fully convolutional\\nnetwork face detector,‚Äù in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, ‚ÄúDeepdriving:\\nLearning affordance for direct perception in autonomous driving,‚Äù in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, ‚ÄúMulti-view 3d object\\ndetection network for autonomous driving,‚Äù in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, ‚ÄúEmbedded streaming\\ndeep neural networks accelerator with applications,‚Äù IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572‚Äì1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, ‚ÄúLow-complexity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Neural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572‚Äì1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, ‚ÄúLow-complexity\\napproximate convolutional neural networks,‚ÄùIEEE Trans. Neural Netw.\\n& Learning Syst. , vol. PP, no. 99, pp. 1‚Äì12, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n‚ÄúCost-sensitive learning of deep feature representations from imbal-\\nanced data.‚Äù IEEE Trans. Neural Netw. & Learning Syst. , vol. PP,\\nno. 99, pp. 1‚Äì15, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, ‚ÄúFeature extraction with deep\\nneural networks by a generalized discriminant analysis.‚Äù IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 23, no. 4, pp. 596‚Äì608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, ‚ÄúRich feature\\nhierarchies for accurate object detection and semantic segmentation,‚Äù\\nin CVPR, 2014.\\n[16] R. Girshick, ‚ÄúFast r-cnn,‚Äù in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‚ÄúYou only look'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in CVPR, 2014.\\n[16] R. Girshick, ‚ÄúFast r-cnn,‚Äù in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‚ÄúYou only look\\nonce: UniÔ¨Åed, real-time object detection,‚Äù in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-\\ntime object detection with region proposal networks,‚Äù in NIPS, 2015,\\npp. 91‚Äì99.\\n[19] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant key-\\npoints,‚Äù Int. J. of Comput. Vision , vol. 60, no. 2, pp. 91‚Äì110, 2004.\\n[20] N. Dalal and B. Triggs, ‚ÄúHistograms of oriented gradients for human\\ndetection,‚Äù in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, ‚ÄúAn extended set of haar-like features for\\nrapid object detection,‚Äù in ICIP, 2002.\\n[22] C. Cortes and V . Vapnik, ‚ÄúSupport vector machine,‚ÄùMachine Learning,\\nvol. 20, no. 3, pp. 273‚Äì297, 1995.\\n[23] Y . Freund and R. E. Schapire, ‚ÄúA desicion-theoretic generalization of\\non-line learning and an application to boosting,‚Äù J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663‚Äì671, 1997.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[23] Y . Freund and R. E. Schapire, ‚ÄúA desicion-theoretic generalization of\\non-line learning and an application to boosting,‚Äù J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663‚Äì671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n‚ÄúObject detection with discriminatively trained part-based models,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627‚Äì1645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, ‚ÄúThe pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),‚Äù 2008.\\n[26] Y . LeCun, Y . Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù Nature, vol.\\n521, no. 7553, pp. 436‚Äì444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, ‚ÄúPredicting eye Ô¨Åxations\\nusing convolutional neural networks,‚Äù in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, ‚ÄúLarge-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,‚Äù in CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='using convolutional neural networks,‚Äù in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, ‚ÄúLarge-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,‚Äù in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, ‚ÄúFace detection with the faster r-cnn,‚Äù\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun, ‚ÄúJoint cascade face\\ndetection and alignment,‚Äù in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, ‚ÄúSupervised transformer network\\nfor efÔ¨Åcient face detection,‚Äù in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, ‚ÄúA real-time\\npedestrian detector using deep learning for human-aware navigation,‚Äù\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y . Lin, ‚ÄúExploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassiÔ¨Åers,‚Äù in CVPR, 2016.\\n[34] P. Druzhkov and V . Kustikova, ‚ÄúA survey of deep learning methods and\\nsoftware tools for image classiÔ¨Åcation and object detection,‚Äù Pattern\\nRecognition and Image Anal. , vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, ‚ÄúHow we know universals the perception\\nof auditory and visual forms,‚ÄùThe Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127‚Äì147, 1947.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[35] W. Pitts and W. S. McCulloch, ‚ÄúHow we know universals the perception\\nof auditory and visual forms,‚ÄùThe Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127‚Äì147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‚ÄúLearning internal\\nrepresentation by back-propagation of errors,‚Äù Nature, vol. 323, no.\\n323, pp. 533‚Äì536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, ‚ÄúReducing the dimensionality\\nof data with neural networks,‚Äù Sci., vol. 313, pp. 504‚Äì507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainathet al., ‚ÄúDeep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,‚Äù IEEE Signal Process. Mag. , vol. 29,\\nno. 6, pp. 82‚Äì97, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:\\nA large-scale hierarchical image database,‚Äù in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='A large-scale hierarchical image database,‚Äù in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, ‚ÄúBinary coding of speech spectrograms using a deep auto-\\nencoder,‚Äù in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., ‚ÄúPhone recognition with\\nthe mean-covariance restricted boltzmann machine,‚Äù in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, ‚ÄúImproving neural networks by preventing co-\\nadaptation of feature detectors,‚Äù arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,‚Äù in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun,\\n‚ÄúOverfeat: Integrated recognition, localization and detection using\\nconvolutional networks,‚Äù arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚ÄúOverfeat: Integrated recognition, localization and detection using\\nconvolutional networks,‚Äù arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with\\nconvolutions,‚Äù in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks\\nfor large-scale image recognition,‚Äù arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\\nrecognition,‚Äù in CVPR, 2016.\\n[48] V . Nair and G. E. Hinton, ‚ÄúRectiÔ¨Åed linear units improve restricted\\nboltzmann machines,‚Äù in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al. , ‚ÄúWeakly supervised\\nobject recognition with convolutional neural networks,‚Äù in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, ‚ÄúLearning and transferring\\nmid-level image representations using convolutional neural networks,‚Äù\\nin CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, ‚ÄúLearning and transferring\\nmid-level image representations using convolutional neural networks,‚Äù\\nin CVPR, 2014.\\n[51] F. M. Wadley, ‚ÄúProbit analysis: a statistical treatment of the sigmoid\\nresponse curve,‚Äù Annals of the Entomological Soc. of America , vol. 67,\\nno. 4, pp. 549‚Äì553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y . LeCun et al. , ‚ÄúLearning invariant\\nfeatures through topographic Ô¨Ålter maps,‚Äù in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y .-L. Boureau, K. Gregor, M. Mathieu,\\nand Y . LeCun, ‚ÄúLearning convolutional feature hierarchies for visual\\nrecognition,‚Äù in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, ‚ÄúDeconvolu-\\ntional networks,‚Äù in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, ‚ÄúLearning deconvolution network for\\nsemantic segmentation,‚Äù in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, ‚ÄúPlant leaf iden-\\ntiÔ¨Åcation via a growing convolution neural network with progressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantic segmentation,‚Äù in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, ‚ÄúPlant leaf iden-\\ntiÔ¨Åcation via a growing convolution neural network with progressive\\nsample learning,‚Äù in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V . Lempitsky, ‚ÄúNeural codes\\nfor image retrieval,‚Äù in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y . Zhang, and J. Li,\\n‚ÄúDeep learning for content-based image retrieval: A comprehensive\\nstudy,‚Äù in ACM MM, 2014.\\n[59] D. Tom `e, F. Monti, L. BarofÔ¨Åo, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, ‚ÄúDeep convolutional neural networks for pedestrian detec-\\ntion,‚Äù Signal Process.: Image Commun. , vol. 47, pp. 482‚Äì489, 2016.\\n[60] Y . Xiang, W. Choi, Y . Lin, and S. Savarese, ‚ÄúSubcategory-aware\\nconvolutional neural networks for object proposals and detection,‚Äù in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, ‚ÄúPedestrian\\ndetection based on fast r-cnn and batch normalization,‚Äù in ICIC, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='WACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, ‚ÄúPedestrian\\ndetection based on fast r-cnn and batch normalization,‚Äù in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng,\\n‚ÄúMultimodal deep learning,‚Äù in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y .-G. Jiang, H. Ye, and X. Xue, ‚ÄúModeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classiÔ¨Å-\\ncation,‚Äù in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúSpatial pyramid pooling in deep\\nconvolutional networks for visual recognition,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 9, pp. 1904‚Äì1916, 2015.\\n[65] Y . Li, K. He, J. Sun et al., ‚ÄúR-fcn: Object detection via region-based\\nfully convolutional networks,‚Äù in NIPS, 2016, pp. 379‚Äì387.\\n[66] T.-Y . Lin, P. Doll ¬¥ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, ‚ÄúFeature pyramid networks for object detection,‚Äù in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ¬¥ar, and R. B. Girshick, ‚ÄúMask r-cnn,‚Äù in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Belongie, ‚ÄúFeature pyramid networks for object detection,‚Äù in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ¬¥ar, and R. B. Girshick, ‚ÄúMask r-cnn,‚Äù in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, ‚ÄúScalable object\\ndetection using deep neural networks,‚Äù in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon, ‚ÄúAttentionnet:\\nAggregating weak directions for accurate object detection,‚Äù in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, ‚ÄúG-cnn: an iterative grid\\nbased object detector,‚Äù in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\\nA. C. Berg, ‚ÄúSsd: Single shot multibox detector,‚Äù in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, ‚ÄúYolo9000: better, faster, stronger,‚Äù\\narXiv:1612.08242, 2016.\\n[73] C. Y . Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, ‚ÄúDssd:\\nDeconvolutional single shot detector,‚Äù arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, ‚ÄúDsod:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deconvolutional single shot detector,‚Äù arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, ‚ÄúDsod:\\nLearning deeply supervised object detectors from scratch,‚Äù in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, ‚ÄúTransforming auto-\\nencoders,‚Äù in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, ‚ÄúLearning invariance\\nthrough imitation,‚Äù in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, ‚ÄúHistograms of sparse codes for object\\ndetection,‚Äù in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n‚ÄúSelective search for object recognition,‚Äù Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154‚Äì171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun, ‚ÄúPedestrian\\ndetection with unsupervised multi-stage feature learning,‚Äù in CVPR,\\n2013.\\n[80] P. Kr ¬®ahenb¬®uhl and V . Koltun, ‚ÄúGeodesic object proposals,‚Äù in ECCV,\\n2014.\\n[81] P. Arbel ¬¥aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2013.\\n[80] P. Kr ¬®ahenb¬®uhl and V . Koltun, ‚ÄúGeodesic object proposals,‚Äù in ECCV,\\n2014.\\n[81] P. Arbel ¬¥aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n‚ÄúMultiscale combinatorial grouping,‚Äù in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll ¬¥ar, ‚ÄúEdge boxes: Locating object proposals\\nfrom edges,‚Äù in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, ‚ÄúDeepbox: Learning objectness\\nwith convolutional networks,‚Äù in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll ¬¥ar, ‚ÄúLearning to\\nreÔ¨Åne object segments,‚Äù in ECCV, 2016.\\n[85] Y . Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, ‚ÄúImproving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,‚Äù in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel ¬¥aez, and J. Malik, ‚ÄúLearning rich features\\nfrom rgb-d images for object detection and segmentation,‚Äù in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='from rgb-d images for object detection and segmentation,‚Äù in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., ‚ÄúDeepid-net: Deformable deep convolutional\\nneural networks for object detection,‚Äù in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, ‚ÄúR-cnn minus r,‚Äù arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, ‚ÄúBeyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,‚Äù\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S ¬¥anchez, and T. Mensink, ‚ÄúImproving the Ô¨Åsher kernel\\nfor large-scale image classiÔ¨Åcation,‚Äù in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y . Gong, ‚ÄúRestructuring of deep neural network\\nacoustic models with singular value decomposition.‚Äù in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-time\\nobject detection with region proposal networks,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 39, no. 6, pp. 1137‚Äì1149, 2017.\\n[93] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚ÄúRethink-\\ning the inception architecture for computer vision,‚Äù in CVPR, 2016.\\n[94] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll ¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in\\ncontext,‚Äù in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, ‚ÄúInside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,‚Äù in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, ‚ÄúPixelwise instance segmentation with a\\ndynamically instantiated network,‚Äù in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, ‚ÄúInstance-aware semantic segmentation via'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[96] A. Arnab and P. H. S. Torr, ‚ÄúPixelwise instance segmentation with a\\ndynamically instantiated network,‚Äù in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, ‚ÄúInstance-aware semantic segmentation via\\nmulti-task network cascades,‚Äù in CVPR, 2016.\\n[98] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, ‚ÄúFully convolutional instance-\\naware semantic segmentation,‚Äù in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n‚ÄúSpatial transformer networks,‚Äù in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, ‚ÄúStuffnet: Using stuffto\\nimprove object detection,‚Äù in WACV, 2017.\\n[101] T. Kong, A. Yao, Y . Chen, and F. Sun, ‚ÄúHypernet: Towards accurate\\nregion proposal generation and joint object detection,‚Äù in CVPR, 2016.\\n[102] A. Pentina, V . Sharmanska, and C. H. Lampert, ‚ÄúCurriculum learning\\nof multiple tasks,‚Äù in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, ‚ÄúRotating your\\nface using multi-task deep neural network,‚Äù in CVPR, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of multiple tasks,‚Äù in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, ‚ÄúRotating your\\nface using multi-task deep neural network,‚Äù in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, ‚ÄúMulti-stage object\\ndetection with group recursive learning,‚Äù arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, ‚ÄúA uniÔ¨Åed multi-scale\\ndeep convolutional neural network for fast object detection,‚Äù in ECCV,\\n2016.\\n[106] Y . Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, ‚Äúsegdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,‚Äù in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, ‚ÄúScene labeling\\nwith lstm recurrent neural networks,‚Äù in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, ‚ÄúLearning to detect and\\nlocalize many objects from few examples,‚Äù arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, ‚ÄúGated bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localize many objects from few examples,‚Äù arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, ‚ÄúGated bi-\\ndirectional cnn for object detection,‚Äù in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, ‚ÄúObject detection via a multi-region and\\nsemantic segmentation-aware cnn model,‚Äù in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, ‚ÄúBidirectional recurrent neural net-\\nworks,‚Äù IEEE Trans. Signal Process. , vol. 45, pp. 2673‚Äì2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y . Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll ¬¥ar, ‚ÄúA multipath network for object detection,‚Äù\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, ‚ÄúTraining region-based\\nobject detectors with online hard example mining,‚Äù in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, ‚ÄúObject detection\\nnetworks on convolutional feature maps,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476‚Äì1481, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, ‚ÄúObject detection\\nnetworks on convolutional feature maps,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476‚Äì1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, ‚ÄúFactors in Ô¨Ånetuning\\ndeep model for object detection with long-tail distribution,‚Äù in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y . Cheon, and M. Park, ‚ÄúPvanet:\\nLightweight deep neural networks for real-time object detection,‚Äù\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, ‚ÄúUnderstanding and\\nimproving convolutional neural networks via concatenated rectiÔ¨Åed\\nlinear units,‚Äù in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, ‚ÄúDeep neural networks for object\\ndetection,‚Äù in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll ¬¥ar, ‚ÄúLearning to segment object\\ncandidates,‚Äù in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, ‚ÄúScalable,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[119] P. O. Pinheiro, R. Collobert, and P. Doll ¬¥ar, ‚ÄúLearning to segment object\\ncandidates,‚Äù in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, ‚ÄúScalable,\\nhigh-quality object detection,‚Äù arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n‚ÄúThe pascal visual object classes challenge 2012 (voc2012) results\\n(2012),‚Äù in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, ‚ÄúVisualizing and understanding convolu-\\ntional networks,‚Äù in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll ¬¥ar, Z. Tu, and K. He, ‚ÄúAggregated residual\\ntransformations for deep neural networks,‚Äù in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei,\\n‚ÄúDeformable convolutional networks,‚Äù arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, ‚ÄúAutocollage,‚ÄùACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847‚Äì852, 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚ÄúDeformable convolutional networks,‚Äù arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, ‚ÄúAutocollage,‚ÄùACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847‚Äì852, 2006.\\n[126] C. Jung and C. Kim, ‚ÄúA uniÔ¨Åed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,‚Äù IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272‚Äì1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y . Chien, ‚ÄúReal-time salient object\\ndetection with a minimum spanning tree,‚Äù in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, ‚ÄúTop-down visual saliency via joint crf and\\ndictionary learning,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576‚Äì588, 2017.\\n[129] P. L. Rosin, ‚ÄúA simple method for detecting salient regions,‚Äù Pattern\\nRecognition, vol. 42, no. 11, pp. 2363‚Äì2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n‚ÄúLearning to detect a salient object,‚Äù IEEE Trans. Pattern Anal. Mach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Recognition, vol. 42, no. 11, pp. 2363‚Äì2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n‚ÄúLearning to detect a salient object,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353‚Äì367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, ‚ÄúFully convolutional networks\\nfor semantic segmentation,‚Äù in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, ‚ÄúDiscriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, pp. 989‚Äì1005, 2009.\\n[133] S. Xie and Z. Tu, ‚ÄúHolistically-nested edge detection,‚Äù in ICCV, 2015.\\n[134] M. K ¬®ummerer, L. Theis, and M. Bethge, ‚ÄúDeep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,‚Äù\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, ‚ÄúSalicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,‚Äù\\nin ICCV, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='arXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, ‚ÄúSalicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,‚Äù\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, ‚ÄúDeep networks for saliency\\ndetection via local estimation and global search,‚Äù in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, ‚ÄúWeakly supervised top-down\\nsalient object detection,‚Äù arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, ‚ÄúSaliency detection by\\nmulti-context deep learning,‚Äù in CVPR, 2015.\\n[139] C ¬∏ . Bak, A. Erdem, and E. Erdem, ‚ÄúTwo-stream convolutional networks\\nfor dynamic saliency prediction,‚Äù arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, ‚ÄúSupercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,‚Äù\\nInt. J. of Comput. Vision , vol. 115, no. 3, pp. 330‚Äì344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Int. J. of Comput. Vision , vol. 115, no. 3, pp. 330‚Äì344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and\\nJ. Wang, ‚ÄúDeepsaliency: Multi-task deep neural network model for\\nsalient object detection,‚Äù IEEE Trans. Image Process. , vol. 25, no. 8,\\npp. 3919‚Äì3930, 2016.\\n[142] Y . Tang and X. Wu, ‚ÄúSaliency detection via combining region-level\\nand pixel-level predictions with cnns,‚Äù in ECCV, 2016.\\n[143] G. Li and Y . Yu, ‚ÄúDeep contrast learning for salient object detection,‚Äù\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, ‚ÄúEdge preserving and\\nmulti-scale contextual neural network for salient object detection,‚Äù\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, ‚ÄúA deep multi-level\\nnetwork for saliency prediction,‚Äù in ICPR, 2016.\\n[146] G. Li and Y . Yu, ‚ÄúVisual saliency detection based on multiscale deep\\ncnn features,‚Äù IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012‚Äì\\n5024, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[146] G. Li and Y . Yu, ‚ÄúVisual saliency detection based on multiscale deep\\ncnn features,‚Äù IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012‚Äì\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O‚ÄôConnor,\\n‚ÄúShallow and deep convolutional networks for saliency prediction,‚Äù in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, ‚ÄúRecurrent attentional networks for\\nsaliency detection,‚Äù in CVPR, 2016.\\n[149] Y . Tang, X. Wu, and W. Bu, ‚ÄúDeeply-supervised recurrent convolutional\\nneural network for saliency detection,‚Äù in ACM MM, 2016.\\n[150] X. Li, Y . Li, C. Shen, A. Dick, and A. Van Den Hengel, ‚ÄúContextual\\nhypergraph modeling for salient object detection,‚Äù in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, ‚ÄúGlobal\\ncontrast based salient region detection,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569‚Äì582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, ‚ÄúSalient object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='contrast based salient region detection,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569‚Äì582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, ‚ÄúSalient object\\ndetection: A discriminative regional feature integration approach,‚Äù in\\nCVPR, 2013.\\n[153] G. Lee, Y .-W. Tai, and J. Kim, ‚ÄúDeep saliency with encoded low level\\ndistance map and high level features,‚Äù in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n‚ÄúNon-local deep features for salient object detection,‚Äù in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n‚ÄúDeeply supervised salient object detection with short connections,‚Äù\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, ‚ÄúHierarchical saliency detection,‚Äù in\\nCVPR, 2013.\\n[157] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, ‚ÄúThe secrets of\\nsalient object segmentation,‚Äù in CVPR, 2014.\\n[158] V . Movahedi and J. H. Elder, ‚ÄúDesign and perceptual validation of\\nperformance measures for salient object segmentation,‚Äù in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, ‚ÄúSalient object detection:\\nA benchmark,‚Äù IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706‚Äì\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, ‚ÄúGraphical representation for\\nheterogeneous face recognition,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301‚Äì312, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, ‚ÄúGraphical representation for\\nheterogeneous face recognition,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301‚Äì312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, ‚ÄúFace recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,‚Äù in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, ‚ÄúFace sketchcphoto synthesis\\nand retrieval using sparse representation,‚Äù IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213‚Äì1226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, ‚ÄúA comprehensive survey\\nto face hallucination,‚Äù Int. J. of Comput. Vision , vol. 106, no. 1, pp.\\n9‚Äì30, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, ‚ÄúMultiple\\nrepresentations-based face sketch-photo synthesis.‚ÄùIEEE Trans. Neural\\nNetw. & Learning Syst. , vol. 27, no. 11, pp. 2201‚Äì2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, ‚ÄúAutomatic facial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Netw. & Learning Syst. , vol. 27, no. 11, pp. 2201‚Äì2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, ‚ÄúAutomatic facial\\nexpression recognition system using deep network-based data fusion,‚Äù\\nIEEE Trans. Cybern. , vol. 48, pp. 103‚Äì114, 2018.\\n[166] P. Viola and M. Jones, ‚ÄúRobust real-time face detection,‚Äù Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137‚Äì154, 2004.\\n[167] J. Yu, Y . Jiang, Z. Wang, Z. Cao, and T. Huang, ‚ÄúUnitbox: An advanced\\nobject detection network,‚Äù in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, ‚ÄúMulti-view face detection\\nusing deep convolutional neural networks,‚Äù in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, ‚ÄúFrom facial parts responses\\nto face detection: A deep learning approach,‚Äù in ICCV, 2015.\\n[170] S. Yang, Y . Xiong, C. C. Loy, and X. Tang, ‚ÄúFace detection through\\nscale-friendly deep convolutional networks,‚Äù in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, ‚ÄúScale-aware face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale-friendly deep convolutional networks,‚Äù in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, ‚ÄúScale-aware face\\ndetection,‚Äù in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y . Wang, ‚ÄúFace r-cnn,‚Äù arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, ‚ÄúFace detection using deep learning: An\\nimproved faster rcnn approach,‚Äù arXiv:1701.08289, 2017.\\n[174] L. Huang, Y . Yang, Y . Deng, and Y . Yu, ‚ÄúDensebox: Unifying landmark\\nlocalization with end to end object detection,‚Äù arXiv:1509.04874, 2015.\\n[175] Y . Li, B. Sun, T. Wu, and Y . Wang, ‚Äúface detection with end-to-end\\nintegration of a convnet and a 3d model,‚Äù in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, ‚ÄúJoint face detection and\\nalignment using multitask cascaded convolutional networks,‚Äù IEEE\\nSignal Process. Lett. , vol. 23, no. 10, pp. 1499‚Äì1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, ‚ÄúCompact convolutional neural\\nnetwork cascadefor face detection,‚Äù in CEUR Workshop, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Signal Process. Lett. , vol. 23, no. 10, pp. 1499‚Äì1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, ‚ÄúCompact convolutional neural\\nnetwork cascadefor face detection,‚Äù in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, ‚ÄúJoint training of cascaded cnn for\\nface detection,‚Äù in CVPR, 2016.\\n[179] V . Jain and E. Learned-Miller, ‚ÄúFddb: A benchmark for face detection\\nin unconstrained settings,‚Äù Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, ‚ÄúA convolutional neural\\nnetwork cascade for face detection,‚Äù in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúAggregate channel features for\\nmulti-view face detection,‚Äù in IJCB, 2014.\\n[182] N. Marku Àás, M. Frljak, I. S. Pand Àázi¬¥c, J. Ahlberg, and R. Forchheimer,\\n‚ÄúObject detection with pixel intensity comparisons organized in deci-\\nsion trees,‚Äù arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, ‚ÄúFace\\ndetection without bells and whistles,‚Äù in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sion trees,‚Äù arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, ‚ÄúFace\\ndetection without bells and whistles,‚Äù in ECCV, 2014.\\n[184] J. Li and Y . Zhang, ‚ÄúLearning surf cascade for fast and accurate object\\ndetection,‚Äù in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, ‚ÄúA fast and accurate unconstrained\\nface detector,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211‚Äì223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúConvolutional channel features,‚Äù\\nin ICCV, 2015.\\n[187] R. Ranjan, V . M. Patel, and R. Chellappa, ‚ÄúHyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,‚Äù arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, ‚ÄúFinding tiny faces,‚Äù in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, ‚ÄúMultiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,‚Äù IEEE Trans.\\nImage Process., vol. 27, pp. 1361‚Äì1375, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[189] Z. Jiang and D. Q. Huynh, ‚ÄúMultiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,‚Äù IEEE Trans.\\nImage Process., vol. 27, pp. 1361‚Äì1375, 2018.\\n[190] D. Gavrila and S. Munder, ‚ÄúMulti-cue pedestrian detection and tracking\\nfrom a moving vehicle,‚Äù Int. J. of Comput. Vision , vol. 73, pp. 41‚Äì59,\\n2006.\\n[191] S. Xu, Y . Cheng, K. Gu, Y . Yang, S. Chang, and P. Zhou, ‚ÄúJointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidentiÔ¨Åcation,‚Äù in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, ‚ÄúStepwise metric promotion for unsuper-\\nvised video person re-identiÔ¨Åcation,‚Äù in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, ‚ÄúCooperative robots to observe\\nmoving targets: Review,‚Äù IEEE Trans. Cybern. , vol. 48, pp. 187‚Äì198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets robotics:\\nThe kitti dataset,‚Äù Int. J. of Robotics Res. , vol. 32, pp. 1231‚Äì1237,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets robotics:\\nThe kitti dataset,‚Äù Int. J. of Robotics Res. , vol. 32, pp. 1231‚Äì1237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, ‚ÄúLearning complexity-aware\\ncascades for deep pedestrian detection,‚Äù in ICCV, 2015.\\n[196] Y . Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning strong parts\\nfor pedestrian detection,‚Äù in CVPR, 2015.\\n[197] P. Doll ¬¥ar, R. Appel, S. Belongie, and P. Perona, ‚ÄúFast feature pyramids\\nfor object detection,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 36,\\nno. 8, pp. 1532‚Äì1545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, ‚ÄúFiltered channel features for\\npedestrian detection,‚Äù in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, ‚ÄúPedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243‚Äì1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, ‚ÄúDiscriminatively trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243‚Äì1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, ‚ÄúDiscriminatively trained\\nand-or graph models for object shape detection,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 5, pp. 959‚Äì972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, ‚ÄúHandling\\nocclusions with franken-classiÔ¨Åers,‚Äù in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, ‚ÄúDetection and tracking of\\noccluded people,‚Äù Int. J. of Comput. Vision, vol. 110, pp. 58‚Äì69, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, ‚ÄúIs faster r-cnn doing well for\\npedestrian detection?‚Äù in ECCV, 2016.\\n[204] Y . Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning strong parts\\nfor pedestrian detection,‚Äù in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, ‚ÄúMultispectral deep\\nneural networks for pedestrian detection,‚Äù arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúPedestrian detection aided by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='neural networks for pedestrian detection,‚Äù arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúPedestrian detection aided by\\ndeep learning semantic tasks,‚Äù in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, ‚ÄúFused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,‚Äù in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, ‚ÄúPushing\\nthe limits of deep cnns for pedestrian detection,‚Äù IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom ¬¥e, L. Bondi, L. BarofÔ¨Åo, S. Tubaro, E. Plebani, and D. Pau,\\n‚ÄúReduced memory region based deep convolutional neural network\\ndetection,‚Äù in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, ‚ÄúTaking a deeper\\nlook at pedestrians,‚Äù in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, ‚ÄúScale-aware fast\\nr-cnn for pedestrian detection,‚Äù arXiv:1510.08160, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='look at pedestrians,‚Äù in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, ‚ÄúScale-aware fast\\nr-cnn for pedestrian detection,‚Äù arXiv:1510.08160, 2015.\\n[212] Y . Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, ‚ÄúVisual-textual\\njoint relevance learning for tag-based social image search,‚ÄùIEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363‚Äì376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y . Chen, ‚ÄúRon: Reverse\\nconnection with objectness prior networks for object detection,‚Äù in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y . Bengio, ‚ÄúGenerative adversarial nets,‚Äù\\nin NIPS, 2014.\\n[215] Y . Fang, K. Kuan, J. Lin, C. Tan, and V . Chandrasekhar, ‚ÄúObject\\ndetection meets knowledge graphs,‚Äù in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, ‚ÄúSaliency-based sequential\\nimage attention with multiset prediction,‚Äù in NIPS, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection meets knowledge graphs,‚Äù in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, ‚ÄúSaliency-based sequential\\nimage attention with multiset prediction,‚Äù in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, ‚ÄúLearning detection with diverse\\nproposals,‚Äù in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, ‚ÄúEnd-to-end\\nmemory networks,‚Äù in NIPS, 2015.\\n[219] P. Dabkowski and Y . Gal, ‚ÄúReal time image saliency for black box\\nclassiÔ¨Åers,‚Äù in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúCraft objects from images,‚Äù in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V . Bogolin, and M. Leordeanu, ‚ÄúUnsupervised learning\\nfrom video to detect foreground objects in single images,‚Äù in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, ‚ÄúWeakly supervised object\\nlocalization with latent category learning,‚Äù in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V . Ferrari,\\n‚ÄúTraining object class detectors with click supervision,‚Äù inCVPR, 2017.\\n[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, ‚ÄúSpeed/accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, ‚ÄúSpeed/accuracy\\ntrade-offs for modern convolutional object detectors,‚Äù in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, ‚ÄúMimicking very efÔ¨Åcient network for object\\ndetection,‚Äù in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a\\nneural network,‚Äù Comput. Sci., vol. 14, no. 7, pp. 38‚Äì39, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY . Bengio, ‚ÄúFitnets: Hints for thin deep nets,‚Äù Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y . Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, ‚Äú3d object proposals for accurate object class detection,‚Äù\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, ‚ÄúVisual-inertial-semantic scene repre-\\nsentation for 3d object detection,‚Äù in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[229] J. Dong, X. Fei, and S. Soatto, ‚ÄúVisual-inertial-semantic scene repre-\\nsentation for 3d object detection,‚Äù in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n‚ÄúObject detection in videos with tubelet proposal networks,‚Äù in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='His research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor‚Äôs degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in ArtiÔ¨Åcial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='formation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='I am writing to present our proposal for a strategic skilling partnership between Krish Naik\\nAcademy (KrishAI Technologies Pvt. Ltd.) and the Nipuna Karnataka Scheme, aimed at\\ntransforming Karnataka into India‚Äôs AI Talent Engine.\\nAs a proud native of Gulbarga, Karnataka, my journey in AI began in 2017, overcoming resource\\nconstraints to build one of the world‚Äôs largest AI and Data Science learning communities, now\\nwith over 1.5 million learners trained across 180+ countries and 60,000+ successful career\\ntransitions. Our mission is to make AI & Data Science education accessible, affordable, and\\nindustry-aligned, ensuring every learner, regardless of location, has a pathway to global\\ntechnology careers.\\nOur proposal aligns directly with Nipuna Karnataka‚Äôs vision of skilling 1 crore youth by 2030,\\nfocusing on:\\nComprehensive AI & Data Skilling ‚Äì Industry-aligned training tracks including Data Analyst,\\nData Scientist, GenAI Developer, AI Engineer (MLOps), and Big Data Engineer.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='focusing on:\\nComprehensive AI & Data Skilling ‚Äì Industry-aligned training tracks including Data Analyst,\\nData Scientist, GenAI Developer, AI Engineer (MLOps), and Big Data Engineer.\\nPlacement-Focused Learning ‚Äì Leveraging our AI Mock Interview Platform, structured\\nhiring network, and measurable KPIs to ensure employability.\\nRural & Inclusive Outreach ‚Äì Hybrid delivery in Kannada + English, targeting Tier-2/3\\ndistricts and underrepresented groups.\\nFaculty Enablement ‚Äì Train-the-Trainer programs for government institution educators to\\ncreate a self-sustaining AI talent ecosystem.\\nBy combining our proven expertise in large-scale, outcome-driven AI skilling with the\\nGovernment of Karnataka‚Äôs vision, we can build a future-ready workforce, strengthen\\nKarnataka‚Äôs leadership in technology, and make the state a national lighthouse for AI skills\\ndevelopment.\\nWe are excited about the possibility of contributing to this transformative journey and look'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Karnataka‚Äôs leadership in technology, and make the state a national lighthouse for AI skills\\ndevelopment.\\nWe are excited about the possibility of contributing to this transformative journey and look\\nforward to discussing how we can work together to achieve the ambitious goals of the Nipuna\\nKarnataka Scheme.\\nThank you for your time and consideration.\\nWarm regards,\\nKrish Naik\\n10 August, 2025\\nTo Hon‚Äôble Minister Priyank Kharge\\nIT BT Minister Karnataka\\nKrish Naik\\nFounder & CEO, KrishAI Technologies Pvt. Ltd.\\ncontact@krishnaik.in\\n+91 88673 53949\\nLinkedIn | YouTube')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe92ea",
   "metadata": {},
   "source": [
    "### embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ae3031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\YTRAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543614c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x26010000ec0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9e3b",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c276d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2604e956cf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5d2c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n‚àödk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n‚àödk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1‚àödk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1‚àödk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ‚àà Rdmodel√ódk , WK\\ni ‚àà Rdmodel√ódk , WV\\ni ‚àà Rdmodel√ódv\\nand WO ‚àà Rhdv√ódmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by ‚àödmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ¬∑ d) O(1) O(1)\\nRecurrent O(n ¬∑ d2) O(n) O(n)\\nConvolutional O(k ¬∑ n ¬∑ d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r ¬∑ n ¬∑ d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to 10000 ¬∑ 2œÄ. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ‚àà Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k ¬∑ n ¬∑ d + n ¬∑ d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with Œ≤1 = 0.9, Œ≤2 = 0.98 and œµ = 10‚àí9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d‚àí0.5\\nmodel ¬∑ min(step_num‚àí0.5, step_num ¬∑ warmup_steps‚àí1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 ¬∑ 1020\\nGNMT + RL [38] 24.6 39.92 2.3 ¬∑ 1019 1.4 ¬∑ 1020\\nConvS2S [9] 25.16 40.46 9.6 ¬∑ 1018 1.5 ¬∑ 1020\\nMoE [32] 26.03 40.56 2.0 ¬∑ 1019 1.2 ¬∑ 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 ¬∑ 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 ¬∑ 1020 1.1 ¬∑ 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 ¬∑ 1019 1.2 ¬∑ 1021\\nTransformer (base model) 27.3 38.1 3.3 ¬∑ 1018\\nTransformer (big) 28.4 41.8 2.3 ¬∑ 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value œµls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty Œ± = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop œµls\\ntrain PPL BLEU params\\nsteps (dev) (dev) √ó106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and Œ± = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770‚Äì778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735‚Äì1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832‚Äì841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832‚Äì841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] ≈Åukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313‚Äì330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152‚Äì159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433‚Äì440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433‚Äì440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929‚Äì1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929‚Äì1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440‚Äì2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104‚Äì3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434‚Äì443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ‚àó\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniÔ¨Åe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciÔ¨Åc training strate gies enhance model learn-\\ning eÔ¨Éciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diÔ¨Éculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='example generation to improve the semantic richness and sam ple diÔ¨Éculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task Ô¨Åne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking Ô¨Årst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our Ô¨Åndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs‚Äô generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Our model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiÔ¨Åcantly enhan ced agent systems\\n‚àó https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on Ô¨Åne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiÔ¨Åcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliÔ¨Åed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures‚Äîwhere causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture‚Äîseve ral approaches have'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the limitation of decoder-only architectures‚Äîwhere causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture‚Äîseve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nÔ¨Åes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ‚Äùsignal states‚Äù repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and Ô¨Ånally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe Ô¨Ånal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiÔ¨Åed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='dient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever‚Äôs[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos Ô¨Åltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model‚Äôs semantic understanding, we designed a uniÔ¨Åed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='enhance the model‚Äôs semantic understanding, we designed a uniÔ¨Åed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neÔ¨Écient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiÔ¨Åcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciÔ¨Åc require ments of retrieval,\\nNLI, and classiÔ¨Åcation tasks, enabling eÔ¨Äective feature extractio n from heterogeneous\\ndata sources, signiÔ¨Åcantly beneÔ¨Åting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task‚Äôs charact eristics, optimizing\\nmodel training eÔ¨Éciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ‚ÄùData Grouping Strategy‚Äù, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diÔ¨Éculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the Ô¨Årst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eÔ¨Äectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n‚Ä¢ We propose a uniÔ¨Åed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eÔ¨Éciency in model training ;\\n‚Ä¢ We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniÔ¨Åcantly enhance the quality of training corpora, thereby impro ving model‚Äôs\\nrobustness and generalization capabilities;\\n‚Ä¢ We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='capability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiÔ¨Åcation (CLS), pa ir classiÔ¨Åcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eÔ¨Äectiveness;\\n‚Ä¢ Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eÔ¨Äectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneÔ¨Åt from optimized LLM architectures‚Äîsuc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]‚Äîcombined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model‚Äôs dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by Ô¨Åne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiÔ¨Åcantly fe wer parameters.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='followed by Ô¨Åne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiÔ¨Åcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised Ô¨Åne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciÔ¨Åc Ô¨Åne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diÔ¨Äeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiÔ¨Åcantly enhancing domain data discrimina tion. Xiaobu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='we also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiÔ¨Åcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniÔ¨Åed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diÔ¨Äerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='for improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eÔ¨Äectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiÔ¨Åed prompting strategy by assigning document-s peciÔ¨Åc\\nroles to simulate potential users querying that document, enabling LLMs to generate'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Embedding[34] designs a diversiÔ¨Åed prompting strategy by assigning document-s peciÔ¨Åc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos Ô¨Åltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever‚Äôs strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos Ô¨Ålter ing to eliminate\\nfalse negatives.\\n3 UniÔ¨Åed Multi-task Learning Framework'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos Ô¨Ålter ing to eliminate\\nfalse negatives.\\n3 UniÔ¨Åed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiÔ¨Åcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniÔ¨Åed strategy to eÔ¨Äectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniÔ¨Åed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiÔ¨Åcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciÔ¨Åc losses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciÔ¨Åc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eÔ¨Äectively enhance embedding capa bilities. We propose\\na uniÔ¨Åed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiÔ¨Åcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk‚Äôs components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiÔ¨Åcantly constrainin g tokens‚Äô ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiÔ¨Åcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiÔ¨Åcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce Ô¨Ånal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuÔ¨Écient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n‚Ä¢ Title-Body/Abstract ‚ÄùTitle-Body/Abstract‚Äù type data primarily consists of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='categories of training data used for transformation and their pro cessing procedures:\\n‚Ä¢ Title-Body/Abstract ‚ÄùTitle-Body/Abstract‚Äù type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n‚Ä¢ Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriÔ¨Åcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ‚ÄùSupports‚Äù\\nor ‚ÄùRefutes‚Äù. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ‚ÄùSupports‚Äù, the text is treated as a\\npositive sample; for evidence labeled as ‚ÄùRefutes‚Äù, it is converted in to a negative\\nsample.\\n‚Ä¢ Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ‚ÄùQuestion/Query/User‚Äù portion is converted into que ries, while the\\n‚ÄùAnswer/Response/Assistant‚Äù portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n‚Ä¢ STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='mantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiÔ¨Åcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ‚Äùyes‚Äù/‚Äùtrue‚Äù are mapped to a numerical va lue of 1, while\\n‚Äùno‚Äù/‚Äùfalse‚Äù are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n‚Ä¢ Textual Entailment Textual entailment further examines a model‚Äôs capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiÔ¨Åcation tasks encompass text categorization and sentiment classiÔ¨Åcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diÔ¨Äeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiÔ¨Åcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diÔ¨Äerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category‚Äîretrieval, NLI, and classiÔ¨Åcation‚Äîoperates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eÔ¨Éciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiÔ¨Åcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. SpeciÔ¨Åcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The Ô¨Ånal loss formulation is ex plicitly described'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='additional query-query loss term. SpeciÔ¨Åcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The Ô¨Ånal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = ‚àí 1\\nn\\n‚àë\\ni\\nlog esim(qi,d +\\ni )/œÑ\\nesim(qi,d +\\ni )/œÑ + ‚àë\\nj esim(qi,d ‚àí\\nj )/œÑ + ‚àë\\njÃ∏=i esim(qi,q j )/œÑ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n‚àë\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) ‚àí sim(xi, x j)\\nœÑ )) (2)\\n3.3.3 CLS\\nThe classiÔ¨Åcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiÔ¨Åcation datasets with limited categories may lead to false neg atives from items\\nof diÔ¨Äerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query‚Äôs\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='sample from other data instances, we check whether its label matc hes the current query‚Äôs\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = ‚àí 1\\nn\\n‚àë\\ni\\nlog esim(ti,t +\\ni )/œÑ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/œÑ +\\n‚àë\\nn\\nMASK(ti, t ‚àí\\ni,n ) ¬∑esim(ti,t ‚àí\\ni,n )/œÑ +\\n‚àë\\njÃ∏=i\\nMASK(ti, t j ) ¬∑esim(ti,t j )/œÑ +\\n‚àë\\njÃ∏=i\\n‚àë\\nn\\nMASK(ti, t ‚àí\\nj,n ) ¬∑esim(ti,t ‚àí\\nj,n )/œÑ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eÔ¨Éciency, thus developing eÔ¨Äective automated data sy nthesis methods has'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='portance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eÔ¨Éciency, thus developing eÔ¨Äective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiÔ¨Åcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eÔ¨Äectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diÔ¨Éculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diÔ¨Écu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciÔ¨Åed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reÔ¨Çect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='which represent relatively surface-level characteristics reÔ¨Çect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diÔ¨Äerences, should be recognized as semantic ally equivalent:\\n‚Ä¢ The cat chased the mouse.\\n‚Ä¢ The mouse was chased by the cat.\\nTo eÔ¨Äectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workÔ¨Çow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing WorkÔ¨Çow'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='semantic equivalence while introducing structural divergence. The prompt constraints\\nand workÔ¨Çow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing WorkÔ¨Çow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superÔ¨Åcial structural modiÔ¨Åcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diÔ¨Äerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation WorkÔ¨Çow\\nFigure 5: Hard Negative Synthesis WorkÔ¨Çow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eÔ¨Äort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciÔ¨Åc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tailored for retrieval datasets. Our domain-speciÔ¨Åc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciÔ¨Åc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion‚Äîwithout applying augmentation to prevent ambi-\\nguity; for classiÔ¨Åcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reÔ¨Åned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciÔ¨Åc\\nclustering patterns‚Äîsamples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with Ô¨Åle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='enable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lŒ±\\ni‚àë m\\nj=1 lŒ±\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed‚Äôs[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the Ô¨Årst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the Ô¨Ågure 6. Two key distinctions\\nare incorporated: Ô¨Årst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our Ô¨Åndings indicate that naively incorporating add itional data signiÔ¨Å-\\ncantly degrades retrieval performance.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='datasets, since our Ô¨Åndings indicate that naively incorporating add itional data signiÔ¨Å-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter Œ∑ is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for Œ∑,\\nwhile non-retrieval data constitutes 1 ‚àí Œ∑. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we Ô¨Årst apply an exponential scalin g factor Œ± , a mask fac-\\ntor M is then applied to Ô¨Ålter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n‚àë\\ni\\nMi ¬∑lŒ±\\ni\\nSnon ret =\\n‚àë\\ni\\n(1 ‚àí Mi) ¬∑lŒ±\\ni\\nwhere M i =\\n{\\n0 if di ‚àà RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Sret =\\n‚àë\\ni\\nMi ¬∑lŒ±\\ni\\nSnon ret =\\n‚àë\\ni\\n(1 ‚àí Mi) ¬∑lŒ±\\ni\\nwhere M i =\\n{\\n0 if di ‚àà RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using Œ∑ to derive the Ô¨Ånal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ Œ∑RET ¬∑lŒ±\\ni\\nSret\\nif di ‚àà RET,\\n(1‚àíŒ∑RET )¬∑lŒ±\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='passage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella‚Äôs[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface‚Äôs sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/Ô¨Åle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='4https://drive.google.com/Ô¨Åle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- ClassiÔ¨Åcation[58],\\nMassiveIntent-ClassiÔ¨Åcation[59], MassiveScenario-ClassiÔ¨Åcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiÔ¨Åcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='ClassiÔ¨Åcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and Ô¨Ålter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The Ô¨Ånal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iÔ¨Åc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the Ô¨Årst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconÔ¨Ågurations the same as in the Ô¨Årst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiÔ¨Åcation), considerin g data using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='conÔ¨Ågurations the same as in the Ô¨Årst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiÔ¨Åcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bÔ¨Çoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio Œ∑RET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-Ô¨Åle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-Ô¨Åle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA Ô¨Åne-tuning, we employ full-parameter Ô¨Åne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conÔ¨Ågurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter SpeciÔ¨Åcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bÔ¨Çoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='boards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eÔ¨Äectiveness of our\\napproach. Furthermore, under MTEB‚Äôs oÔ¨Écial ranking protocol, Q zhou-embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='both task-level and task-type average metrics, demonstrating the eÔ¨Äectiveness of our\\napproach. Furthermore, under MTEB‚Äôs oÔ¨Écial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reÔ¨Çect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliÔ¨Åed by the MTEB/c lassiÔ¨Åcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='NV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Qwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniÔ¨Åed multi-task framework comprising specialized data transform ation and training\\nstrategies, eÔ¨Äectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model‚Äôs generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task Ô¨Åne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='strategy comprising initial retrieval-focused training followed by fu ll-task Ô¨Åne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking Ô¨Årst on both leaderboards. Our Ô¨Åndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eÔ¨Äective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ‚ÄùSome simple eÔ¨Äective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.‚Äù In SIGIR‚Äô9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin RaÔ¨Äel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniÔ¨Åed text-to-text transformer. Journal of machine le arning research,\\n21(140):1‚Äì67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894‚Äì6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern¬¥ andez ¬¥Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern¬¥ andez ¬¥Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ‚ÄùLanguage models are few-s hot learners.‚Äù\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ‚ÄùF ine-tuning\\nllama for multi-stage text retrieval.‚Äù In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ‚ÄùRepetition improves language model embeddings.‚Äù a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Raghunathan. ‚ÄùRepetition improves language model embeddings.‚Äù a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ‚ÄùLlm2vec: Large languag e models are\\nsecretly powerful text encoders.‚Äù arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ‚ÄùJaspe r and stella:\\ndistillation of sota embedding models.‚Äù arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ‚ÄùBge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.‚Äù arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ‚ÄùLearning more eÔ¨Äective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.‚Äù arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ‚ÄùApproximate nearest neighbo r negative con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ‚ÄùApproximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.‚Äù arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ‚ÄùNv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.‚Äù arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiÔ¨Äerer, and Even Oldridge. ‚ÄùNV-Retriever: Improving text emb edding models\\nwith eÔ¨Äective hard-negative mining.‚Äù arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ‚ÄùQwen2 technical report.‚Äù arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas MuennighoÔ¨Ä, Defu L ian, and Jian-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[21] Team, Qwen. ‚ÄùQwen2 technical report.‚Äù arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas MuennighoÔ¨Ä, Defu L ian, and Jian-\\nYun Nie. ‚ÄùC-pack: Packed resources for general chinese embedd ings.‚Äù In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] MuennighoÔ¨Ä, Niklas, Nouamane Tazi, Lo¬® ƒ±c Magne, and Nils Reimers . ‚ÄùMteb: Mas-\\nsive text embedding benchmark.‚Äù arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ‚ÄùConan-embed ding: Gen-\\neral text embedding with more and better negative samples.‚Äù arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ‚ÄùAn information-theoretic perspective of tf‚Äìid f measures.‚Äù Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ‚ÄùSome simple eÔ¨Äectiv e approximations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ‚ÄùSome simple eÔ¨Äectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.‚Äù In SIGI R‚Äô94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ‚ÄùIndexing by latent semantic analysis.‚Äù Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, ShaÔ¨Åq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ‚ÄùSfrembedding-mistral: enhance text retrieval with tran sfer learning.‚Äù Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] MuennighoÔ¨Ä, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ‚ÄùGenerative representational instruction tun-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='multi-stage training, 2024[J].\\n[31] MuennighoÔ¨Ä, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ‚ÄùGenerative representational instruction tun-\\ning.‚Äù In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ‚ÄùQwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.‚Äù arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Through Foundation Models.‚Äù arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n‚ÄùRoformer: Enhanced transformer with rotary position embeddin g.‚Äù Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ‚ÄùRoot mean square layer norma lization.‚Äù Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ‚ÄùGlu variants improve transformer.‚Äù arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ‚ÄùPic-\\ncolo2: General text embedding with multi-task hybrid loss training.‚Äù a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ‚ÄùCircle loss: A uniÔ¨Åed perspective of pair similarit y op-\\ntimization.‚Äù In Proceedings of the IEEE/CVF conference on comput er vision and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Wang, and Yichen Wei. ‚ÄùCircle loss: A uniÔ¨Åed perspective of pair similarit y op-\\ntimization.‚Äù In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414‚Äì9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Generative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345‚Äì2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ‚ÄùUnn atural in-\\nstructions: Tuning language models with (almost) no human labor.‚Äù ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ‚ÄùApproximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.‚Äù arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiÔ¨Äerer, and Even Oldridge. ‚ÄùNV-Retriever: Improving text emb edding models\\nwith eÔ¨Äective hard-negative mining.‚Äù arXiv preprint arXiv:2407.15831 (2024).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='SchiÔ¨Äerer, and Even Oldridge. ‚ÄùNV-Retriever: Improving text emb edding models\\nwith eÔ¨Äective hard-negative mining.‚Äù arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ‚ÄùGecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.‚Äù URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='models, 2024.‚Äù URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciÔ¨Åc control and quality reÔ¨Ånement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics‚ÄìVolume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385‚Äì393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ‚ÄùLcqmc: A large-scale chinese question matching corpus.‚Äù In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ‚ÄùLcqmc: A large-scale chinese question matching corpus.‚Äù In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ‚ÄùPAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiÔ¨Åcation .‚Äù arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n‚ÄùSemeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.‚Äù arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='ing comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia RedÔ¨Åeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453‚Äì466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558‚Äì3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369‚Äì2 380, Brussels,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='explainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369‚Äì2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114‚Äì1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriÔ¨Åca tion. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriÔ¨Åca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37‚Äì46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriÔ¨Åcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441‚Äì3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='2020, pages 3441‚Äì3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969‚Äì4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: SpeciÔ¨Åcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ¬±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in Ô¨Åeld The content must remain strictly aligned with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='should not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in Ô¨Åeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuÔ¨Écient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suÔ¨Éciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- DiÔ¨Äerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you‚Äôve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat‚Äôs the ideal credit\\ncard for a person without\\nany credit history?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat‚Äôs the ideal credit\\ncard for a person without\\nany credit history?\\nIf you‚Äôve never had a credit card, it‚Äôs probably\\nbecause you don‚Äôt have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat‚Äôs the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you‚Äôve never owned a credit card, it‚Äôs probably\\nbecause you don‚Äôt have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ‚ÄôThe Faerie\\nQueene‚Äô?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ‚ÄôThe Faerie Queene‚Äô had a strong\\ninÔ¨Çuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ‚ÄôThe\\nFaerie Queene‚Äô?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='manuscript form.\\nWho was the English Poet\\nLaureate that penned ‚ÄôThe\\nFaerie Queene‚Äô?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniÔ¨Åcantly inÔ¨Çuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ‚ÄôThe\\nFaerie Queene‚Äô?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Laureate authored ‚ÄôThe\\nFaerie Queene‚Äô?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you‚Äôve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oÔ¨Äer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ‚ÄôThe\\nFaerie Queene‚Äô?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ‚ÄôThe Faerie Queene‚Äô\\nhad a strong inÔ¨Çuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth‚Äôs\\nreign?\\nEdmund Spenser composed\\n‚ÄôThe Faerie Queene‚Äô, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniÔ¨Åcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare‚Äôs popularity.\\nChristopher Marlowe‚Äôs\\n‚ÄôHero and Leander‚Äô was an-\\nother notable Elizabethan\\npoem, but unlike Spenser‚Äôs'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='signiÔ¨Åcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare‚Äôs popularity.\\nChristopher Marlowe‚Äôs\\n‚ÄôHero and Leander‚Äô was an-\\nother notable Elizabethan\\npoem, but unlike Spenser‚Äôs\\nwork it wasn‚Äôt an epic\\nallegory.\\nWhich poet created\\n‚ÄôParadise Lost‚Äô during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ‚ÄôParadise Lost‚Äô\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake‚Äôs ‚ÄôThe Mar-\\nriage of Heaven and Hell‚Äô\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton‚Äôs mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank‚Äôs occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank‚Äôs occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conÔ¨Çicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank‚Äôs occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='wary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank‚Äôs occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer‚Äôs\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract‚ÄîDue to object detection‚Äôs close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiÔ¨Åers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiÔ¨Åcations and useful tricks\\nto improve detection performance further. As distinct speciÔ¨Åc\\ndetection tasks exhibit different characteristics, we also brieÔ¨Çy\\nsurvey several speciÔ¨Åc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='are also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms‚Äîdeep learning, object detection, neural network\\nI. I NTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiÔ¨Åcation [5], [6], human behavior analysis [7][S4],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='valuable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiÔ¨Åcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these Ô¨Åelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]‚Äì[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it‚Äôs difÔ¨Å-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this Ô¨Åeld in recent years [15]‚Äì[18].\\nThe problem deÔ¨Ånition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiÔ¨Åca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiÔ¨Åcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan Ô¨Ånd out all possible positions of the objects, its short-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='or sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan Ô¨Ånd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a Ô¨Åxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit‚Äôs difÔ¨Åcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sity of appearances, illumination conditions and backgrounds,\\nit‚Äôs difÔ¨Åcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiÔ¨Åcation. Besides, a classiÔ¨Åer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiÔ¨Åers, the\\nDPM is a Ô¨Çexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='allows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefÔ¨Åcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-featureBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signiÔ¨Åcant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='capacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classiÔ¨Åcation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a Ô¨Åxed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]‚Äì[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversiÔ¨Åed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ditions. It should be noticed that the covered domains are\\ndiversiÔ¨Åed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiÔ¨Åcation and object detection, but\\npays little attention on detailing speciÔ¨Åc algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speciÔ¨Åc tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A B RIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the overÔ¨Åtting of training, lack of large scale\\ntraining data, limited computation power and insigniÔ¨Åcance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n‚Ä¢The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n‚Ä¢Fast development of high performance parallel computing\\nsystems, such as GPU clusters;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='as ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n‚Ä¢Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n‚Ä¢SigniÔ¨Åcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\noverÔ¨Åtting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite efÔ¨Åcient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton‚Äôs group, whose continuous efforts have demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='What prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton‚Äôs group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and ‚Äòdropout‚Äô regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose ‚Äòpixel‚Äô can be viewed as a speciÔ¨Åc feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive Ô¨Åeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as Ô¨Åltering and pooling. Filtering (convolution)\\noperation convolutes a Ô¨Ålter matrix (learned weights) with\\nthe values of a receptive Ô¨Åeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain Ô¨Ånal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='linear function (such as sigmoid [51], ReLU) to obtain Ô¨Ånal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive Ô¨Åeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be Ô¨Åne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the Ô¨Ånal layer with different activation functions [6]\\nis added to get a speciÔ¨Åc conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='loss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassiÔ¨Åcation layer. The conv feature maps are produced by\\nconvoluting 3*3 Ô¨Ålter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n‚Ä¢Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n‚Ä¢ Compared with traditional shallow models, a deeper'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n‚Ä¢ Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n‚Ä¢ The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classiÔ¨Åcation and bounding box regression\\ninto a multi-task leaning manner).\\n‚Ä¢ BeneÔ¨Åtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research Ô¨Åelds, such as image super-resolution\\nreconstruction [54], [55], image classiÔ¨Åcation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]‚Äì[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='age retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]‚Äì[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the conÔ¨Ådences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at Ô¨Årst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classiÔ¨Åcation problem, adopting a uniÔ¨Åed\\nframework to achieve Ô¨Ånal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiÔ¨Åes R-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiÔ¨Åes R-\\nCNN with a SPP layer). The regression /classiÔ¨Åcation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario Ô¨Årstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='works [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the conÔ¨Ådences of\\nunderlying object categories.\\n1) R-CNN: It is of signiÔ¨Åcance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the Ô¨Çowchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a Ô¨Åxed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the Ô¨Ånal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassiÔ¨Åcation and localization. With pre-trained category-\\nspeciÔ¨Åc linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN BN\\nMulti-scale\\nGridregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression /classiÔ¨Åcation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].\\nRich feature hierarchies for accurate object detection and semantic segmentation\\nRoss Girshick1 Jeff Donahue1,2 Trevor Darrell1,2 Jitendra Malik1\\n1UC Berkeley and 2ICSI\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012‚Äîachieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciÔ¨Åc Ô¨Åne-tuning, yields a signiÔ¨Å-\\ncant performance boost. Since we combine region propos-\\nals with CNNs, we call our method R-CNN: Regions with\\nCNN features. We also present experiments that provide\\ninsight into what the network learns, revealing a rich hier-\\narchy of image features. Source code for the complete sys-\\ntem is available at http://www.cs.berkeley.edu/\\nÀúrbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='use of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the Ô¨Årst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima‚Äôs ‚Äúneocognitron‚Äù [16], a biologically-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiÔ¨Åes each\\nregion using class-speciÔ¨Åc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [32] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='recognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm\\nby showing that stochastic gradient descent, via backprop-\\nagation, can train convolutional neural networks (CNNs), a\\nclass of models that extend the neocognitron.\\nCNNs saw heavy use in the 1990s ( e.g., [24]), but then\\nfell out of fashion, particularly in computer vision, with the\\nrise of support vector machines. In 2012, Krizhevsky et al.\\n[22] rekindled interest in CNNs by showing substantially\\nhigher image classiÔ¨Åcation accuracy on the ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC) [9, 10].\\nTheir success resulted from training a large CNN on 1.2\\nmillion labeled images, together with a few twists on Le-\\nCun‚Äôs CNN (e.g., max(x, 0) rectifying non-linearities and\\n‚Äúdropout‚Äù regularization).\\nThe signiÔ¨Åcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Cun‚Äôs CNN (e.g., max(x, 0) rectifying non-linearities and\\n‚Äúdropout‚Äù regularization).\\nThe signiÔ¨Åcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiÔ¨Åcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question decisively by bridging the\\nchasm between image classiÔ¨Åcation and object detection.\\nThis paper is the Ô¨Årst to show that a CNN can lead to dra-\\n1\\nFig. 3. The Ô¨Çowchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classiÔ¨Åes each region with class-speciÔ¨Åc linear SVMs.\\nbounding box regression and Ô¨Åltered with a greedy non-\\nmaximum suppression (NMS) to produce Ô¨Ånal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufÔ¨Åcient labeled data, pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding box regression and Ô¨Åltered with a greedy non-\\nmaximum suppression (NMS) to produce Ô¨Ånal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufÔ¨Åcient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN Ô¨Årstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speciÔ¨Åc Ô¨Åne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigniÔ¨Åcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n‚Ä¢Due to the existence of FC layers, the CNN requires a\\nÔ¨Åxed-size (e.g., 227√ó227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n‚Ä¢Training of R-CNN is a multi-stage pipeline. At Ô¨Årst,\\na convolutional network (ConvNet) on object proposals is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region, taking a great deal of time in the testing period.\\n‚Ä¢Training of R-CNN is a multi-stage pipeline. At Ô¨Årst,\\na convolutional network (ConvNet) on object proposals is\\nÔ¨Åne-tuned. Then the softmax classiÔ¨Åer learned by Ô¨Åne-\\ntuning is replaced by SVMs to Ô¨Åt in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n‚Ä¢ Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n‚Ä¢Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or reÔ¨Åne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speciÔ¨Åc CNN classiÔ¨Åers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classiÔ¨Åcation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net: FC layers must take a Ô¨Åxed-size input. That‚Äôs\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several Ô¨Åner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 5\\n9\\nmethod VOC 2007 Caltech101\\nVQ [15]‚Ä† 56.07 74.41 ¬±1.0\\nLLC [18]‚Ä† 57.66 76.95 ¬±0.4\\nFK [19]‚Ä† 61.69 77.78 ¬±0.6\\nDeCAF [13] - 86.91 ¬±0.7\\nZeiler & Fergus [4] 75.90‚Ä° 86.5¬±0.5\\nOquab et al. [34] 77.7 -\\nChatÔ¨Åeld et al. [6] 82.42 88.54¬±0.3\\nours 82.44 93.42 ¬±0.5\\nTable 8: ClassiÔ¨Åcation results for Pascal VOC 2007\\n(mAP) and Caltech101 (accuracy). ‚Ä†numbers reported\\nby [27]. ‚Ä°our implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the\\nstate-of-the-art methods on Caltech101. Our result\\n(93.42%) exceeds the previous record (88.54%) by a\\nsubstantial margin (4.88%).\\n4 SPP- NET FOR OBJECT DETECTION\\nDeep networks have been used for object detection.\\nWe brieÔ¨Çy review the recent state-of-the-art R-CNN\\nmethod [7]. R-CNN Ô¨Årst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='method [7]. R-CNN Ô¨Årst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped\\nto a Ô¨Åxed size (227 √ó227). A pre-trained deep network\\nis used to extract the feature of each window. A\\nbinary SVM classiÔ¨Åer is then trained on these features\\nfor detection. R-CNN generates results of compelling\\nquality and substantially outperforms previous meth-\\nods. However, because R-CNN repeatedly applies the\\ndeep convolutional network to about 2,000 windows\\nper image, it is time-consuming. Feature extraction is\\nthe major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image\\nonly once (possibly at multiple scales). Then we ap-\\nply the spatial pyramid pooling on each candidate\\nwindow of the feature maps to pool a Ô¨Åxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='window of the feature maps to pool a Ô¨Åxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied\\nonce, our method can run orders of magnitude faster.\\nOur method extracts window-wise features from\\nregions of the feature maps, while R-CNN extracts\\ndirectly from image regions. In previous works, the\\nDeformable Part Model (DPM) [23] extracts features\\nfrom windows in HOG [24] feature maps, and the\\nSelective Search (SS) method [20] extracts from win-\\ndows in encoded SIFT feature maps. The Overfeat\\ndetection method [5] also extracts from windows of\\ndeep convolutional feature maps, but needs to pre-\\ndeÔ¨Åne the window size. On the contrary, our method\\nenables feature extraction in arbitrary windows from\\nthe deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n‚Ä¶...\\nfully-connected layers (fc6, fc7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n‚Ä¶...\\nfully-connected layers (fc6, fc7)\\nFigure 5: Pooling features from arbitrary windows\\non feature maps. The feature maps are computed\\nfrom the entire image. The pooling is performed in\\ncandidate windows.\\n4.1 Detection Algorithm\\nWe use the ‚Äúfast‚Äù mode of selective search [20] to\\ngenerate about 2,000 candidate windows per image.\\nThen we resize the image such that min(w,h) = s,\\nand extract the feature maps from the entire image.\\nWe use the SPP-net model of ZF-5 (single-size trained)\\nfor the time being. In each candidate window, we use\\na 4-level spatial pyramid (1 √ó1, 2√ó2, 3√ó3, 6√ó6, totally\\n50 bins) to pool the features. This generates a 12,800-\\nd (256 √ó50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='d (256 √ó50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear\\nSVM classiÔ¨Åer for each category on these features.\\nOur implementation of the SVM training follows\\n[20], [7]. We use the ground-truth windows to gen-\\nerate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%\\n(measured by the intersection-over-union (IoU) ratio).\\nAny negative sample is removed if it overlaps another\\nnegative sample by more than 70%. We apply the stan-\\ndard hard negative mining [23] to train the SVM. This\\nstep is iterated once. It takes less than 1 hour to train\\nSVMs for all 20 categories. In testing, the classiÔ¨Åer\\nis used to score the candidate windows. Then we use\\nnon-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) ='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='non-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) =\\ns ‚àà S = {480,576,688,864,1200}, and compute the\\nfeature maps of conv 5 for each scale. One strategy of\\ncombining the features from these scales is to pool\\nthem channel-by-channel. But we empirically Ô¨Ånd\\nthat another strategy provides better results. For each\\ncandidate window, we choose a single scale s ‚àà S\\nsuch that the scaled candidate window has a number\\nof pixels closest to 224 √ó224. Then we only use the\\nfeature maps extracted from this scale to compute\\nFig. 4. The architecture of SPP-net for object detection [64].\\nSPPnet also has notable drawbacks. Like R-CNN, train-\\ning is a multi-stage pipeline that involves extracting fea-\\ntures, Ô¨Åne-tuning a network with log loss, training SVMs,\\nand Ô¨Ånally Ô¨Åtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the Ô¨Åne-tuning al-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tures, Ô¨Åne-tuning a network with log loss, training SVMs,\\nand Ô¨Ånally Ô¨Åtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the Ô¨Åne-tuning al-\\ngorithm proposed in [\\n11] cannot update the convolutional\\nlayers that precede the spatial pyramid pooling. Unsurpris-\\ningly, this limitation (Ô¨Åxed convolutional layers) limits the\\naccuracy of very deep networks.\\n1.2. Contributions\\nWe propose a new training algorithm that Ô¨Åxes the disad-\\nvantages of R-CNN and SPPnet, while improving on their\\nspeed and accuracy. We call this method Fast R-CNN be-\\ncause it‚Äôs comparatively fast to train and test. The Fast R-\\nCNN method has several advantages:\\n1. Higher detection quality (mAP) than R-CNN, SPPnet\\n2. Training is single-stage, using a multi-task loss\\n3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-\\ncense at https://github.com/rbgirshick/\\nfast-rcnn.\\n2. Fast R-CNN architecture and training\\nFig. 1 illustrates the Fast R-CNN architecture. A Fast\\nR-CNN network takes as input an entire image and a set\\nof object proposals. The network Ô¨Årst processes the whole\\nimage with several convolutional ( conv) and max pooling\\nlayers to produce a conv feature map. Then, for each ob-\\nject proposal a region of interest ( RoI) pooling layer ex-\\ntracts a Ô¨Åxed-length feature vector from the feature map.\\nEach feature vector is fed into a sequence of fully connected\\n(fc) layers that Ô¨Ånally branch into two sibling output lay-\\ners: one that produces softmax probability estimates over\\nK object classes plus a catch-all ‚Äúbackground‚Äù class and\\nanother layer that outputs four real-valued numbers for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ers: one that produces softmax probability estimates over\\nK object classes plus a catch-all ‚Äúbackground‚Äù class and\\nanother layer that outputs four real-valued numbers for each\\nof the K object classes. Each set of 4 values encodes reÔ¨Åned\\nbounding-box positions for one of the K classes.\\n2.1. The RoI pooling layer\\nThe RoI pooling layer uses max pooling to convert the\\nfeatures inside any valid region of interest into a small fea-\\nture map with a Ô¨Åxed spatial extent of H √ó W (e.g., 7 √ó 7),\\nwhere H and W are layer hyper-parameters that are inde-\\npendent of any particular RoI. In this paper, an RoI is a\\nrectangular window into a conv feature map. Each RoI is\\ndeÔ¨Åned by a four-tuple (r, c, h, w ) that speciÔ¨Åes its top-left\\ncorner (r, c) and its height and width (h, w ).\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-\\nple regions of interest (RoIs) are input into a fully convolutional\\nnetwork. Each RoI is pooled into a Ô¨Åxed-size feature map and\\nthen mapped to a feature vector by fully connected layers (FCs).\\nThe network has two output vectors per RoI: softmax probabilities\\nand per-class bounding-box regression offsets. The architecture is\\ntrained end-to-end with a multi-task loss.\\nRoI max pooling works by dividing the h √ó w RoI win-\\ndow into an H √ó W grid of sub-windows of approximate\\nsize h/H √ó w/W and then max-pooling the values in each\\nsub-window into the corresponding output grid cell. Pool-\\ning is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets ['),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets [\\n11] in which there is only one pyramid level. We\\nuse the pooling sub-window calculation given in [ 11].\\n2.2. Initializing from pre-trained networks\\nWe experiment with three pre-trained ImageNet [ 4] net-\\nworks, each with Ô¨Åve max pooling layers and between Ô¨Åve\\nand thirteen conv layers (see Section\\n4.1 for network de-\\ntails). When a pre-trained network initializes a Fast R-CNN\\nnetwork, it undergoes three transformations.\\nFirst, the last max pooling layer is replaced by a RoI\\npooling layer that is conÔ¨Ågured by setting H and W to be\\ncompatible with the net‚Äôs Ô¨Årst fully connected layer ( e.g.,\\nH = W = 7 for VGG16).\\nSecond, the network‚Äôs last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiÔ¨Å-\\ncation) are replaced with the two sibling layers described'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='H = W = 7 for VGG16).\\nSecond, the network‚Äôs last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiÔ¨Å-\\ncation) are replaced with the two sibling layers described\\nearlier (a fully connected layer and softmax over K + 1 cat-\\negories and category-speciÔ¨Åc bounding-box regressors).\\nThird, the network is modiÔ¨Åed to take two data inputs: a\\nlist of images and a list of RoIs in those images.\\n2.3. Fine-tuning for detection\\nTraining all network weights with back-propagation is an\\nimportant capability of Fast R-CNN. First, let‚Äôs elucidate\\nwhy SPPnet is unable to update weights below the spatial\\npyramid pooling layer.\\nThe root cause is that back-propagation through the SPP\\nlayer is highly inefÔ¨Åcient when each training sample ( i.e.\\nRoI) comes from a different image, which is exactly how\\nR-CNN and SPPnet networks are trained. The inefÔ¨Åciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to Ô¨Åxed-length feature vectors. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN and SPPnet networks are trained. The inefÔ¨Åciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to Ô¨Åxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the Ô¨Ånal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe Ô¨Ånal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 √ó(12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection efÔ¨Åciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='also improves detection efÔ¨Åciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efÔ¨Åciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network Ô¨Åne-tuning, SVM training and bounding-\\nbox regressor Ô¨Åtting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the Ô¨Åne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classiÔ¨Åcation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a Ô¨Åxed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\nÔ¨Ånally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C+ 1categories (C object classes plus one ‚Äòbackground‚Äô\\nclass) and the other output layer encodes reÔ¨Åned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deÔ¨Åned as below to jointly train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deÔ¨Åned as below to jointly train\\nclassiÔ¨Åcation and bounding-box regression,\\nL(p,u,t u,v) =Lcls(p,u) +Œª[u‚â•1]Lloc(tu,v) (1)\\nwhere Lcls(p,u) =‚àílog pu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p= (p0,¬∑¬∑¬∑ ,pC) over the C+1 outputs from the\\nlast FC layer. Lloc(tu,v) is deÔ¨Åned over the predicted offsets\\ntu = (tu\\nx,tu\\ny,tu\\nw,tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx,vy,vw,vh), where x,y,w,h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u‚â•1] is employed to omit all background RoIs. To provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u‚â•1] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to Ô¨Åt\\nbounding-box regressors as below\\nLloc(tu,v) =\\n‚àë\\ni‚ààx,y,w,h\\nsmoothL1 (tu\\ni ‚àívi) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if|x|<1\\n|x|‚àí0.5 otherwise (3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inefÔ¨Åcient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at Ô¨Årst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='where R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nefÔ¨Åciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving efÔ¨Åciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speciÔ¨Åc conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 6\\ncar : 1.000\\ndog : 0.997\\nperson : 0.992\\nperson : 0.979\\nhorse : 0.993\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores 4k coordinates\\nsliding window\\nreg layercls layer\\nk anchor boxes\\nbus : 0.996\\nperson : 0.736\\nboat : 0.970\\nperson : 0.989\\nperson : 0.983person : 0.983\\nperson : 0.925\\ncat : 0.982\\ndog : 0.994\\nFigure 1: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals\\non PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\\nfeature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d\\nfor VGG). This vector is fed into two sibling fully-connected layers‚Äîa box-regression layer ( reg)\\nand a box-classiÔ¨Åcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nÔ¨Åeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and a box-classiÔ¨Åcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nÔ¨Åeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-\\nnetwork is illustrated at a single position in Fig. 1 (left). Note that because the mini-network operates\\nin a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This\\narchitecture is naturally implemented with an n√ónconv layer followed by two sibling 1 √ó1 conv\\nlayers (for reg and cls, respectively). ReLUs [15] are applied to the output of the n√ónconv layer.\\nTranslation-Invariant Anchors\\nAt each sliding-window location, we simultaneously predict k region proposals, so the reg layer\\nhas 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='has 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to\\nkreference boxes, called anchors. Each anchor is centered at the sliding window in question, and is\\nassociated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yieldingk= 9anchors\\nat each sliding position. For a conv feature map of a sizeW√óH(typically ‚àº2,400), there are WHk\\nanchors in total. An important property of our approach is that it is translation invariant, both in\\nterms of the anchors and the functions that compute proposals relative to the anchors.\\nAs a comparison, the MultiBox method [20] uses k-means to generate 800 anchors, which are not\\ntranslation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='translation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the\\nMultiBox anchors are not translation invariant, it requires a (4+1) √ó800-dimensional output layer,\\nwhereas our method requires a (4+2)√ó9-dimensional output layer. Our proposal layers have an order\\nof magnitude fewer parameters (27 million for MultiBox using GoogLeNet [20] vs. 2.4 million for\\nRPN using VGG-16), and thus have less risk of overÔ¨Åtting on small datasets, like PASCAL VOC.\\nA Loss Function for Learning Region Proposals\\nFor training RPNs, we assign a binary class label (of being an object or not) to each anchor. We\\nassign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-\\nover-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='over-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher\\nthan 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels\\nto multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than\\n0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the\\ntraining objective.\\nWith these deÔ¨Ånitions, we minimize an objective function following the multi-task loss in Fast R-\\nCNN [5]. Our loss function for an image is deÔ¨Åned as:\\nL({pi},{ti}) = 1\\nNcls\\n‚àë\\ni\\nLcls (pi,p‚àó\\ni ) +Œª 1\\nNreg\\n‚àë\\ni\\np‚àó\\ni Lreg(ti,t‚àó\\ni ). (1)\\n2For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic\\nregression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeÔ¨Åned anchor boxes are\\nconvoluted with each sliding window to produce Ô¨Åxed-length vectors which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeÔ¨Åned anchor boxes are\\nconvoluted with each sliding window to produce Ô¨Åxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn√ón spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classiÔ¨Åcation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n√ón conv layer followed by two sibling 1 √ó1 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n√ón conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n‚àë\\ni\\nLcls(pi,p‚àó'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n‚àë\\ni\\nLcls(pi,p‚àó\\ni) +Œª 1\\nNreg\\n‚àë\\ni\\np‚àó\\niLreg(ti,t‚àó\\ni)\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p‚àó\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t‚àó\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size ( Ncls)\\nand the number of anchor locations ( Nreg), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='With the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassiÔ¨Åcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciÔ¨Åc spatial pooling layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='classiÔ¨Åcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciÔ¨Åc spatial pooling layer.\\nRecent state-of-the-art image classiÔ¨Åcation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it‚Äôs\\nFeature Pyramid Networks for Object Detection\\nTsung-Yi Lin1,2, Piotr Doll¬¥ar1, Ross Girshick1,\\nKaiming He1, Bharath Hariharan1, and Serge Belongie2\\n1Facebook AI Research (FAIR)\\n2Cornell University and Cornell Tech\\nAbstract\\nFeature pyramids are a basic component in recognition\\nsystems for detecting objects at different scales. But recent\\ndeep learning object detectors have avoided pyramid rep-\\nresentations, in part because they are compute and memory\\nintensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='intensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-\\ndown architecture with lateral connections is developed for\\nbuilding high-level semantic feature maps at all scales. This\\narchitecture, called a Feature Pyramid Network (FPN),\\nshows signiÔ¨Åcant improvement as a generic feature extrac-\\ntor in several applications. Using FPN in a basic Faster\\nR-CNN system, our method achieves state-of-the-art single-\\nmodel results on the COCO detection benchmark without\\nbells and whistles, surpassing all existing single-model en-\\ntries including those from the COCO 2016 challenge win-\\nners. In addition, our method can run at 6 FPS on a GPU\\nand thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-\\ndamental challenge in computer vision. Feature pyramids\\nbuilt upon image pyramids (for short we call these featur-\\nized image pyramids) form the basis of a standard solution\\n[1] (Fig. 1(a)). These pyramids are scale-invariant in the\\nsense that an object‚Äôs scale change is offset by shifting its\\nlevel in the pyramid. Intuitively, this property enables a\\nmodel to detect objects across a large range of scales by\\nscanning the model over both positions and pyramid levels.\\nFeaturized image pyramids were heavily used in the\\nera of hand-engineered features [5, 25]. They were so\\ncritical that object detectors like DPM [7] required dense\\nscale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFigure 1. (a) Using an image pyramid to build a feature pyramid.\\nFeatures are computed on each of the image scales independently,\\nwhich is slow. (b) Recent detection systems have opted to use\\nonly single scale features for faster detection. (c) An alternative is\\nto reuse the pyramidal feature hierarchy computed by a ConvNet\\nas if it were a featurized image pyramid. (d) Our proposed Feature\\nPyramid Network (FPN) is fast like (b) and (c), but more accurate.\\nIn this Ô¨Ågure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In this Ô¨Ågure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-\\nvolutional networks (ConvNets) [19, 20]. Aside from being\\ncapable of representing higher-level semantics, ConvNets\\nare also more robust to variance in scale and thus facilitate\\nrecognition from features computed on a single input scale\\n[15, 11, 29] (Fig. 1(b)). But even with this robustness, pyra-\\nmids are still needed to get the most accurate results. All re-\\ncent top entries in the ImageNet [33] and COCO [21] detec-\\ntion challenges use multi-scale testing on featurized image\\npyramids (e.g., [16, 35]). The principle advantage of fea-\\nturizing each level of an image pyramid is that it produces\\na multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-\\nmid has obvious limitations. Inference time increases con-\\nsiderably (e.g., by four times [11]), making this approach\\nimpractical for real applications. Moreover, training deep\\n1\\narXiv:1612.03144v2  [cs.CV]  19 Apr 2017\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiÔ¨Åcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiÔ¨Åcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a Ô¨Åxed grid of k√ók Ô¨Årstly and a position-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a Ô¨Åxed grid of k√ók Ô¨Årstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classiÔ¨Åcation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at Ô¨Årst and then enhanced with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='set of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at Ô¨Årst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 √ó1 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a3√ó3\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the Ô¨Ånal feature map is\\ngenerated. This process is iterated until the Ô¨Ånest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacriÔ¨Åcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classiÔ¨Åcation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m√óm mask to maintain the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m√óm mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassiÔ¨Åcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deÔ¨Åned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classiÔ¨Åcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiÔ¨Åcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='It affects classiÔ¨Åcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='other tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a Ô¨Çexible and efÔ¨Åcient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modiÔ¨Åcation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling: Although the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and ‚Äòstuff‚Äô (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classiÔ¨Åcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region proposal generation, pixel-level instance segmentation\\nand regional instance classiÔ¨Åcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive Ô¨Åelds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classiÔ¨Åers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='support regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modiÔ¨Åcations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modiÔ¨Åed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='summing different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and efÔ¨Åcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classiÔ¨Åers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classiÔ¨Åer\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiÔ¨Åcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Traditional CNN framework for object detection is not\\nskilled in handling signiÔ¨Åcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classiÔ¨Åcation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\nÔ¨Årstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‚Äòdeep and thin‚Äô design'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‚Äòdeep and thin‚Äô design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PV ANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression /ClassiÔ¨Åcation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiÔ¨Åcation and bounding box\\nregression, which are usually trained separately. Even in recent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='eral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiÔ¨Åcation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step frameworks based on global regres-\\nsion/classiÔ¨Åcation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We Ô¨Årstly reviews some pioneer CNN\\nmodels, and then focus on two signiÔ¨Åcant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiÔ¨Åcation task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiÔ¨Åcation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difÔ¨Åculty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is efÔ¨Åcient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniÔ¨Åed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniÔ¨Åed\\nloss was introduced to bias both localization and conÔ¨Ådences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the Ô¨Ånal layer.\\nYoo et al. adopted an iterative classiÔ¨Åcation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inefÔ¨Åcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the model becomes quite inefÔ¨Åcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 9\\nFig. 9. Main idea of YOLO [17].\\nÔ¨Ånding a path from a Ô¨Åxed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a Ô¨Åxed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndifÔ¨Åculty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both conÔ¨Ådences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S√óS grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conÔ¨Ådence scores. Formally, conÔ¨Å-\\ndence scores are deÔ¨Åned as Pr(Object) ‚àóIOUtruth\\npred , which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conÔ¨Ådence scores. Formally, conÔ¨Å-\\ndence scores are deÔ¨Åned as Pr(Object) ‚àóIOUtruth\\npred , which\\nindicates how likely there exist objects ( Pr(Object) ‚â•0) and\\nshows conÔ¨Ådences of its prediction ( IOUtruth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speciÔ¨Åc conÔ¨Ådence scores for each box\\nare achieved by multiplying the individual box conÔ¨Ådence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) ‚àóIOUtruth\\npred ‚àóPr(Classi|Object)\\n= Pr(Classi) ‚àóIOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciÔ¨Åc objects in the\\nbox and the Ô¨Åtness between the predicted box and the object\\nare both taken into consideration.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='= Pr(Classi) ‚àóIOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciÔ¨Åc objects in the\\nbox and the Ô¨Åtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\nŒªcoord\\nS2\\n‚àë\\ni=0\\nB‚àë\\nj=0\\n1 obj\\nij\\n[\\n(xi ‚àíÀÜxi)2 + (yi ‚àíÀÜyi)2]\\n+Œªcoord\\nS2\\n‚àë\\ni=0\\nB‚àë\\nj=0\\n1 obj\\nij\\n[(‚àöwi ‚àí\\n‚àö\\nÀÜwi)2 + (\\n‚àö\\nhi ‚àí\\n‚àö\\nÀÜhi\\n)2]\\n+\\nS2\\n‚àë\\ni=0\\nB‚àë\\nj=0\\n1 obj\\nij\\n(\\nCi ‚àíÀÜCi\\n)2\\n+Œªnoobj\\nS2\\n‚àë\\ni=0\\nB‚àë\\nj=0\\n1 noobj\\nij\\n(\\nCi ‚àíÀÜCi\\n)2\\n+\\nS2\\n‚àë\\ni=0\\n1 obj\\ni\\n‚àë\\nc‚ààclasses\\n(pi(c) ‚àíÀÜpi(c))2\\n(6)\\nIn a certain cell i, (xi,yi) denote the center of the box relative\\nto the bounds of the grid cell,(wi,hi) are the normalized width\\nand height relative to the image size, Ci represents conÔ¨Ådence\\nscores, 1 obj\\ni indicates the existence of objects and 1 obj\\nij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiÔ¨Åcation errors. Similarly,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiÔ¨Åcation errors. Similarly,\\nwhen the predictor is ‚Äòresponsible‚Äô for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 √ó1 reduction layers followed by 3 √ó3 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpliÔ¨Åed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='makes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a difÔ¨Åculty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\nconÔ¨Ågurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speciÔ¨Åc feature map, instead of\\nÔ¨Åxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Ô¨Åxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated conÔ¨Ådences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and conÔ¨Ådence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale reÔ¨Åned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiÔ¨Åcantly outperforms the Faster R-CNN in terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Integrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiÔ¨Åcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300√ó300) runs\\nat 59 FPS, which is more accurate and efÔ¨Åcient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated conÔ¨Ådences. Final detection results are obtained by conducting NMS on multi-scale reÔ¨Åned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN &S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speciÔ¨Åc instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classiÔ¨Åcation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n‚Ä¢If incorporated with a proper way, more powerful back-\\nbone CNN models can deÔ¨Ånitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n‚Ä¢ With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n‚Ä¢Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith ‚Äò07‚Äô ,‚Äò07+12‚Äô and ‚Äò07+12+coco‚Äô).\\n‚Ä¢Apart from basic models, there are still many other factors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='important for deep learning based models (Faster R-CNN\\nwith ‚Äò07‚Äô ,‚Äò07+12‚Äô and ‚Äò07+12+coco‚Äô).\\n‚Ä¢Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\nÔ¨Åed classiÔ¨Åcation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n‚Ä¢As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and Ô¨Åne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n‚Ä¢By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n‚Ä¢ Multi-scale training and test are beneÔ¨Åcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Multi-scale training and test are beneÔ¨Åcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n‚Ä¢ Overall, region proposal based methods, such as\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/classÔ¨Åcation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/classÔ¨Åcation based approaches.\\n‚Ä¢ Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n‚Ä¢Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='objects and surroundings (GBD-Net and multi-path).\\n‚Ä¢Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n‚Ä¢The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classiÔ¨Åers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES .\\nFramework Proposal Multi-scale Input Learning Method Loss Function Softmax Layer End-to-end Train Platform Language\\nR-CNN [15] Selective Search - SGD,BP Hinge loss (classiÔ¨Åcation),Bounding box regression + - Caffe Matlab\\nSPP-net [64] EdgeBoxes + SGD Hinge loss (classiÔ¨Åcation),Bounding box regression + - Caffe Matlab\\nFast RCNN [16] Selective Search + SGD Class Log loss+bounding box regression + - Caffe Python\\nFaster R-CNN [18] RPN + SGD Class Log loss+bounding box regression + + Caffe Python/Matlab\\nR-FCN [65] RPN + SGD Class Log loss+bounding box regression - + Caffe Matlab\\nMask R-CNN [67] RPN + SGD Class Log loss+bounding box regression + + TensorFlow/Keras Python+Semantic sigmoid loss\\nFPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='FPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python\\nYOLO [17] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conÔ¨Ådence+background conÔ¨Ådence\\nSSD [71] - - SGD Class softmax loss+bounding box regression - + Caffe C++\\nYOLOv2 [72] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conÔ¨Ådence+background conÔ¨Ådence\\n* ‚Äò+‚Äô denotes that corresponding techniques are employed while ‚Äò-‚Äô denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='by the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN (Alex) [15] 07 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 68.6 58.5\\nR-CNN(VGG16) [15] 07 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nSPP-net(ZF) [64] 07 68.5 71.7 58.7 41.9 42.5 67.7 72.1 73.8 34.7 67.0 63.4 66.0 72.5 71.3 58.9 32.8 60.9 56.1 67.9 68.8 60.9\\nGCNN [70] 07 68.3 77.3 68.5 52.4 38.6 78.5 79.5 81.0 47.1 73.6 64.5 77.2 80.5 75.8 66.6 34.3 65.2 64.4 75.6 66.4 66.8\\nBayes [85] 07 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 70.1 41.4 69.6 60.8 70.2 73.7 68.5\\nFast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0\\nSDP+CRC [33] 07 76.1 79.4 68.2 52.6 46.0 78.4 78.4 81.0 46.7 73.5 65.3 78.6 81.0 76.7 77.3 39.0 65.1 67.2 77.5 70.3 68.9\\nSubCNN [60] 07 70.2 80.5 69.5 60.3 47.9 79.0 78.7 84.2 48.5 73.9 63.0 82.7 80.6 76.0 70.2 38.2 62.4 67.7 77.7 60.5 68.5\\nStuffNet30 [100] 07 72.6 81.7 70.6 60.5 53.0 81.5 83.7 83.9 52.2 78.9 70.7 85.0 85.7 77.0 78.7 42.2 73.6 69.2 79.2 73.8 72.7\\nNOC [114] 07+12 76.3 81.4 74.4 61.7 60.8 84.7 78.2 82.9 53.0 79.2 69.2 83.2 83.2 78.5 68.0 45.0 71.6 76.7 82.2 75.7 73.3\\nMR-CNN&S-CNN [110] 07+12 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 78.2\\nHyperNet [101] 07+12 77.4 83.3 75.0 69.1 62.4 83.1 87.4 87.4 57.1 79.8 71.4 85.1 85.1 80.0 79.1 51.2 79.1 75.7 80.9 76.5 76.3\\nMS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6\\nOHEM+Fast R-CNN [113] 07+12 80.6 85.7 79.8 69.9 60.8 88.3 87.9 89.6 59.7 85.1 76.5 87.1 87.3 82.4 78.8 53.7 80.5 78.7 84.5 80.7 78.9\\nION [95] 07+12+S 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 79.2\\nFaster R-CNN [18] 07 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6 69.9\\nFaster R-CNN [18] 07+12 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 73.2\\nFaster R-CNN [18] 07+12+COCO 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9 78.8\\nSSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6\\nSSD512 [71] 07+12+COCO 86.6 88.3 82.4 76.0 66.3 88.6 88.9 89.1 65.1 88.4 73.6 86.5 88.9 85.3 84.6 59.1 85.0 80.4 87.4 81.2 81.6\\n* ‚Äò07‚Äô: VOC2007 trainval, ‚Äò07+12‚Äô: union of VOC2007 and VOC2012 trainval, ‚Äò07+12+COCO‚Äô: trained on COCO trainval35k at Ô¨Årst and then Ô¨Åne-tuned on 07+12. The S in ION ‚Äò07+12+S‚Äô denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN(Alex) [15] 12 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1 53.3\\nR-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4\\nBayes [85] 12 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2 66.4\\nFast R-CNN [16] 07++12 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 68.4\\nSutffNet30 [100] 12 83.0 76.9 71.2 51.6 50.1 76.4 75.7 87.8 48.3 74.8 55.7 85.7 81.2 80.3 79.5 44.2 71.8 61.0 78.5 65.4 70.0\\nNOC [114] 07+12 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1 68.8\\nMR-CNN&S-CNN [110] 07++12 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 73.9\\nHyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='HyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4\\nOHEM+Fast R-CNN [113] 07++12+coco 90.1 87.4 79.9 65.8 66.3 86.1 85.0 92.9 62.4 83.4 69.5 90.6 88.9 88.9 83.6 59.0 82.0 74.7 88.2 77.3 80.1\\nION [95] 07+12+S 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 76.4\\nFaster R-CNN [18] 07++12 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 70.4\\nFaster R-CNN [18] 07++12+coco 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2 75.9\\nYOLO [17] 07++12 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 57.9\\nYOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7\\nYOLOv2 [72] 07++12+coco 88.8 87.0 77.8 64.9 51.8 85.2 79.3 93.1 64.4 81.4 70.2 91.3 88.1 87.2 81.0 57.7 78.1 71.0 88.5 76.8 78.2\\nSSD300 [71] 07++12+coco 91.0 86.0 78.1 65.0 55.4 84.9 84.0 93.4 62.1 83.6 67.3 91.3 88.9 88.6 85.6 54.7 83.8 77.3 88.3 76.5 79.3\\nSSD512 [71] 07++12+coco 91.4 88.6 82.6 71.4 63.1 87.4 88.1 93.9 66.9 86.6 66.3 92.0 91.7 90.8 88.5 60.9 87.0 75.4 90.2 80.4 82.2\\nR-FCN (ResNet101) [16] 07++12+coco 92.3 89.9 86.7 74.7 75.2 86.7 89.0 95.8 70.2 90.4 66.5 95.0 93.2 92.1 91.1 71.0 89.7 76.0 92.0 83.4 85.0\\n* ‚Äò07++12‚Äô: union of VOC2007 trainval and test and VOC2012 trainval. ‚Äò07++12+COCO‚Äô: trained on COCO trainval35k at Ô¨Årst then Ô¨Åne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='TABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L\\nFast R-CNN [16] train 20.5 39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95] train 23.6 43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114] train 21.2 41.5 19.7 - - - - - - - - -\\nNOC+FRCN(Google) [114] train 24.8 44.4 25.2 - - - - - - - - -\\nNOC+FRCN (ResNet101) [114] train 27.2 48.4 27.6 - - - - - - - - -\\nGBD-Net [109] train 27.0 45.8 - - - - - - - - - -\\nOHEM+FRCN [113] train 22.6 42.5 22.2 5.0 23.7 34.6 - - - - - -\\nOHEM+FRCN* [113] train 24.4 44.4 24.8 7.1 26.4 37.9 - - - - - -\\nOHEM+FRCN* [113] trainval 25.5 45.9 26.1 7.4 27.7 38.5 - - - - - -\\nFaster R-CNN [18] trainval 24.2 45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71] trainval35k 26.8 46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65] trainval 29.2 51.5 - 10.8 32.8 45.0 - - - - - -\\nR-FCN*(ResNet101) [65] trainval 29.9 51.9 - 10.4 32.4 43.3 - - - - - -\\nR-FCN**(ResNet101) [65] trainval 31.5 53.2 - 14.3 35.5 44.2 - - - - - -\\nMulti-path [112] trainval 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66] trainval35k 36.2 59.1 39.0 18.2 39.0 48.2 - - - - - -\\nMask (ResNet101+FPN) [67] trainval35k 38.2 60.3 41.7 20.1 41.1 50.2 - - - - - -\\nMask (ResNeXt101+FPN) [67] trainval35k 39.8 62.3 43.4 22.1 43.2 51.2 - - - - - -\\nDSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='DSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for ‚ÄòSS‚Äô which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n‚Ä¢ By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uniÔ¨Åed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It‚Äôs also helpful to compress the parameters\\nof FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03\\nSS+SPP-net [64] 07 63.1 2.3 0.44\\nSS+FRCN [16] 07+12 66.9 1.72 0.6\\nSDP+CRC [33] 07 68.9 0.47 2.1\\nSS+HyperNet* [101] 07+12 76.3 0.20 5\\nMR-CNN&S-CNN [110] 07+12 78.2 30 0.03\\nION [95] 07+12+S 79.2 1.92 0.5\\nFaster R-CNN(VGG16) [18] 07+12 73.2 0.11 9.1\\nFaster R-CNN(ResNet101) [18] 07+12 83.8 2.24 0.4\\nYOLO [17] 07+12 63.4 0.02 45\\nSSD300 [71] 07+12 74.3 0.02 46\\nSSD512 [71] 07+12 76.8 0.05 19\\nR-FCN(ResNet101) [65] 07+12+coco 83.6 0.17 5.9\\nYOLOv2(544*544) [72] 07+12 78.6 0.03 40\\nDSSD321(ResNet101) [73] 07+12 78.6 0.07 13.6\\nDSOD300 [74] 07+12+coco 81.7 0.06 17.4\\nPV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‚Äòfast mode‚Äô Selective Search [16], HyperNet*: the speed up version of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='PV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‚Äòfast mode‚Äô Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PA VNET+ (compresss): PA VNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n‚Ä¢It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN &S-\\nRCNN).\\n‚Ä¢It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n‚Ä¢Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modiÔ¨Åed into real-time systems with the introduction\\nof other tricks [116] (PV ANET), such as BN [43], residual\\nconnections [123].\\nIV. S ALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signiÔ¨Åcance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by Ô¨Åne-tuning DNNs‚Äô\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reÔ¨Åned the results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reÔ¨Åned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uniÔ¨Åed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is beneÔ¨Åcial. To learn internal represen-\\ntations of saliency efÔ¨Åciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tations of saliency efÔ¨Åciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive Ô¨Åelds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiÔ¨Åcance for improving detection performance. Based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='convolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiÔ¨Åcance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye Ô¨Åxations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efÔ¨Åcient and accurate to train a direct pixel-wise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efÔ¨Åcient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='this problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Two standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deÔ¨Åned as below\\nFŒ≤ = (1 +Œ≤2)Presion √óRecall\\nŒ≤2Presion + Recall (7)\\nwhere Œ≤2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE = 1\\nH√óW\\nH‚àë\\ni=1\\nW‚àë\\nj=1\\n‚èê‚èê‚èêÀÜS(i,j) = ÀÜZ(i,j)\\n‚èê‚èê‚èê (8)\\nwhere ÀÜZ and ÀÜS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can Ô¨Ånd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insufÔ¨Åcient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='segmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signiÔ¨Åcance for measuring local\\nconspicuity. Finally, it‚Äôs necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='complementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]‚Äì[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural conÔ¨Ågurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiÔ¨Åers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiÔ¨Åers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nefÔ¨Åciency. However, this detector may degrade signiÔ¨Åcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]‚Äì[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='been proposed [167]‚Äì[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='into smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nefÔ¨Åcient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS .\\nDataset Metrics CHM [150] RC [151] DRFI [152] MC [138] MDF [146] LEGS [136] DSR [149] MTDNN [141] CRPSD [142] DCL [143] ELD [153] NLDF [154] DSSC [155]\\nPASCAL-S wFŒ≤ 0.631 0.640 0.679 0.721 0.764 0.756 0.697 0.818 0.776 0.822 0.767 0.831 0.830\\nMAE 0.222 0.225 0.221 0.147 0.145 0.157 0.128 0.170 0.063 0.108 0.121 0.099 0.080\\nECSSD wFŒ≤ 0.722 0.741 0.787 0.822 0.833 0.827 0.872 0.810 0.849 0.898 0.865 0.905 0.915\\nMAE 0.195 0.187 0.166 0.107 0.108 0.118 0.037 0.160 0.046 0.071 0.098 0.063 0.052\\nHKU-IS wFŒ≤ 0.728 0.726 0.783 0.781 0.860 0.770 0.833 - 0.821 0.907 0.844 0.902 0.913\\nMAE 0.158 0.165 0.143 0.098 0.129 0.118 0.040 - 0.043 0.048 0.071 0.048 0.039\\nSOD wFŒ≤ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SOD wFŒ≤ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118\\n* The bigger wFŒ≤ is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modiÔ¨Åcations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uniÔ¨Åed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a Ô¨Åxed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating predeÔ¨Åned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='addressed to transfer from generic object detection to face\\ndetection, namely eliminating predeÔ¨Åned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na conÔ¨Åguration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-Ô¨Åne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and reÔ¨Åne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='To reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to reÔ¨Åne the positions of possible\\nfaces. Qin et al. proposed a uniÔ¨Åed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reÔ¨Çect the dependence of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='types of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reÔ¨Çect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the reÔ¨Çection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='NPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signiÔ¨Åcant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is efÔ¨Åcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cascaded CNNs to locate face regions, which is efÔ¨Åcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nefÔ¨Åciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 15\\nFaster R-CNN architecture to face detection, which validate\\nthe signiÔ¨Åcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. P EDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identiÔ¨Åcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='which has a close relationship to pedestrian tracking [189],\\n[190], person re-identiÔ¨Åcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]‚Äì[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin ‚Äòplain‚Äô features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='main source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conÔ¨Ågurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modiÔ¨Åed the down-\\nstream classiÔ¨Åer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[18] to pedestrian detection [203]. They modiÔ¨Åed the down-\\nstream classiÔ¨Åer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand Ô¨Åne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='al. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE -ART MODELS ON CALTECH PEDESTRIAN DATASET . ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod Reasonable All Far Medium Near none partial heavy\\nCheckerboards+ [198] 17.1 68.4 100 58.3 5.1 15.6 31.4 78.4\\nLDCF++[S2] 15.2 67.1 100 58.4 5.4 13.3 33.3 76.2\\nSCF+AlexNet [210] 23.3 70.3 100 62.3 10.2 20.0 48.5 74.7\\nSA-FastRCNN [211] 9.7 62.6 100 51.8 0 7.7 24.8 64.3\\nMS-CNN [105] 10.0 61.0 97.2 49.1 2.6 8.2 19.2 60.0\\nDeepParts [204] 11.9 64.8 100 56.4 4.8 10.6 19.9 60.4\\nCompACT-Deep [195] 11.8 64.4 100 53.2 4.0 9.6 25.1 65.8\\nRPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassiÔ¨Åers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of conÔ¨Ådences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 Ô¨Åne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiÔ¨Åcation and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='45 Ô¨Åne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiÔ¨Åcation and\\nsimpliÔ¨Åcation is of signiÔ¨Åcance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classiÔ¨Åers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ACF detectors and SVM classiÔ¨Åers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely ‚ÄòPerson (clear identiÔ¨Åcations)‚Äô,\\n‚ÄòPerson? (unclear identiÔ¨Åcations)‚Äô and ‚ÄòPeople (large group of\\nindividuals)‚Äô, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10‚àí2 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The Ô¨Årst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classiÔ¨Åers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='candidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiÔ¨Åers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P ROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe Ô¨Årst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n‚Ä¢ Multi-task joint optimization and multi-modal infor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n‚Ä¢ Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speciÔ¨Åc\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='application, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n‚Ä¢Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all beneÔ¨Åcial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='jects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n‚Ä¢Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='selection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n‚Ä¢Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classiÔ¨Åers at latter stages can\\nhandle more difÔ¨Åcult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are Ô¨Åxed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='when training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n‚Ä¢ Unsupervised and weakly supervised learning. It‚Äôs\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand reÔ¨Åne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n‚Ä¢ Network optimization. Given speciÔ¨Åc applications and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='for achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n‚Ä¢ Network optimization. Given speciÔ¨Åc applications and\\nplatforms, it is signiÔ¨Åcant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n‚Ä¢3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='transportation and intelligent surveillance.\\n‚Ä¢3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n‚Ä¢ Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical Ô¨Çow [199] and LSTM [107] should'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='blur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical Ô¨Çow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. C ONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modiÔ¨Åcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieÔ¨Çy reviewed. Finally, we propose several promising future'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieÔ¨Çy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n‚ÄúObject detection with discriminatively trained part-based models,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚ÄúObject detection with discriminatively trained part-based models,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, ‚ÄúExample-based learning for view-based\\nhuman face detection,‚ÄùIEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39‚Äì51, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, ‚ÄúPedestrian detection:\\nAn evaluation of the state of the art,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y . Yoshinaga, ‚ÄúDetection of spicules on mammogram\\nbased on skeleton analysis.‚Äù IEEE Trans. Med. Imag. , vol. 15, no. 3,\\npp. 235‚Äì245, 1996.\\n[5] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, ‚ÄúCaffe: Convolutional architecture for\\nfast feature embedding,‚Äù in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation\\nwith deep convolutional neural networks,‚Äù in NIPS, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='fast feature embedding,‚Äù in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation\\nwith deep convolutional neural networks,‚Äù in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, ‚ÄúRealtime multi-person\\n2d pose estimation using part afÔ¨Ånity Ô¨Åelds,‚Äù in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, ‚ÄúA multi-scale cascade fully convolutional\\nnetwork face detector,‚Äù in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, ‚ÄúDeepdriving:\\nLearning affordance for direct perception in autonomous driving,‚Äù in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, ‚ÄúMulti-view 3d object\\ndetection network for autonomous driving,‚Äù in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, ‚ÄúEmbedded streaming\\ndeep neural networks accelerator with applications,‚Äù IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572‚Äì1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, ‚ÄúLow-complexity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Neural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572‚Äì1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, ‚ÄúLow-complexity\\napproximate convolutional neural networks,‚ÄùIEEE Trans. Neural Netw.\\n& Learning Syst. , vol. PP, no. 99, pp. 1‚Äì12, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n‚ÄúCost-sensitive learning of deep feature representations from imbal-\\nanced data.‚Äù IEEE Trans. Neural Netw. & Learning Syst. , vol. PP,\\nno. 99, pp. 1‚Äì15, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, ‚ÄúFeature extraction with deep\\nneural networks by a generalized discriminant analysis.‚Äù IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 23, no. 4, pp. 596‚Äì608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, ‚ÄúRich feature\\nhierarchies for accurate object detection and semantic segmentation,‚Äù\\nin CVPR, 2014.\\n[16] R. Girshick, ‚ÄúFast r-cnn,‚Äù in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‚ÄúYou only look'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in CVPR, 2014.\\n[16] R. Girshick, ‚ÄúFast r-cnn,‚Äù in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, ‚ÄúYou only look\\nonce: UniÔ¨Åed, real-time object detection,‚Äù in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-\\ntime object detection with region proposal networks,‚Äù in NIPS, 2015,\\npp. 91‚Äì99.\\n[19] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant key-\\npoints,‚Äù Int. J. of Comput. Vision , vol. 60, no. 2, pp. 91‚Äì110, 2004.\\n[20] N. Dalal and B. Triggs, ‚ÄúHistograms of oriented gradients for human\\ndetection,‚Äù in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, ‚ÄúAn extended set of haar-like features for\\nrapid object detection,‚Äù in ICIP, 2002.\\n[22] C. Cortes and V . Vapnik, ‚ÄúSupport vector machine,‚ÄùMachine Learning,\\nvol. 20, no. 3, pp. 273‚Äì297, 1995.\\n[23] Y . Freund and R. E. Schapire, ‚ÄúA desicion-theoretic generalization of\\non-line learning and an application to boosting,‚Äù J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663‚Äì671, 1997.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[23] Y . Freund and R. E. Schapire, ‚ÄúA desicion-theoretic generalization of\\non-line learning and an application to boosting,‚Äù J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663‚Äì671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n‚ÄúObject detection with discriminatively trained part-based models,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627‚Äì1645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, ‚ÄúThe pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),‚Äù 2008.\\n[26] Y . LeCun, Y . Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù Nature, vol.\\n521, no. 7553, pp. 436‚Äì444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, ‚ÄúPredicting eye Ô¨Åxations\\nusing convolutional neural networks,‚Äù in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, ‚ÄúLarge-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,‚Äù in CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='using convolutional neural networks,‚Äù in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, ‚ÄúLarge-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,‚Äù in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, ‚ÄúFace detection with the faster r-cnn,‚Äù\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun, ‚ÄúJoint cascade face\\ndetection and alignment,‚Äù in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, ‚ÄúSupervised transformer network\\nfor efÔ¨Åcient face detection,‚Äù in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, ‚ÄúA real-time\\npedestrian detector using deep learning for human-aware navigation,‚Äù\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y . Lin, ‚ÄúExploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassiÔ¨Åers,‚Äù in CVPR, 2016.\\n[34] P. Druzhkov and V . Kustikova, ‚ÄúA survey of deep learning methods and\\nsoftware tools for image classiÔ¨Åcation and object detection,‚Äù Pattern\\nRecognition and Image Anal. , vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, ‚ÄúHow we know universals the perception\\nof auditory and visual forms,‚ÄùThe Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127‚Äì147, 1947.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[35] W. Pitts and W. S. McCulloch, ‚ÄúHow we know universals the perception\\nof auditory and visual forms,‚ÄùThe Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127‚Äì147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‚ÄúLearning internal\\nrepresentation by back-propagation of errors,‚Äù Nature, vol. 323, no.\\n323, pp. 533‚Äì536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, ‚ÄúReducing the dimensionality\\nof data with neural networks,‚Äù Sci., vol. 313, pp. 504‚Äì507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainathet al., ‚ÄúDeep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,‚Äù IEEE Signal Process. Mag. , vol. 29,\\nno. 6, pp. 82‚Äì97, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:\\nA large-scale hierarchical image database,‚Äù in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='A large-scale hierarchical image database,‚Äù in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, ‚ÄúBinary coding of speech spectrograms using a deep auto-\\nencoder,‚Äù in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., ‚ÄúPhone recognition with\\nthe mean-covariance restricted boltzmann machine,‚Äù in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, ‚ÄúImproving neural networks by preventing co-\\nadaptation of feature detectors,‚Äù arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,‚Äù in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun,\\n‚ÄúOverfeat: Integrated recognition, localization and detection using\\nconvolutional networks,‚Äù arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚ÄúOverfeat: Integrated recognition, localization and detection using\\nconvolutional networks,‚Äù arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with\\nconvolutions,‚Äù in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks\\nfor large-scale image recognition,‚Äù arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\\nrecognition,‚Äù in CVPR, 2016.\\n[48] V . Nair and G. E. Hinton, ‚ÄúRectiÔ¨Åed linear units improve restricted\\nboltzmann machines,‚Äù in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al. , ‚ÄúWeakly supervised\\nobject recognition with convolutional neural networks,‚Äù in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, ‚ÄúLearning and transferring\\nmid-level image representations using convolutional neural networks,‚Äù\\nin CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, ‚ÄúLearning and transferring\\nmid-level image representations using convolutional neural networks,‚Äù\\nin CVPR, 2014.\\n[51] F. M. Wadley, ‚ÄúProbit analysis: a statistical treatment of the sigmoid\\nresponse curve,‚Äù Annals of the Entomological Soc. of America , vol. 67,\\nno. 4, pp. 549‚Äì553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y . LeCun et al. , ‚ÄúLearning invariant\\nfeatures through topographic Ô¨Ålter maps,‚Äù in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y .-L. Boureau, K. Gregor, M. Mathieu,\\nand Y . LeCun, ‚ÄúLearning convolutional feature hierarchies for visual\\nrecognition,‚Äù in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, ‚ÄúDeconvolu-\\ntional networks,‚Äù in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, ‚ÄúLearning deconvolution network for\\nsemantic segmentation,‚Äù in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, ‚ÄúPlant leaf iden-\\ntiÔ¨Åcation via a growing convolution neural network with progressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantic segmentation,‚Äù in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, ‚ÄúPlant leaf iden-\\ntiÔ¨Åcation via a growing convolution neural network with progressive\\nsample learning,‚Äù in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V . Lempitsky, ‚ÄúNeural codes\\nfor image retrieval,‚Äù in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y . Zhang, and J. Li,\\n‚ÄúDeep learning for content-based image retrieval: A comprehensive\\nstudy,‚Äù in ACM MM, 2014.\\n[59] D. Tom `e, F. Monti, L. BarofÔ¨Åo, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, ‚ÄúDeep convolutional neural networks for pedestrian detec-\\ntion,‚Äù Signal Process.: Image Commun. , vol. 47, pp. 482‚Äì489, 2016.\\n[60] Y . Xiang, W. Choi, Y . Lin, and S. Savarese, ‚ÄúSubcategory-aware\\nconvolutional neural networks for object proposals and detection,‚Äù in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, ‚ÄúPedestrian\\ndetection based on fast r-cnn and batch normalization,‚Äù in ICIC, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='WACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, ‚ÄúPedestrian\\ndetection based on fast r-cnn and batch normalization,‚Äù in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng,\\n‚ÄúMultimodal deep learning,‚Äù in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y .-G. Jiang, H. Ye, and X. Xue, ‚ÄúModeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classiÔ¨Å-\\ncation,‚Äù in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúSpatial pyramid pooling in deep\\nconvolutional networks for visual recognition,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 9, pp. 1904‚Äì1916, 2015.\\n[65] Y . Li, K. He, J. Sun et al., ‚ÄúR-fcn: Object detection via region-based\\nfully convolutional networks,‚Äù in NIPS, 2016, pp. 379‚Äì387.\\n[66] T.-Y . Lin, P. Doll ¬¥ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, ‚ÄúFeature pyramid networks for object detection,‚Äù in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ¬¥ar, and R. B. Girshick, ‚ÄúMask r-cnn,‚Äù in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Belongie, ‚ÄúFeature pyramid networks for object detection,‚Äù in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ¬¥ar, and R. B. Girshick, ‚ÄúMask r-cnn,‚Äù in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, ‚ÄúScalable object\\ndetection using deep neural networks,‚Äù in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon, ‚ÄúAttentionnet:\\nAggregating weak directions for accurate object detection,‚Äù in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, ‚ÄúG-cnn: an iterative grid\\nbased object detector,‚Äù in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\\nA. C. Berg, ‚ÄúSsd: Single shot multibox detector,‚Äù in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, ‚ÄúYolo9000: better, faster, stronger,‚Äù\\narXiv:1612.08242, 2016.\\n[73] C. Y . Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, ‚ÄúDssd:\\nDeconvolutional single shot detector,‚Äù arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, ‚ÄúDsod:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deconvolutional single shot detector,‚Äù arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, ‚ÄúDsod:\\nLearning deeply supervised object detectors from scratch,‚Äù in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, ‚ÄúTransforming auto-\\nencoders,‚Äù in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, ‚ÄúLearning invariance\\nthrough imitation,‚Äù in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, ‚ÄúHistograms of sparse codes for object\\ndetection,‚Äù in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n‚ÄúSelective search for object recognition,‚Äù Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154‚Äì171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun, ‚ÄúPedestrian\\ndetection with unsupervised multi-stage feature learning,‚Äù in CVPR,\\n2013.\\n[80] P. Kr ¬®ahenb¬®uhl and V . Koltun, ‚ÄúGeodesic object proposals,‚Äù in ECCV,\\n2014.\\n[81] P. Arbel ¬¥aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2013.\\n[80] P. Kr ¬®ahenb¬®uhl and V . Koltun, ‚ÄúGeodesic object proposals,‚Äù in ECCV,\\n2014.\\n[81] P. Arbel ¬¥aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n‚ÄúMultiscale combinatorial grouping,‚Äù in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll ¬¥ar, ‚ÄúEdge boxes: Locating object proposals\\nfrom edges,‚Äù in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, ‚ÄúDeepbox: Learning objectness\\nwith convolutional networks,‚Äù in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll ¬¥ar, ‚ÄúLearning to\\nreÔ¨Åne object segments,‚Äù in ECCV, 2016.\\n[85] Y . Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, ‚ÄúImproving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,‚Äù in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel ¬¥aez, and J. Malik, ‚ÄúLearning rich features\\nfrom rgb-d images for object detection and segmentation,‚Äù in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='from rgb-d images for object detection and segmentation,‚Äù in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., ‚ÄúDeepid-net: Deformable deep convolutional\\nneural networks for object detection,‚Äù in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, ‚ÄúR-cnn minus r,‚Äù arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, ‚ÄúBeyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,‚Äù\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S ¬¥anchez, and T. Mensink, ‚ÄúImproving the Ô¨Åsher kernel\\nfor large-scale image classiÔ¨Åcation,‚Äù in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y . Gong, ‚ÄúRestructuring of deep neural network\\nacoustic models with singular value decomposition.‚Äù in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-time\\nobject detection with region proposal networks,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 39, no. 6, pp. 1137‚Äì1149, 2017.\\n[93] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚ÄúRethink-\\ning the inception architecture for computer vision,‚Äù in CVPR, 2016.\\n[94] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll ¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in\\ncontext,‚Äù in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, ‚ÄúInside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,‚Äù in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, ‚ÄúPixelwise instance segmentation with a\\ndynamically instantiated network,‚Äù in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, ‚ÄúInstance-aware semantic segmentation via'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[96] A. Arnab and P. H. S. Torr, ‚ÄúPixelwise instance segmentation with a\\ndynamically instantiated network,‚Äù in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, ‚ÄúInstance-aware semantic segmentation via\\nmulti-task network cascades,‚Äù in CVPR, 2016.\\n[98] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, ‚ÄúFully convolutional instance-\\naware semantic segmentation,‚Äù in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n‚ÄúSpatial transformer networks,‚Äù in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, ‚ÄúStuffnet: Using stuffto\\nimprove object detection,‚Äù in WACV, 2017.\\n[101] T. Kong, A. Yao, Y . Chen, and F. Sun, ‚ÄúHypernet: Towards accurate\\nregion proposal generation and joint object detection,‚Äù in CVPR, 2016.\\n[102] A. Pentina, V . Sharmanska, and C. H. Lampert, ‚ÄúCurriculum learning\\nof multiple tasks,‚Äù in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, ‚ÄúRotating your\\nface using multi-task deep neural network,‚Äù in CVPR, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of multiple tasks,‚Äù in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, ‚ÄúRotating your\\nface using multi-task deep neural network,‚Äù in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, ‚ÄúMulti-stage object\\ndetection with group recursive learning,‚Äù arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, ‚ÄúA uniÔ¨Åed multi-scale\\ndeep convolutional neural network for fast object detection,‚Äù in ECCV,\\n2016.\\n[106] Y . Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, ‚Äúsegdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,‚Äù in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, ‚ÄúScene labeling\\nwith lstm recurrent neural networks,‚Äù in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, ‚ÄúLearning to detect and\\nlocalize many objects from few examples,‚Äù arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, ‚ÄúGated bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localize many objects from few examples,‚Äù arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, ‚ÄúGated bi-\\ndirectional cnn for object detection,‚Äù in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, ‚ÄúObject detection via a multi-region and\\nsemantic segmentation-aware cnn model,‚Äù in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, ‚ÄúBidirectional recurrent neural net-\\nworks,‚Äù IEEE Trans. Signal Process. , vol. 45, pp. 2673‚Äì2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y . Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll ¬¥ar, ‚ÄúA multipath network for object detection,‚Äù\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, ‚ÄúTraining region-based\\nobject detectors with online hard example mining,‚Äù in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, ‚ÄúObject detection\\nnetworks on convolutional feature maps,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476‚Äì1481, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, ‚ÄúObject detection\\nnetworks on convolutional feature maps,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476‚Äì1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, ‚ÄúFactors in Ô¨Ånetuning\\ndeep model for object detection with long-tail distribution,‚Äù in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y . Cheon, and M. Park, ‚ÄúPvanet:\\nLightweight deep neural networks for real-time object detection,‚Äù\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, ‚ÄúUnderstanding and\\nimproving convolutional neural networks via concatenated rectiÔ¨Åed\\nlinear units,‚Äù in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, ‚ÄúDeep neural networks for object\\ndetection,‚Äù in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll ¬¥ar, ‚ÄúLearning to segment object\\ncandidates,‚Äù in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, ‚ÄúScalable,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[119] P. O. Pinheiro, R. Collobert, and P. Doll ¬¥ar, ‚ÄúLearning to segment object\\ncandidates,‚Äù in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, ‚ÄúScalable,\\nhigh-quality object detection,‚Äù arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n‚ÄúThe pascal visual object classes challenge 2012 (voc2012) results\\n(2012),‚Äù in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, ‚ÄúVisualizing and understanding convolu-\\ntional networks,‚Äù in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll ¬¥ar, Z. Tu, and K. He, ‚ÄúAggregated residual\\ntransformations for deep neural networks,‚Äù in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei,\\n‚ÄúDeformable convolutional networks,‚Äù arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, ‚ÄúAutocollage,‚ÄùACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847‚Äì852, 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='‚ÄúDeformable convolutional networks,‚Äù arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, ‚ÄúAutocollage,‚ÄùACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847‚Äì852, 2006.\\n[126] C. Jung and C. Kim, ‚ÄúA uniÔ¨Åed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,‚Äù IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272‚Äì1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y . Chien, ‚ÄúReal-time salient object\\ndetection with a minimum spanning tree,‚Äù in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, ‚ÄúTop-down visual saliency via joint crf and\\ndictionary learning,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576‚Äì588, 2017.\\n[129] P. L. Rosin, ‚ÄúA simple method for detecting salient regions,‚Äù Pattern\\nRecognition, vol. 42, no. 11, pp. 2363‚Äì2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n‚ÄúLearning to detect a salient object,‚Äù IEEE Trans. Pattern Anal. Mach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Recognition, vol. 42, no. 11, pp. 2363‚Äì2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n‚ÄúLearning to detect a salient object,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353‚Äì367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, ‚ÄúFully convolutional networks\\nfor semantic segmentation,‚Äù in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, ‚ÄúDiscriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, pp. 989‚Äì1005, 2009.\\n[133] S. Xie and Z. Tu, ‚ÄúHolistically-nested edge detection,‚Äù in ICCV, 2015.\\n[134] M. K ¬®ummerer, L. Theis, and M. Bethge, ‚ÄúDeep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,‚Äù\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, ‚ÄúSalicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,‚Äù\\nin ICCV, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='arXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, ‚ÄúSalicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,‚Äù\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, ‚ÄúDeep networks for saliency\\ndetection via local estimation and global search,‚Äù in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, ‚ÄúWeakly supervised top-down\\nsalient object detection,‚Äù arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, ‚ÄúSaliency detection by\\nmulti-context deep learning,‚Äù in CVPR, 2015.\\n[139] C ¬∏ . Bak, A. Erdem, and E. Erdem, ‚ÄúTwo-stream convolutional networks\\nfor dynamic saliency prediction,‚Äù arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, ‚ÄúSupercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,‚Äù\\nInt. J. of Comput. Vision , vol. 115, no. 3, pp. 330‚Äì344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Int. J. of Comput. Vision , vol. 115, no. 3, pp. 330‚Äì344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and\\nJ. Wang, ‚ÄúDeepsaliency: Multi-task deep neural network model for\\nsalient object detection,‚Äù IEEE Trans. Image Process. , vol. 25, no. 8,\\npp. 3919‚Äì3930, 2016.\\n[142] Y . Tang and X. Wu, ‚ÄúSaliency detection via combining region-level\\nand pixel-level predictions with cnns,‚Äù in ECCV, 2016.\\n[143] G. Li and Y . Yu, ‚ÄúDeep contrast learning for salient object detection,‚Äù\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, ‚ÄúEdge preserving and\\nmulti-scale contextual neural network for salient object detection,‚Äù\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, ‚ÄúA deep multi-level\\nnetwork for saliency prediction,‚Äù in ICPR, 2016.\\n[146] G. Li and Y . Yu, ‚ÄúVisual saliency detection based on multiscale deep\\ncnn features,‚Äù IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012‚Äì\\n5024, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[146] G. Li and Y . Yu, ‚ÄúVisual saliency detection based on multiscale deep\\ncnn features,‚Äù IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012‚Äì\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O‚ÄôConnor,\\n‚ÄúShallow and deep convolutional networks for saliency prediction,‚Äù in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, ‚ÄúRecurrent attentional networks for\\nsaliency detection,‚Äù in CVPR, 2016.\\n[149] Y . Tang, X. Wu, and W. Bu, ‚ÄúDeeply-supervised recurrent convolutional\\nneural network for saliency detection,‚Äù in ACM MM, 2016.\\n[150] X. Li, Y . Li, C. Shen, A. Dick, and A. Van Den Hengel, ‚ÄúContextual\\nhypergraph modeling for salient object detection,‚Äù in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, ‚ÄúGlobal\\ncontrast based salient region detection,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569‚Äì582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, ‚ÄúSalient object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='contrast based salient region detection,‚Äù IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569‚Äì582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, ‚ÄúSalient object\\ndetection: A discriminative regional feature integration approach,‚Äù in\\nCVPR, 2013.\\n[153] G. Lee, Y .-W. Tai, and J. Kim, ‚ÄúDeep saliency with encoded low level\\ndistance map and high level features,‚Äù in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n‚ÄúNon-local deep features for salient object detection,‚Äù in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n‚ÄúDeeply supervised salient object detection with short connections,‚Äù\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, ‚ÄúHierarchical saliency detection,‚Äù in\\nCVPR, 2013.\\n[157] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, ‚ÄúThe secrets of\\nsalient object segmentation,‚Äù in CVPR, 2014.\\n[158] V . Movahedi and J. H. Elder, ‚ÄúDesign and perceptual validation of\\nperformance measures for salient object segmentation,‚Äù in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, ‚ÄúSalient object detection:\\nA benchmark,‚Äù IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706‚Äì\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, ‚ÄúGraphical representation for\\nheterogeneous face recognition,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301‚Äì312, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, ‚ÄúGraphical representation for\\nheterogeneous face recognition,‚Äù IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301‚Äì312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, ‚ÄúFace recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,‚Äù in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, ‚ÄúFace sketchcphoto synthesis\\nand retrieval using sparse representation,‚Äù IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213‚Äì1226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, ‚ÄúA comprehensive survey\\nto face hallucination,‚Äù Int. J. of Comput. Vision , vol. 106, no. 1, pp.\\n9‚Äì30, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, ‚ÄúMultiple\\nrepresentations-based face sketch-photo synthesis.‚ÄùIEEE Trans. Neural\\nNetw. & Learning Syst. , vol. 27, no. 11, pp. 2201‚Äì2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, ‚ÄúAutomatic facial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Netw. & Learning Syst. , vol. 27, no. 11, pp. 2201‚Äì2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, ‚ÄúAutomatic facial\\nexpression recognition system using deep network-based data fusion,‚Äù\\nIEEE Trans. Cybern. , vol. 48, pp. 103‚Äì114, 2018.\\n[166] P. Viola and M. Jones, ‚ÄúRobust real-time face detection,‚Äù Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137‚Äì154, 2004.\\n[167] J. Yu, Y . Jiang, Z. Wang, Z. Cao, and T. Huang, ‚ÄúUnitbox: An advanced\\nobject detection network,‚Äù in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, ‚ÄúMulti-view face detection\\nusing deep convolutional neural networks,‚Äù in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, ‚ÄúFrom facial parts responses\\nto face detection: A deep learning approach,‚Äù in ICCV, 2015.\\n[170] S. Yang, Y . Xiong, C. C. Loy, and X. Tang, ‚ÄúFace detection through\\nscale-friendly deep convolutional networks,‚Äù in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, ‚ÄúScale-aware face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale-friendly deep convolutional networks,‚Äù in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, ‚ÄúScale-aware face\\ndetection,‚Äù in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y . Wang, ‚ÄúFace r-cnn,‚Äù arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, ‚ÄúFace detection using deep learning: An\\nimproved faster rcnn approach,‚Äù arXiv:1701.08289, 2017.\\n[174] L. Huang, Y . Yang, Y . Deng, and Y . Yu, ‚ÄúDensebox: Unifying landmark\\nlocalization with end to end object detection,‚Äù arXiv:1509.04874, 2015.\\n[175] Y . Li, B. Sun, T. Wu, and Y . Wang, ‚Äúface detection with end-to-end\\nintegration of a convnet and a 3d model,‚Äù in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, ‚ÄúJoint face detection and\\nalignment using multitask cascaded convolutional networks,‚Äù IEEE\\nSignal Process. Lett. , vol. 23, no. 10, pp. 1499‚Äì1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, ‚ÄúCompact convolutional neural\\nnetwork cascadefor face detection,‚Äù in CEUR Workshop, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Signal Process. Lett. , vol. 23, no. 10, pp. 1499‚Äì1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, ‚ÄúCompact convolutional neural\\nnetwork cascadefor face detection,‚Äù in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, ‚ÄúJoint training of cascaded cnn for\\nface detection,‚Äù in CVPR, 2016.\\n[179] V . Jain and E. Learned-Miller, ‚ÄúFddb: A benchmark for face detection\\nin unconstrained settings,‚Äù Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, ‚ÄúA convolutional neural\\nnetwork cascade for face detection,‚Äù in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúAggregate channel features for\\nmulti-view face detection,‚Äù in IJCB, 2014.\\n[182] N. Marku Àás, M. Frljak, I. S. Pand Àázi¬¥c, J. Ahlberg, and R. Forchheimer,\\n‚ÄúObject detection with pixel intensity comparisons organized in deci-\\nsion trees,‚Äù arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, ‚ÄúFace\\ndetection without bells and whistles,‚Äù in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sion trees,‚Äù arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, ‚ÄúFace\\ndetection without bells and whistles,‚Äù in ECCV, 2014.\\n[184] J. Li and Y . Zhang, ‚ÄúLearning surf cascade for fast and accurate object\\ndetection,‚Äù in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, ‚ÄúA fast and accurate unconstrained\\nface detector,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211‚Äì223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúConvolutional channel features,‚Äù\\nin ICCV, 2015.\\n[187] R. Ranjan, V . M. Patel, and R. Chellappa, ‚ÄúHyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,‚Äù arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, ‚ÄúFinding tiny faces,‚Äù in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, ‚ÄúMultiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,‚Äù IEEE Trans.\\nImage Process., vol. 27, pp. 1361‚Äì1375, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[189] Z. Jiang and D. Q. Huynh, ‚ÄúMultiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,‚Äù IEEE Trans.\\nImage Process., vol. 27, pp. 1361‚Äì1375, 2018.\\n[190] D. Gavrila and S. Munder, ‚ÄúMulti-cue pedestrian detection and tracking\\nfrom a moving vehicle,‚Äù Int. J. of Comput. Vision , vol. 73, pp. 41‚Äì59,\\n2006.\\n[191] S. Xu, Y . Cheng, K. Gu, Y . Yang, S. Chang, and P. Zhou, ‚ÄúJointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidentiÔ¨Åcation,‚Äù in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, ‚ÄúStepwise metric promotion for unsuper-\\nvised video person re-identiÔ¨Åcation,‚Äù in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, ‚ÄúCooperative robots to observe\\nmoving targets: Review,‚Äù IEEE Trans. Cybern. , vol. 48, pp. 187‚Äì198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets robotics:\\nThe kitti dataset,‚Äù Int. J. of Robotics Res. , vol. 32, pp. 1231‚Äì1237,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets robotics:\\nThe kitti dataset,‚Äù Int. J. of Robotics Res. , vol. 32, pp. 1231‚Äì1237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, ‚ÄúLearning complexity-aware\\ncascades for deep pedestrian detection,‚Äù in ICCV, 2015.\\n[196] Y . Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning strong parts\\nfor pedestrian detection,‚Äù in CVPR, 2015.\\n[197] P. Doll ¬¥ar, R. Appel, S. Belongie, and P. Perona, ‚ÄúFast feature pyramids\\nfor object detection,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 36,\\nno. 8, pp. 1532‚Äì1545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, ‚ÄúFiltered channel features for\\npedestrian detection,‚Äù in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, ‚ÄúPedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,‚Äù\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243‚Äì1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, ‚ÄúDiscriminatively trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243‚Äì1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, ‚ÄúDiscriminatively trained\\nand-or graph models for object shape detection,‚Äù IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 5, pp. 959‚Äì972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, ‚ÄúHandling\\nocclusions with franken-classiÔ¨Åers,‚Äù in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, ‚ÄúDetection and tracking of\\noccluded people,‚Äù Int. J. of Comput. Vision, vol. 110, pp. 58‚Äì69, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, ‚ÄúIs faster r-cnn doing well for\\npedestrian detection?‚Äù in ECCV, 2016.\\n[204] Y . Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúDeep learning strong parts\\nfor pedestrian detection,‚Äù in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, ‚ÄúMultispectral deep\\nneural networks for pedestrian detection,‚Äù arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúPedestrian detection aided by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='neural networks for pedestrian detection,‚Äù arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, ‚ÄúPedestrian detection aided by\\ndeep learning semantic tasks,‚Äù in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, ‚ÄúFused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,‚Äù in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, ‚ÄúPushing\\nthe limits of deep cnns for pedestrian detection,‚Äù IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom ¬¥e, L. Bondi, L. BarofÔ¨Åo, S. Tubaro, E. Plebani, and D. Pau,\\n‚ÄúReduced memory region based deep convolutional neural network\\ndetection,‚Äù in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, ‚ÄúTaking a deeper\\nlook at pedestrians,‚Äù in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, ‚ÄúScale-aware fast\\nr-cnn for pedestrian detection,‚Äù arXiv:1510.08160, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='look at pedestrians,‚Äù in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, ‚ÄúScale-aware fast\\nr-cnn for pedestrian detection,‚Äù arXiv:1510.08160, 2015.\\n[212] Y . Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, ‚ÄúVisual-textual\\njoint relevance learning for tag-based social image search,‚ÄùIEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363‚Äì376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y . Chen, ‚ÄúRon: Reverse\\nconnection with objectness prior networks for object detection,‚Äù in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y . Bengio, ‚ÄúGenerative adversarial nets,‚Äù\\nin NIPS, 2014.\\n[215] Y . Fang, K. Kuan, J. Lin, C. Tan, and V . Chandrasekhar, ‚ÄúObject\\ndetection meets knowledge graphs,‚Äù in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, ‚ÄúSaliency-based sequential\\nimage attention with multiset prediction,‚Äù in NIPS, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection meets knowledge graphs,‚Äù in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, ‚ÄúSaliency-based sequential\\nimage attention with multiset prediction,‚Äù in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, ‚ÄúLearning detection with diverse\\nproposals,‚Äù in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, ‚ÄúEnd-to-end\\nmemory networks,‚Äù in NIPS, 2015.\\n[219] P. Dabkowski and Y . Gal, ‚ÄúReal time image saliency for black box\\nclassiÔ¨Åers,‚Äù in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúCraft objects from images,‚Äù in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V . Bogolin, and M. Leordeanu, ‚ÄúUnsupervised learning\\nfrom video to detect foreground objects in single images,‚Äù in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, ‚ÄúWeakly supervised object\\nlocalization with latent category learning,‚Äù in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V . Ferrari,\\n‚ÄúTraining object class detectors with click supervision,‚Äù inCVPR, 2017.\\n[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, ‚ÄúSpeed/accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, ‚ÄúSpeed/accuracy\\ntrade-offs for modern convolutional object detectors,‚Äù in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, ‚ÄúMimicking very efÔ¨Åcient network for object\\ndetection,‚Äù in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a\\nneural network,‚Äù Comput. Sci., vol. 14, no. 7, pp. 38‚Äì39, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY . Bengio, ‚ÄúFitnets: Hints for thin deep nets,‚Äù Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y . Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, ‚Äú3d object proposals for accurate object class detection,‚Äù\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, ‚ÄúVisual-inertial-semantic scene repre-\\nsentation for 3d object detection,‚Äù in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[229] J. Dong, X. Fei, and S. Soatto, ‚ÄúVisual-inertial-semantic scene repre-\\nsentation for 3d object detection,‚Äù in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n‚ÄúObject detection in videos with tubelet proposal networks,‚Äù in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='His research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor‚Äôs degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in ArtiÔ¨Åcial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='formation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='I am writing to present our proposal for a strategic skilling partnership between Krish Naik\\nAcademy (KrishAI Technologies Pvt. Ltd.) and the Nipuna Karnataka Scheme, aimed at\\ntransforming Karnataka into India‚Äôs AI Talent Engine.\\nAs a proud native of Gulbarga, Karnataka, my journey in AI began in 2017, overcoming resource\\nconstraints to build one of the world‚Äôs largest AI and Data Science learning communities, now\\nwith over 1.5 million learners trained across 180+ countries and 60,000+ successful career\\ntransitions. Our mission is to make AI & Data Science education accessible, affordable, and\\nindustry-aligned, ensuring every learner, regardless of location, has a pathway to global\\ntechnology careers.\\nOur proposal aligns directly with Nipuna Karnataka‚Äôs vision of skilling 1 crore youth by 2030,\\nfocusing on:\\nComprehensive AI & Data Skilling ‚Äì Industry-aligned training tracks including Data Analyst,\\nData Scientist, GenAI Developer, AI Engineer (MLOps), and Big Data Engineer.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='focusing on:\\nComprehensive AI & Data Skilling ‚Äì Industry-aligned training tracks including Data Analyst,\\nData Scientist, GenAI Developer, AI Engineer (MLOps), and Big Data Engineer.\\nPlacement-Focused Learning ‚Äì Leveraging our AI Mock Interview Platform, structured\\nhiring network, and measurable KPIs to ensure employability.\\nRural & Inclusive Outreach ‚Äì Hybrid delivery in Kannada + English, targeting Tier-2/3\\ndistricts and underrepresented groups.\\nFaculty Enablement ‚Äì Train-the-Trainer programs for government institution educators to\\ncreate a self-sustaining AI talent ecosystem.\\nBy combining our proven expertise in large-scale, outcome-driven AI skilling with the\\nGovernment of Karnataka‚Äôs vision, we can build a future-ready workforce, strengthen\\nKarnataka‚Äôs leadership in technology, and make the state a national lighthouse for AI skills\\ndevelopment.\\nWe are excited about the possibility of contributing to this transformative journey and look'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Karnataka‚Äôs leadership in technology, and make the state a national lighthouse for AI skills\\ndevelopment.\\nWe are excited about the possibility of contributing to this transformative journey and look\\nforward to discussing how we can work together to achieve the ambitious goals of the Nipuna\\nKarnataka Scheme.\\nThank you for your time and consideration.\\nWarm regards,\\nKrish Naik\\n10 August, 2025\\nTo Hon‚Äôble Minister Priyank Kharge\\nIT BT Minister Karnataka\\nKrish Naik\\nFounder & CEO, KrishAI Technologies Pvt. Ltd.\\ncontact@krishnaik.in\\n+91 88673 53949\\nLinkedIn | YouTube')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bde24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 359 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:06<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (359, 384)\n",
      "Adding 359 documents to vector store...\n",
      "Successfully added 359 documents to vector store\n",
      "Total documents in collection: 1077\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acd10",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351730b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x2604efc0ec0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7e78529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention is all you need'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 83.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_f4b24756_12',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'page': 2,\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'keywords': '',\n",
       "   'doc_index': 12,\n",
       "   'total_pages': 15,\n",
       "   'author': '',\n",
       "   'content_length': 216,\n",
       "   'page_label': '3',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'subject': '',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00'},\n",
       "  'similarity_score': 0.1399550437927246,\n",
       "  'distance': 0.8600449562072754,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_1949e541_12',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'author': '',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'total_pages': 15,\n",
       "   'doc_index': 12,\n",
       "   'file_type': 'pdf',\n",
       "   'page': 2,\n",
       "   'title': '',\n",
       "   'page_label': '3',\n",
       "   'subject': '',\n",
       "   'content_length': 216,\n",
       "   'keywords': '',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'trapped': '/False',\n",
       "   'source_file': 'attention.pdf'},\n",
       "  'similarity_score': 0.1399550437927246,\n",
       "  'distance': 0.8600449562072754,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_1b0dbfc8_12',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'keywords': '',\n",
       "   'page_label': '3',\n",
       "   'content_length': 216,\n",
       "   'author': '',\n",
       "   'page': 2,\n",
       "   'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'trapped': '/False',\n",
       "   'doc_index': 12,\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'title': '',\n",
       "   'total_pages': 15,\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.1399550437927246,\n",
       "  'distance': 0.8600449562072754,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d8141ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 112.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_bd5cc745_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n‚Ä¢ We propose a uniÔ¨Åed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eÔ¨Éciency in model training ;\\n‚Ä¢ We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniÔ¨Åcantly enhance the quality of training corpora, thereby impro ving model‚Äôs\\nrobustness and generalization capabilities;\\n‚Ä¢ We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'page': 2,\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941,\n",
       "   'doc_index': 61,\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'page_label': '3',\n",
       "   'total_pages': 27,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'keywords': '',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_442dbe7b_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n‚Ä¢ We propose a uniÔ¨Åed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eÔ¨Éciency in model training ;\\n‚Ä¢ We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniÔ¨Åcantly enhance the quality of training corpora, thereby impro ving model‚Äôs\\nrobustness and generalization capabilities;\\n‚Ä¢ We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'doc_index': 61,\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'content_length': 941,\n",
       "   'page_label': '3',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'keywords': '',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_45e2cfa9_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n‚Ä¢ We propose a uniÔ¨Åed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eÔ¨Éciency in model training ;\\n‚Ä¢ We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniÔ¨Åcantly enhance the quality of training corpora, thereby impro ving model‚Äôs\\nrobustness and generalization capabilities;\\n‚Ä¢ We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'page_label': '3',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'total_pages': 27,\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941,\n",
       "   'doc_index': 61,\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'keywords': '',\n",
       "   'page': 2},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3497d198_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciÔ¨Åc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eÔ¨Äectively enhance embedding capa bilities. We propose\\na uniÔ¨Åed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiÔ¨Åcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk‚Äôs components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'page': 4,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'doc_index': 71,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'content_length': 937,\n",
       "   'total_pages': 27,\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1'},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_bb00aef7_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciÔ¨Åc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eÔ¨Äectively enhance embedding capa bilities. We propose\\na uniÔ¨Åed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiÔ¨Åcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk‚Äôs components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'keywords': '',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'total_pages': 27,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'doc_index': 71,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'content_length': 937,\n",
       "   'page': 4,\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23783e",
   "metadata": {},
   "source": [
    "### RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4b617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40bba05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: gemma2-9b-it\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4110c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 88.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_bd5cc745_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n‚Ä¢ We propose a uniÔ¨Åed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eÔ¨Éciency in model training ;\\n‚Ä¢ We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniÔ¨Åcantly enhance the quality of training corpora, thereby impro ving model‚Äôs\\nrobustness and generalization capabilities;\\n‚Ä¢ We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'keywords': '',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'total_pages': 27,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'page_label': '3',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'content_length': 941,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'page': 2,\n",
       "   'doc_index': 61,\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_442dbe7b_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n‚Ä¢ We propose a uniÔ¨Åed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eÔ¨Éciency in model training ;\\n‚Ä¢ We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniÔ¨Åcantly enhance the quality of training corpora, thereby impro ving model‚Äôs\\nrobustness and generalization capabilities;\\n‚Ä¢ We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 61,\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'keywords': '',\n",
       "   'content_length': 941,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_45e2cfa9_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking Ô¨Årst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eÔ¨Äectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n‚Ä¢ We propose a uniÔ¨Åed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eÔ¨Éciency in model training ;\\n‚Ä¢ We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniÔ¨Åcantly enhance the quality of training corpora, thereby impro ving model‚Äôs\\nrobustness and generalization capabilities;\\n‚Ä¢ We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'keywords': '',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'total_pages': 27,\n",
       "   'content_length': 941,\n",
       "   'page': 2,\n",
       "   'doc_index': 61,\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'file_type': 'pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'page_label': '3',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'title': 'QZhou-Embedding Technical Report'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3497d198_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciÔ¨Åc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eÔ¨Äectively enhance embedding capa bilities. We propose\\na uniÔ¨Åed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiÔ¨Åcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk‚Äôs components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'page': 4,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'total_pages': 27,\n",
       "   'content_length': 937,\n",
       "   'doc_index': 71,\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_bb00aef7_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciÔ¨Åc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eÔ¨Äectively enhance embedding capa bilities. We propose\\na uniÔ¨Åed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiÔ¨Åcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk‚Äôs components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'content_length': 937,\n",
       "   'page': 4,\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'doc_index': 71,\n",
       "   'page_label': '5',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea465ac",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df1bf366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 76.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An attention mechanism is a function that maps a query and a set of key-value pairs to an output vector, using a weighted sum of the values.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857b1c2",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2832fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Hard Negative Mining Technqiues'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 95.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The text describes several hard negative mining techniques used in contrastive learning for retrieval models:\n",
      "\n",
      "* **ANCE:** Uses asynchronous ANN indexing and checkpoint states to periodically update hard negatives.\n",
      "* **Conan-Embedding:** Employs a dynamic strategy, excluding and refreshing samples based on score thresholds.\n",
      "* **NV-Retriever:**  Proposes positive-aware mining with TopK-MarginPos and TopKPercPos filtering to reduce false negatives.\n",
      "* **LGAI-Embedding:** Builds on NV-Retriever, using ANNA IR as a teacher model to identify high-quality hard negatives and TopKPercPos filtering. \n",
      "\n",
      "\n",
      "\n",
      "Sources: [{'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}, {'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}, {'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}]\n",
      "Confidence: 0.18709993362426758\n",
      "Context Preview: QZhou-Embedding Technical Report\n",
      " Kingsoft AI\n",
      "2.4 Hard Negative Mining Techniques\n",
      "Hard negatives serve as essential components in contrastive lear ning for retrieval model\n",
      "training. Early work like ANCE[\n",
      "46] proposed an asynchronous ANN indexing mech-\n",
      "anism that periodically updates hard negatives u\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Hard Negative Mining Technqiues\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa6150d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is attention is all you need'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 88.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention functi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "Question: what is attention is all you need\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: \"Attention Is All You Need\" is a paper that introduced the Transformer model, which relies solely on attention mechanisms for sequence transduction tasks, eliminating the need for recurrent or convolutional networks.  \n",
      "\n",
      "\n",
      "Citations:\n",
      "[1] attention.pdf (page 2)\n",
      "[2] attention.pdf (page 2)\n",
      "[3] attention.pdf (page 2)\n",
      "Summary: The paper \"Attention Is All You Need\" introduced the Transformer model, a novel architecture for sequence transduction tasks.  This model utilizes only attention mechanisms, dispensing with traditional recurrent or convolutional networks. \n",
      "\n",
      "\n",
      "\n",
      "History: {'question': 'what is attention is all you need', 'answer': '\"Attention Is All You Need\" is a paper that introduced the Transformer model, which relies solely on attention mechanisms for sequence transduction tasks, eliminating the need for recurrent or convolutional networks.  \\n', 'sources': [{'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}], 'summary': 'The paper \"Attention Is All You Need\" introduced the Transformer model, a novel architecture for sequence transduction tasks.  This model utilizes only attention mechanisms, dispensing with traditional recurrent or convolutional networks. \\n\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695e1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
