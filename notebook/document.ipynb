{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4531f634",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc7d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Document Structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8546ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'exmaple.txt', 'pages': 1, 'author': 'Krish Naik', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"exmaple.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Krish Naik\",\n",
    "        \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62c4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3034428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"‚úÖ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b987179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Python313\\\\python313.zip',\n",
       " 'c:\\\\Python313\\\\DLLs',\n",
       " 'c:\\\\Python313\\\\Lib',\n",
       " 'c:\\\\Python313',\n",
       " '',\n",
       " 'C:\\\\Users\\\\nilad\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages',\n",
       " 'C:\\\\Users\\\\nilad\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\nilad\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\nilad\\\\AppData\\\\Roaming\\\\Python\\\\Python313\\\\site-packages\\\\Pythonwin',\n",
       " 'c:\\\\Python313\\\\Lib\\\\site-packages']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac27e9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilad\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f9547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3d9d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2025-10-24T19:40:28+08:00', 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'total_pages': 4, 'format': 'PDF 1.3', 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping', 'author': '', 'subject': 'Science 2025.389:1012-1015', 'keywords': '', 'moddate': '2025-11-30T15:08:48-08:00', 'trapped': '', 'modDate': \"D:20251130150848-08'00'\", 'creationDate': \"D:20251024194028+08'00'\", 'page': 0}, page_content='Research ArticleS\\nScience\\u2003\\n4 September 2025\\n1012\\nASTROPHYSICS\\nImproving cosmological reach  \\nof a gravitational wave observatory \\nusing Deep Loop Shaping\\nJonas Buchli1*‚Ä† Brendan Tracey1‚Ä†, Tomislav Andric2,3‚Ä†, \\nChristopher Wipf4‚Ä†, Yu Him Justin Chiu1‚Ä†, Matthias Lochbrunner1‚Ä†, \\nCraig Donner1‚Ä†, Rana X. Adhikari4*, Jan Harms2,3*‚Ä†,  \\nIain Barr1, Roland Hafner1, Andrea Huber1, Abbas Abdolmaleki1,  \\nCharlie Beattie1, Joseph Betzwieser4, Serkan Cabi1,  \\nJonas Degrave1, Yuzhu Dong1, Leslie Fritz1, Anchal Gupta4,  \\nOliver Groth1, Sandy Huang1, Tamara Norman1, Hannah Openshaw1, \\nJameson Rollins4, Greg Thornton1, George van den Driessche1, \\nMarkus Wulfmeier1, Pushmeet Kohli1*, Martin Riedmiller1,  \\nThe LIGO Instrument Team‚Ä°\\nImproved low-\\xadfrequency sensitivity of gravitational wave \\nobservatories would unlock study of intermediate-\\xadmass black \\nhole mergers and binary black hole eccentricity and provide \\nearly warnings for multimessenger observations of binary \\nneutron star mergers. Today‚Äôs mirror stabilization control \\ninjects harmful noise, constituting a major obstacle to \\nsensitivity improvements. We eliminated this noise through \\nDeep Loop Shaping, a reinforcement learning method using \\nfrequency domain rewards. We proved our methodology on the \\nLIGO Livingston Observatory (LLO). Our controller reduced \\ncontrol noise in the 10-\\xad to 30-\\xadhertz band by over 30x and up to \\n100x in subbands, surpassing the design goal motivated by the \\nquantum limit. These results highlight the potential of Deep \\nLoop Shaping to improve current and future gravitational wave \\nobservatories and, more broadly, instrumentation and control \\nsystems.\\nThe gravitational wave (GW) detectors LIGO and Virgo have revolu-\\ntionized astrophysics by detecting mergers of exotic objects, such as \\nblack holes (BHs) and neutron stars (NSs) (1‚Äì3). Currently, most of \\nthe detectable signal lies in the 30-\\xad to 2000-\\xadHz band, leaving the low-\\xad\\u200b\\nfrequency band (10 to 30 Hz) largely unexplored. Enhancing sensitiv-\\nity in this band could lead to a substantial increase in cosmological \\nreach and thus in the scientific capabilities of LIGO (Fig.\\xa01A). The \\n10-\\xad to 30-\\xadHz band is also important for the early (premerger) detec-\\ntion of binary neutron stars (BNSs), potentially doubling the warning \\ntime, which would enable real-\\xadtime observation of neutron star colli-\\nsions, the subsequent creation of heavy elements, and the birth of\\nblack holes (4‚Äì6). However, such sensitivity improvements are cur-\\nrently partially limited by injected control noise on the interferom-\\neter mirrors. Furthermore, as the control noise is a bottleneck to\\noverall sensitivity improvements, there is currently little to be gained\\nfrom improvements to other noise sources. We addressed this chal-\\nlenge with a new tailored reinforcement learning (RL) method and\\nimproved the alignment control of the LIGO mirrors. We lowered\\nthe injected control noise on the most demanding feedback control\\nloop, the common-\\xadhard-\\xadpitch (Œ∏CHP) loop of the Livingston Obser\\xadva\\xadtory,\\nbelow the quantum back-\\xadaction limit. By eliminating the harmful \\nnoise from this critical representative controller, we paved the path \\nto improve LIGO‚Äôs sensitivity.\\nThe space-\\xadtime strain associated with even the loudest GW signals \\nproduces a signal equivalent to only ‚âà10‚Äì19 meters of mirror motion. \\nAs a comparison, the environmental disturbance, due to Earth tides \\nand seismic vibration, is roughly 13 orders of magnitude larger. To \\nmeasure the weak GW signals, laser-\\xadinterferometric GW detectors \\nhave hundreds of optomechanical degrees of freedom that require \\nstabilization. Active control is used to achieve precise stabilization \\nin the face of complex mirror dynamics and inherently unstable \\ndegrees of freedom. More specifically, the optomechanical response \\n1Google DeepMind, London, UK. 2Gran Sasso Science Institute (GSSI), L‚ÄôAquila, Italy. \\n3Laboratori Nazionali del Gran Sasso, Assergi (INFN), Italy. 4LIGO Laboratory, Division of \\nPhysics, Math, and Astronomy, California Institute of Technology, Pasadena, CA, USA. \\n*Corresponding author. Email: buchli@\\u200bgoogle.\\u200bcom (J.B.); rana@\\u200bcaltech.\\u200bedu (R.X.A.);\\njan.\\u200bharms@\\u200bgssi.\\u200bit (J.H.); pushmeet@\\u200bgoogle.\\u200bcom (P.K.) ‚Ä†These authors contributed\\nequally to this work. ‚Ä°LIGO Instrument Team authors and affiliations are listed in the \\nsupplementary materials.\\nA\\nB\\nFig. 1. Cosmological reach and strain noise from control. (A) The plot shows the \\nvolume in space explored with binary BH merger waveforms (21) for different cases of \\ntechnical noise. The x axis in (A) is the total mass of the equal-\\xadmass binary pair. This \\ncorresponds to the x axis in (B), the frequency of the first quasinormal mode of a \\nSchwarzschild BH with such a mass, as measured in the source frame. The purple trace \\nshows the reach of LIGO as of March 2024. The green trace shows the volumetric \\nimprovement in the case where the technical noise is removed entirely. Many of the \\nknown technical noise sources are linked to controls. (B) LIGO‚Äôs noise budget and \\ncontroller performance. Lavender represents overall measured strain noise, red \\nrepresents strain noise contribution from the currently operational linear controller for \\nŒ∏CHP, and blue represents strain noise contribution from neural network RL policy as\\nrun on the LIGO Livingston Observatory on 5 Dec 2024 (mean, 10th and 90th percentiles \\nof amplitude spectral density (ASD) of control action of neural network control policy). \\nThe dashed green line indicates the control design goal derived from the quantum \\nback-\\xadaction limit by applying a design margin of 10x; the control noise should drop below \\nthis curve. A detailed accounting of technical noise sources is available in (22).\\nCORRECTED 24 OCTOBER 202; SEE LAST PAGE\\nDownloaded from https://www.science.org on November 30, 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2025-10-24T19:40:28+08:00', 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'total_pages': 4, 'format': 'PDF 1.3', 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping', 'author': '', 'subject': 'Science 2025.389:1012-1015', 'keywords': '', 'moddate': '2025-11-30T15:08:48-08:00', 'trapped': '', 'modDate': \"D:20251130150848-08'00'\", 'creationDate': \"D:20251024194028+08'00'\", 'page': 1}, page_content='Research ArticleS\\nScience\\u2003\\n4 September 2025\\n1013\\nof the interferometer (i.e., the plant) is subject to dynamic variations: \\nEven low absorption of the high-\\xadpower laser beam (‚àº300 to 500 kW) \\ncauses thermal distortions in the mirrors, leading to offsets in sensor \\nsignals and changes in optomechanical resonant frequencies. In ad-\\ndition, the high-\\xadpower laser also creates substantial forces and \\ntorques on the suspended mirrors, leading to optomechanical insta-\\nbilities of several mechanical eigenmodes (7‚Äì9). These resonances \\nare stabilized using feedback control, but any noise injected by the \\nfeedback controllers into the GW readout harms the peak astrophysi-\\ncal sensitivity and drowns out the GW signals themselves.\\nIn simplified terms, the main control design challenge is that \\nlarger control action in lower frequencies provides better disturbance \\nrejection but injects higher noise into the observation band. Con\\xad\\nversely, lowering the control action reduces injected noise but results \\nin insufficient disturbance rejection and possible loss of stability. \\nLinear control systems theory shows fundamental limits to this \\ntrade-\\xadoff (10,\\xa011) under certain assumptions about the controller de-\\nsign. The ultimate aim of controller design is to shape the ‚Äúclosed-\\xadloop‚Äù \\nbehavior, i.e., the performance of the designed controller acting in a \\nfeedback loop with the plant.\\nThere are many classical methods to achieve the desired closed-\\xad\\nloop behavior. Early methods, i.e., the classic (open)‚Äìloop shaping \\nmethods, exploit the direct relationship between the open-\\xad and \\nclosed-\\xadloop transfer functions to design the controller. Since the \\n1980s, the focus of design has shifted from open-\\xadloop design to di-\\nrectly shaping closed-\\xadloop transfer functions, i.e., the sensitivity func-\\ntions (12,\\xa013), usually through optimization [e.g., convex optimization \\n(14), H‚àû (15)]. These methods are more general and can take into \\naccount a larger variety of design goals and constraints. Yet they still \\nrequire strong assumptions, such as convexity and linearity. For GW \\ndetectors like LIGO, progress using traditional approaches has come \\nto a plateau. Machine learning for interferometer control was pre-\\nsented in (16), with the primary aim to improve the contrast of the \\ninterferometer rather than to optimize closed-loop performance.\\nIn this work, we present a new control design method, Deep Loop \\nShaping (DLS), to design controllers that satisfy specific demands on \\nthe system‚Äôs frequency \\xaddomain behavior (Fig.\\xa02). DLS has no constraints \\nregarding the use of nonlinear models and control structures. It exploits \\nthe machinery of deep reinforcement learning to directly optimize \\nfrequency domain properties and shape the closed-loop behavior. We \\ndemonstrate DLS‚Äôs utility on the critical LIGO Œ∏CHP control loop, \\nachieving state-\\xadof-\\xadthe-\\xadart feedback control performance. The injected \\ncontrol noise was reduced by up to two orders of magnitude while \\nmaintaining mirror stability. Applying DLS more widely on LIGO can \\nimprove sensitivity. Furthermore, the method has wide applicability \\nto control engineering; for example, highly unstable systems, vibra-\\ntion suppression, and noise cancellation all have strong frequency-\\xad\\ndependent control demands.\\nThe LIGO controls challenge\\nAngular sensing and control (ASC) is the challenge of maintaining the \\norientation of the interferometer mirrors. Stabilization is accom-\\nplished through a hybrid active-\\xadpassive isolation system. Passive sta-\\nbilization happens through a series of pendulums, from which the \\noptics are suspended. These pendulums suppress seismic disturbances \\nat frequencies above 10 Hz by several orders of magnitude. However, \\n3\\nSIMULATION\\nENVIRONMENT\\n2\\nREAL WORLD\\nSYSTEM ID\\nREAL WORLD\\nDEPLOYMENT\\n1\\nREPLAY\\nBUFFER\\nREPLAY\\nBUFFER\\nLIGO\\n4km\\n4km\\nLEARNING\\nLOOPS\\nCODE\\nGENERATION\\nREAL TIME CONTROL SYSTEM\\n<import\\n  control.h>\\nGOOD\\nX\\nf\\nf\\nBAD\\nGOOD\\nr(t)\\n0\\n1\\n1\\n0\\nBAD\\nspectral density\\nFILTER\\nFILTER\\nSCORE\\nSCORE\\nspectral density\\nA\\nB\\nFig. 2. DLS: Reinforcement learning with frequency domain rewards. (A) (1) A model is identified from plant measurements. (2) The identified model is used as a learning \\nenvironment. Frequency domain rewards are used to compute rewards. (3) The optimized control policy is deployed on the plant. (B) Illustration of the frequency domain \\nrewards and the multiplicative scoring.\\nDownloaded from https://www.science.org on November 30, 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2025-10-24T19:40:28+08:00', 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'total_pages': 4, 'format': 'PDF 1.3', 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping', 'author': '', 'subject': 'Science 2025.389:1012-1015', 'keywords': '', 'moddate': '2025-11-30T15:08:48-08:00', 'trapped': '', 'modDate': \"D:20251130150848-08'00'\", 'creationDate': \"D:20251024194028+08'00'\", 'page': 2}, page_content='Research ArticleS\\nScience\\u2003\\n4 September 2025\\n1014\\nactive stabilization is required to reject seismic disturbances at fre-\\nquencies below ~3 Hz. This stabilization is accomplished by a set of \\nactuators that produce a torque on the suspended optics at the pen-\\nultimate stage of the suspension system. Additionally, there are dis-\\nturbances caused by the radiation-\\xadpressure forces of the high-\\xadpower \\nlaser beam (17) that couple the angular motions of the cavity mirrors. \\nThe sensors used to measure the angular motion have a good signal-\\xad\\nto-\\xadnoise ratio at frequencies below a few hertz to enable active sta-\\nbilization, but in the 10-\\xad to 30-\\xadHz band, the sensor noise is orders \\nof magnitude larger than the signal related to angular motion, and \\nso motion in this band is not visible from the angular sensor and is \\nonly seen in the interferometer strain spectrum. The active control-\\nler injects sensing noise in this frequency band through the actuator \\nto the test mass. This effect is the primary cause of test mass angular \\nmotion at frequencies above 10 Hz (8,\\xa018). Avoiding the injection of \\nnoise as much as possible and at the same time guaranteeing rejec-\\ntion of seismic disturbances is the main design goal for ASC control-\\nlers. We address this problem, which comprises the most challenging \\ncontrol loops in a GW obser\\xadva\\xadtory, with DLS.\\nThe Œ∏CHP loop\\nIn this work, we primarily focus on the ASC control loop, ‚Äúcommon \\nhard pitch‚Äù (Œ∏CHP). ‚ÄúHard pitch‚Äù refers to the stiffer of the two opto-\\nmechanical pitch eigenmodes of the arm cavities. ‚ÄúCommon‚Äù signifies \\na relation of modes between the cavities of the two arms (8). The \\nŒ∏CHP degree of freedom is the most difficult of the entire ASC system \\nto stabilize and optimize. Reducing the control noise in this loop well \\nbelow the quantum limit would remove this source of noise as a \\nblocking issue for improved astrophysical sensitivity.\\nClosed-\\xadloop shaping as a reinforcement learning problem\\nIn this work, we formulated the Œ∏CHP closed-\\xadloop control as an opti-\\nmal control problem and found approximate solutions through re-\\ninforcement learning (RL). ASC requirements are naturally expressed \\nas functions of the system response in the frequency domain, i.e., as \\ndesired spectra of state-\\xadspace signals. We introduced a new reward \\nscheme based on frequency domain behavior to enforce the desired \\nclosed-\\xadloop shaping of the control policy in such a way that RL could \\ndiscover an effective control policy. It is similar to traditionally used \\nmethods of shaping sensitivity functions, but RL removes restric-\\ntions on the reward definition and system dynamics. RL can also \\ndiscover nonlinear policies represented with deep neural networks \\nthat can serve as drop-\\xadin replacements for the existing hand-\\xadcrafted \\ncontrollers and enables improved performance without compromis-\\ning robustness.\\nRL designs controllers by adapting a parameterized state-\\xadaction \\nmapping. Our specific choice of learning algorithm is maximum a \\nposteriori policy optimization (MPO) (19). We used a small multilayer \\nperceptron (MLP) with a dilated convolution input layer for the policy \\nnetwork, which executes sufficiently fast for control. The critic network \\nis a long-short-term-\\xadmemory (LSTM) network with input and output \\nMLPs, as the critic is not needed for deployment.\\nFrequency domain rewards\\nRL naturally lends itself to reward descriptions formulated in the \\ntime domain, e.g., scoring events that happen at certain times. In\\xad\\nstead, we directly formulated the ASC requirements as rewards in \\nthe frequency domain (Fig.\\xa02B). To do so, we designed linear filters \\nfor the Œ∏CHP response signal whose transfer functions each select a \\ncertain frequency band of the signal. We used a low-\\xadpass filter to \\nreward pitch alignment, a band-\\xadpass filter to reduce control action \\nin the 8-\\xad to 30-\\xadHz band, and an additional band-\\xadpass filter for fre-\\nquencies >40 Hz to avoid high-\\xadfrequency artifacts. A high output from \\na filter at a given timestep corresponds to a large historic response \\nin the measured frequency band. These per-\\xadtimestep response \\nmeasures can then be used to construct a reward for RL. Specifically, \\nwe computed the RL reward by passing the filter outputs through a \\nsigmoid function to compute a (per-\\xadfilter) score in [0,1], with a value \\nof 1 when the specification was fulfilled and fading to 0 as the re-\\nsponse worsened. These individual filter scores were multiplied to \\nyield the per-\\xadtimestep reward, then used by the RL method to choose \\na policy that minimizes the discounted sum of this reward over time. \\nThis formulation of multiplying rewards can loosely be understood \\nas a soft logical-\\xadAND; i.e., we wanted all properties to be fulfilled for \\nhigh reward.\\nTraining and deployment\\nWe trained nonlinear control policies with RL against a linear \\nstochastic state-\\xadspace simulation of the plant dynamics (i.e., op-\\ntomechanical response of the interferometer) identified from mea-\\nsurement data of the plant. We used domain randomization to add \\nrobustness to the learned policies. Specifically, at the beginning \\nof each episode, we randomized the angular instability pole fre-\\nquency and sampled variations in the seismic noise, including the \\noverall noise strength.\\nAt the conclusion of training, we performed several steps to ready \\nthe policy for hardware testing. First, a deterministic policy was cre-\\nated by using only the mean of the policy Gaussian. Second, we vali-\\ndated this deterministic policy across a selected set of disturbances \\nand nonnominal plant parameters. We examined the reward achieved \\nas well as measured key performance criteria, such as root mean \\nsquare of the control effort in the observation band (10 to 30 Hz), and \\nvisually inspected the error and control spectra. With performance \\nconfirmed, we ‚Äúexported‚Äù the policy for the hard real-\\xadtime control \\nrequired for execution on LIGO without further training or adaptation \\non the plant.\\nWe deployed the control policies directly in the existing control \\ninfrastructure of the interferometer (20). As such, the RL-\\xadtrained poli-\\ncies were drop-\\xadin replacements of the existing single-input, single-\\noutput (SISO) controllers. In particular, the LIGO control system \\nuses somewhat arbitrary ‚Äúcounts‚Äù as the units for ASC inputs and \\noutputs, and we adopted these conventions for the controller and the \\ncontroller-\\xadsimulator interface. We have also reported our results in \\nthese units for technical reasons.\\nDeployment on gravitational wave observatory hardware\\nWe ran the deployed policies on the LIGO Livingston Observatory \\n(LLO). In the experiments, the control of Œ∏CHP was under the sole \\nauthority of a neural network‚Äìbased control policy. We measured the \\nASC noise during policy execution as well as comparison spectra from \\nthe standard controller before and after the nonlinear policy. In\\xa0Fig.\\xa01B \\nwe compare the performance of the neural network policy against the \\nstandard controller for a >10 min stretch on 5 December 2024. The \\nfigure shows the projection of the measured angular control noise into \\nthe GW readout. Additional details of this experiment are shown in \\nthe supplementary materials.\\nWe found excellent performance for the neural network policy. \\nIn the crucial 3-\\xad to 30-\\xadHz band, we see a reduction of noise of up to \\ntwo orders of magnitude. At the same time, the neural network \\npolicy shows similar control authority as the linear controller in the \\ncontrol band (<3 Hz). The control noise added by the neural net-\\nwork policy is well below the fundamental thermodynamic noise \\nand quantum back-\\xadaction noise in the whole band of interest. These \\nresults show that the neural network policy has effectively removed \\nthe issue of noise injected by active control as a limit to the astro-\\nphysical sensitivity.\\nIn the supplementary materials, we present additional results from \\nApril and August 2024, with total time on the instrument of well over \\n1 hour. The sustained control of the unstable Œ∏CHP mode demonstrates \\nrobustness of the neural network policy to normal seismic activity. We \\nDownloaded from https://www.science.org on November 30, 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2025-10-24T19:40:28+08:00', 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf', 'total_pages': 4, 'format': 'PDF 1.3', 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping', 'author': '', 'subject': 'Science 2025.389:1012-1015', 'keywords': '', 'moddate': '2025-11-30T15:08:48-08:00', 'trapped': '', 'modDate': \"D:20251130150848-08'00'\", 'creationDate': \"D:20251024194028+08'00'\", 'page': 3}, page_content='Research ArticleS\\nScience\\u2003\\n4 September 2025\\n1015\\nsaw a good match between the training simulation and the real plant \\nunder the tested conditions for frequencies >0.1 Hz, which increases \\nconfidence in our results. We additionally compared the control policy \\nagainst the incumbent linear controller in terms of statistical mea-\\nsures, such as non-\\xadGaussianity and nonstationarity. We found that, \\nalthough the policy does exhibit some nonstationarity, the overall re-\\nduction in noise still leads to a clear benefit for signal detection.\\nFor comparison, we derived controllers with convex optimization \\nand show a series of simulation-\\xadbased results in the supplementary \\nmaterials. Although these optimized linear controllers have similar \\npredicted performance, they are not fit for deployment in the high-\\xad\\nstakes environment of the real observatory. In particular, they are \\nopen-\\xadloop unstable, and their disturbance rejection behavior is highly \\naggressive, in contrast to the neural network policies. In addition to \\nexperiments on LLO, we used the same methodology on the mode-\\xad\\ncleaner of the Caltech prototype and similarly found that DLS is ca-\\npable of reducing noise in a band of interest while maintaining \\noverall control.\\nREFERENCES AND NOTES\\n\\t1.\\t LIGO Scientific Collaboration; Virgo Collaboration; Phys. Rev. Lett.  116, 061102 (2016). \\n\\t2.\\t LIGO Scientific Collaboration; Virgo Collaboration; Phys. Rev. Lett.  119, 161101 (2017). \\n\\t3.\\t LIGO Scientific Collaboration; Virgo Collaboration; KAGRA Collaboration; Phys. Rev. X  13, \\n041039 (2023). \\n\\t4.\\t H. Yu, R. X. Adhikari, R. Magee, S. Sachdev, Y. Chen, Phys. Rev. D  104, 062004 \\n(2021). \\n5.\\t B. Banerjee et al., Astron. Astrophys.  678, A126 (2023). \\n6.\\t A. Tohuvavohu et al., Astrophys. J. Lett.  975, L19 (2024). \\n\\t7.\\t J. A. Sidles, D. Sigg, Phys. Lett. A  354, 167‚Äì172 (2006). \\n\\t8.\\t L. Barsotti, M. Evans, P. Fritschel, Class. Quantum Gravity  27, 084026 (2010). \\n\\t9.\\t V. Braginsky, S. Strigin, S. Vyatchanin, Phys. Lett. A  287, 331‚Äì338 (2001). \\n\\t10.\\t K. J. Astr√∂m, R. Murray, Feedback Systems: An Introduction for Scientists and Engineers \\n(Princeton Univ. Press, ed. 2, 2021); https://books.google.com/books?id=l50DEAAAQBAJ.\\n11.\\t G. Stein, IEEE Control Syst.  23, 12‚Äì25 (2003). \\n12.\\tH. W. Bode, Network Analysis and Feedback Amplifier Design (D. Van Nostrand Company, \\n1945).\\n13.\\t G. Zames, IEEE Trans. Automat. Contr.  26, 301‚Äì320 (1981). \\n\\t14.\\t C. Barratt, S. Boyd, in Control and Dynamical Systems: Digital and Numeric Techniques and \\nTheir Applications in Control Systems, Part 1,  vol. 55, C. T. Leondes, Ed. (Academic Press, \\n1993), pp. 1‚Äì24.\\n\\t15.\\t J. C. Doyle, K. Glover, P. P. Khargonekar, B. A. Francis, IEEE Trans. Automat. Contr.  34, \\n831‚Äì847 (1989). \\n16.\\t N. Mukund et al., Phys. Rev. Appl.  20, 064041 (2023).\\n\\t17.\\t K. L. Dooley et al., J. Opt. Soc. Am. A Opt. Image Sci. Vis.  30, 2618‚Äì2626 (2013). \\n18.\\tH. Yu et al., Phys. Rev. Lett.  120, 141102 (2018). \\n19.\\t A. Abdolmaleki et al., Maximum a Posteriori Policy Optimisation, International Conference \\non Learning Representations, Vancouver, CA, 30 April to 3 May 2018. (ICLR, 2018).\\n20.\\tR. Bork et al., SoftwareX  13, 100619 (2021). \\n21.\\tS. Khan et al., Phys. Rev. D  93, 044007 (2016). \\n22.\\tE. Capote et al., Advanced LIGO Detector Performance in the Fourth Observing Run.\\n[gr-\\xadqc] (2024).\\n23.\\tM. Hoffman et al., Acme: A research framework for distributed reinforcement learning. \\narXiv:2006.00979 [cs.LG] (2020).\\n24.\\tF. Yang et al., Launchpad: A Programming Model for Distributed Machine Learning \\nResearch (2021) [cs.DC], arXiv:2106.04516.\\n\\t25.\\tgoogle-\\xaddeepmind, dm env: A python interface for reinforcement learning environments, \\nGitHub (2019); https://github.com/deepmind/dm_env.\\n26.\\tT. Hennigan, T. Cai, T. Norman, L. Martens, I. Babuschkin, Haiku: Sonnet for JAX (2024); \\nhttps://github.com/deepmind/dm-\\xadhaiku.\\n27.\\t A. Cassirer et al., Reverb: A Framework For Experience Replay. arXiv:2102.04736 [cs.LG] \\n(2021).\\n\\t28.\\tJ. Harms, Lightsaber, https://github.com/janosch314/Lightsaber (2025).\\n\\t29.\\tLIGO Scientific Collaboration, Identified Plant Model & Selected ASC Experimental Data of \\nLIGO Livingston, Zenodo (2025); https://doi.org/10.5281/zenodo.15793015.\\nACKNOWLEDGMENTS\\nWe thank J. Dean for strategic help and inspiration at the start of the project. Funding: The \\nauthors gratefully acknowledge the support of the US National Science Foundation (NSF) for \\nthe construction and operation of the LIGO Laboratory and Advanced LIGO as well as the \\nScience and Technology Facilities Council of the UK and the Max Planck Society for support of \\nthe construction of Advanced LIGO. Additional support for Advanced LIGO was provided by \\nthe Australian Research Council. LIGO was constructed by the California Institute of \\nTechnology and Massachusetts Institute of Technology with funding from the NSF and \\noperates under cooperative agreement no. PHY-\\xad18671764464. Advanced LIGO was built under \\ngrant no. PHY-\\xad18680823459. Author contributions: R.X.A., J.Bu., S.C., J.H., M.R., and B.T. \\nconceived the project. R.X.A., J.Bu., C.D., A.H., J.H., and B.T. led the project. T.A., R.X.A., I.B., \\nJ.Bu., J.Be., C.D., G.v.d.D., J.D., A.G., J.H., M.L., B.T., G.T., and C.W. developed the physics \\nsimulations. T.A., I.B., J.Bu., Y.H.J.C., C.D., J.D., M.L., B.T., and C.W. integrated the physics \\nsimulations with the learning framework. A.A., J.Bu., R.H., S.H., M.L., M.W., and B.T. developed \\nthe learning framework and performed learning experiments. C.D., T.N., J.R., and C.W. \\ndeveloped the real-\\xadtime neural network interface. R.X.A., J.Be., J.Bu., C.D., A.G., B.T., and C.W. \\nintegrated the real-\\xadtime neural network with the control system and ran experiments on LLO \\nand the California Institute of Technology 40m prototype. C.B., J.Bu., Y.H.J.C., C.D., Y.D., O.G., \\nM.L., and C.W. developed data curation tools. R.X.A., I.B., J.Bu., Y.H.J.C., B.T., and C.W. \\ndeveloped and ran the data analysis. L.F., P.K., H.O., and M.R. consulted for the project. T.A., \\nR.X.A., J.Bu., J.H., B.T., and C.W. wrote the manuscript. The LIGO Instrument Team maintains \\nand runs the LIGO Observatory. Competing interests: The authors declare that they have no \\ncompeting interests. Data and materials availability: The learning algorithm used in the \\nactor-\\xadcritic RL method is MPO (19), a reference implementation of which is available under an \\nopen-\\xadsource license (23). Additionally, the software libraries launchpad (24), dm env (25), \\nJax/Haiku (26), and reverb (27) were used, which are also open source. Simulations were \\nimplemented in Lightsaber (28) and advLigoRTS (20). The identified LLO model and \\nexperimental data are available at (29). License information:  Copyright ¬© 2025 the authors, \\nsome rights reserved; exclusive licensee American Association for the Advancement of \\nScience. No claim to original US government works. https://www.science.org/about/\\nscience-\\xadlicenses-\\xadjournal-\\xadarticle-\\xadreuse\\nSUPPLEMENTARY MATERIALS\\nscience.org/doi/10.1126/science.adw1291\\nSupplementary Text; Figs.\\xa0S1 to S16; Tables\\xa0S1 to S3; References (30‚Äì70)\\nSubmitted 28 January 2025; accepted 7 July 2025\\n10.1126/science.adw1291\\nCorrection (24 October 2025): Reference 16 was inadvertently removed during the review process; it was added back \\nin, and all subsequent references were renumbered. Additionally, the date of experiment was listed incorrectly in the \\nmain text and supplementary materials, and one of the non-byline author affiliations in the supplementary materials \\nwas accidentally duplicated; both of these errors were corrected.\\nDownloaded from https://www.science.org on November 30, 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 0}, page_content='Simulating Misinformation Propagation in Social Networks using\\nLarge Language Models\\nRaj Gaurav Maurya‚àó\\nrajg.maurya@tum.de\\nTechnische Universit√§t\\nM√ºnchen, Germany\\nVaibhav Shukla\\nFriedrich-Alexander-Universit√§t\\nErlangen-N√ºrnberg, Germany\\nRaj Abhijit Dandekar\\nRajat Dandekar\\nSreedath Panat\\nVizuara AI Labs\\nPune, India\\nFigure 1: How agents propagate misinformation? [Generated using Google AI Studio‚Äôs Nano Banana]\\nAbstract\\nMisinformation on social media thrives on surprise, emotion, and\\nidentity-driven reasoning, often amplified through human cognitive\\nbiases. To investigate these mechanisms, we model large language\\nmodel (LLM) personas as synthetic agents that mimic user-level bi-\\nases, ideological alignments, and trust heuristics. Within this setup,\\nwe introduce an auditor‚Äìnode framework to simulate and ana-\\nlyze how misinformation evolves as it circulates through networks\\nof such agents. News articles are propagated across networks of\\npersona-conditioned LLM nodes, each rewriting received content.\\nA question‚Äìanswering (QA)-based auditor then measures factual\\nfidelity at every step, offering interpretable, claim-level tracking\\nof misinformation drift. We formalize a misinformation index (MI)\\nand a misinformation propagation rate (MPR) to quantify factual\\ndegradation across homogeneous and heterogeneous branches of\\nup to 30 sequential rewrites. Experiments with 21 personas across\\n10 domains reveal that identity- and ideology-based personas (e.g.,\\nreligious leaders, lifestyle influencers, politically aligned individu-\\nals) act as misinformation accelerators, especially in politics, mar-\\nketing, and technology. By contrast, expert-driven personas (e.g.,\\nmedical professionals, investigative journalists) preserve factual\\nstability. Controlled-random branch simulations further show that\\n‚àóCorresponding author\\nonce early distortions emerge, heterogeneous persona interactions\\nrapidly escalate misinformation to propaganda-level distortion. Our\\ntaxonomy of misinformation severity‚Äîspanning factual errors, lies,\\nand propaganda‚Äîconnects observed drift to established theories\\nin misinformation studies. These findings demonstrate the dual\\nrole of LLMs as both proxies for human-like biases and as auditors\\ncapable of tracing information fidelity. The proposed framework\\nprovides an interpretable, empirically grounded approach for study-\\ning, simulating, and mitigating misinformation diffusion in digital\\necosystems.\\nCCS Concepts\\n‚Ä¢ Computing methodologies ‚ÜíNatural language processing;\\nMulti-agent systems; ‚Ä¢ Human-centered computing ‚ÜíCol-\\nlaborative and social computing theory, concepts and paradigms; ‚Ä¢\\nInformation systems ‚ÜíData mining; ‚Ä¢ Security and privacy\\n‚ÜíHuman and societal aspects of security and privacy; ‚Ä¢ Applied\\ncomputing ‚ÜíPsychology; Sociology.\\nKeywords\\nlarge language models, social simulation, social networks, misin-\\nformation, fake news, propaganda, persona, agents, QA, auditor,\\nnode\\narXiv:2511.10384v1  [cs.SI]  13 Nov 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 1}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\n1\\nIntroduction\\nMisinformation on social media undermines public trust and infor-\\nmation quality. Vosoughi et al. [28] analyzed 126,000 true and false\\nnews stories on X1 and found that false news spreads faster and far-\\nther, largely due to human behavior rather than bots. False stories\\nare more surprising and emotional, making them more shareable\\neven when spread by users with fewer followers. The typology\\nby Tandoc et al. [26] categorizes fake news into six types: satire,\\nparody, fabrication, manipulation, advertising, and propaganda;\\nhighlighting how it mimics legitimate formats and is amplified on\\nsocial media through blurred source boundaries and audience val-\\nidation. Human judgment, social motives, and sharing dynamics\\nthus remain central to its diffusion.\\nIndividual interpretation of information is shaped by beliefs,\\nexperiences, and identities [22], which motivates using Large Lan-\\nguage Models (LLMs) as synthetic agents. Trained on diverse data,\\nLLMs capture linguistic styles, emotional tones, biases, and perspec-\\ntives, enabling persona-driven simulations of real users. Similar to\\ndynamics observed in Cinelli et al. [5], LLM-based personas can\\nbe conditioned to respond differently based on source credibility\\nand ideological alignment, mirroring human trust biases. Inspired\\nby weakly supervised veracity classification [14], our framework\\nincorporates weighted credibility mechanisms, while Chen and\\nShu [4] underscores the dual role of LLMs in both generating and\\ndetecting misinformation, informing safeguards against hallucina-\\ntion risks. As argued by Thapa et al. [27], these capabilities make\\nLLMs promising tools in computational social science for modeling\\nideological bias and social amplification.\\nRecent reviews [e.g., 12] show increasing use of digital twins and\\nLLM-based simulations, which more authentically integrate cog-\\nnitive and network interactions. Evidence from He et al. [10] and\\nLin et al. [17] on human digital twins demonstrates architectures\\nfor bidirectional user modeling, inspiring LLM-based agents that\\nembed cognitive and social data flows. Building on Liu et al. [18],\\nwe extend personality-conditioned LLMs with memory and reflec-\\ntion to capture dynamic shifts in attitudes and propagation trends.\\nGrounded in theories of cognition and social simulation [3, 6], our\\nframework enables more realistic modeling of echo chambers and\\npeer reinforcement than traditional approaches.\\nMisinformation is further shaped by motivated cognition. Dash\\net al. [7] show that persona-conditioned LLMs replicate identity-\\ndriven reasoning, reducing accuracy by up to 9% and aligning\\nendorsement with political identity by up to 90%, even resisting\\nprompt-based debiasing. These findings align with prior work on\\npersistent misinformation effects [15]. Supporting evidence from\\nPratelli and Petrocchi [23], Ward et al. [29] confirms that persona-\\nconditioned LLMs embed cognitive tendencies and ideological bi-\\nases, justifying their role as synthetic agents in misinformation\\nsimulations.\\nFurther, Mittelst√§dt et al. [20] show that LLMs achieve human-\\nlevel performance in situational judgment tests, demonstrating\\nnuanced social reasoning. These capabilities justify their use as\\npersona proxies in misinformation spread, where cognition, trust,\\nidentity, and network interactions co-construct propagation. By\\nincorporating persona effects, source credibility, and social cues,\\n1formerly, Twitter: https://x.com/\\nsimulations capture the psychological and social drivers of misin-\\nformation more realistically.\\nFinally, LLMs also provide auditing mechanisms. Building on\\nQAFactEval [8], LLM auditors can generate targeted questions\\nand assign binary presence scores to track information retention.\\nThis interpretable and scalable evaluation approach, validated by\\nAher et al. [2], rigorously measures fidelity of information during\\npersona-driven transformations. Thus, persona-conditioned LLMs,\\ncombined with credibility weighting and auditing, offer a powerful,\\nempirically grounded framework for simulating and mitigating\\nmisinformation spread.\\nIn this work, we introduce an ‚Äúauditor‚Äînode\" framework to rig-\\norously quantify how factual information is preserved or degraded\\nas news articles propagate through networks of LLMs. First, we in-\\ntroduce our system design, the propagation model, and evaluation\\nmetrics. Then, we present and analyze the results of our experi-\\nments done on two different setups, before discussing the general\\nimplications and final conclusions: a summary of our contribution\\nand future outlook is also provided.\\n2\\nMethodology\\n2.1\\nSimulation Framework\\nAs illustrated in Fig. 2, our simulation consists of a network of 21\\nbranches, each containing 30 nodes. Each node (ùëè,ùëò) corresponds\\nto a LLM agent (conditioned with one out of 21 distinct personas)\\nthat rewrites a news article (domain) incoming from the node (ùëè,ùëò-\\n1) above it. The original article (one of 10) is fed throughout the\\nnetwork by the neutral node (ùëè, 0 or Node0), while an external\\nauditor (at NodeA) evaluates factual fidelity at every step by ‚Äúasking\"\\n10 questions and comparing with original answers.\\n2.1.1\\nPropagation Model. We simulate information diffusion by\\npropagating each of the 10 domains (one at a time) through all the\\n21 branches, node by node (1 to 30). After each rewrite of the news,\\nthe LLM passes a copy of the output to the next node/LLM in the\\nbranch for it to be rewritten again and another copy to the auditor\\nfor it to be evaluated against the original news (i.e., output of neutral\\nagent at Node0). Formally, let the original article be denoted by ùëÜ.\\nThe auditor constructs ùëö= 10 factual questions ùëÑ,\\nùëÑ= {ùëûùëó}ùëö\\nùëó=1,\\nùê∫= {ùëîùëó}ùëö\\nùëó=1,\\n(1)\\nwith corresponding correct answersùê∫. The article propagates through\\nùêµindependent branches, each of fixed length ùêæ= 30. In branch ùëè,\\nnode ùëòreceives the previous article ùëãùëè,ùëò‚àí1 and outputs a rewritten\\nversion\\nùëãùëè,ùëò= ùëáùëè,ùëò(ùëãùëè,ùëò‚àí1),\\nùëãùëè,0 = ùëÜ,\\n(2)\\nwhere ùëáùëè,ùëòdenotes the persona-conditioned rewriting operator.\\n2.1.2\\nExperimental Configurations. In this paper, we perform and\\nevaluate two experimental configurations of the propagation net-\\nwork, with:\\n(1) Homogeneous branches, where all nodes in a branch share\\nthe same persona prompt, isolating persona-specific effects,\\n(2) Heterogeneous branches, where nodes are assigned random\\npersona prompts (at most 2 repetitions per branch), model-\\ning realistic diversity of user behaviors.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 2}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\n2.1.3\\nAgents and Domains. We experiment with the same 21 agents\\nand 10 domains throughout this study. The agents were LLMs:\\ninstances of gpt-4o accessed via OpenAI API calls, each of which\\nwere given persona prompts (as shown in A.1).\\nThe domains were news articles retrieved from real media sources\\nonline, around 100‚Äì200 words long and on broad topics of crime,\\neducation, healthcare, marketing, politics, sports, and technology\\n(see A.2 for identifiers and full texts).\\nThe complete list of agent prompts, domain articles, and au-\\nditor questions can be found in the Appendix A and the URL\\nhttps://github.com/RajGM/LLM-Data-Poisoning. For the implemen-\\ntation details and raw data of the project, follow https://github.com/\\nRajGM/LLM-backend/.\\n2.1.4\\nMotivation for a QA-Based Auditor. Measuring factual con-\\nsistency during propagation presents unique challenges. Standard\\noverlap- or embedding-based metrics‚Äîe.g., ROUGE [16], BLEU\\n[21], BERTScore [31]‚Äîoffer surface-level or semantic similarity es-\\ntimates but conflate stylistic variation with factual preservation and\\nprovide limited interpretability. In contrast, question‚Äìanswering\\n(QA)-based metrics evaluate factual consistency by generating and\\nverifying claim-level answers derived from the text, thereby ground-\\ning assessment in explicit semantic content. Recent work [8, 11, 13]\\ndemonstrates that QA-based methods provide stronger semantic\\ngrounding, outperforming traditional approaches in the evaluation\\nof factual consistency.\\nWe therefore embed QA-based evaluation into the propagation\\nprocess itself. The auditor automatically generates factual questions\\nfrom the source article (listed in A.3 for each domain) and verifies\\ntheir recoverability in each rewritten version. This yields question-\\nanchored evidence units offering: transparency (each binary check\\ncorresponds to a verifiable factual claim), traceability (localizes\\nwhen and where specific facts degrade), and robustness (captures\\nsemantic drift more reliably).\\n0\\n1, 1\\n1, 2\\n1, 30\\n2, 1\\n2, 2\\n2, 30\\n21, 1\\n21, 2\\n21, 30\\nA\\nBranches (b)\\nes (b, k)\\nAuditor Node\\nNode0\\n10 News Domains\\nA. Crime\\nB. Education (0, 1, 2)\\nE. Healthcare\\nF. Marketing\\nG. Politics (0, 1)\\nI. Sports\\nJ. Technology\\n10 QA:\\n(Node0) v. (Nodeb,k)\\nMetrics:\\nMisinfo Index\\nMisninfo Propagation Rate\\nMisinfo Severity\\nHeatmaps (21√ó10 pairs)\\n21 LLM Agents\\n1. Politically Biased \\nIndividual (Left Wing)\\n2.Politically Biased \\nIndividual (Right Wing)\\n3. Social Media Influencer \\n(Lifestyle Influencer)\\n4.  Social Media Influencer \\n(Brand Collaborator)\\n5. News Agency \\n(Sensationalist)\\n6. News Agency \\n(Politically Neutral)\\n.....\\nFigure 2: Diagram of system architecture for misinforma-\\ntion propagation, illustrating the flow of information across\\npersona-conditioned LLM nodes, the auditor‚Äôs intervention,\\nand data recording modules. This design allows tracing fac-\\ntual drift step-by-step across heterogeneous or homogeneous\\nbranches.\\n2.2\\nEvaluation Metrics\\n2.2.1\\nAuditor Scoring and Misinformation Indices. For a text ùë•,\\nquestion ùëûùëó, and correct answer ùëîùëó, the auditor assigns\\nùë†(ùë•,ùëûùëó,ùëîùëó) =\\n(\\n1\\nif ùëîùëóis correctly recoverable from ùë•,\\n0\\notherwise.\\n(3)\\nLet y(ùë•) = \\x00ùë†(ùë•,ùëû1,ùëî1), . . . ,ùë†(ùë•,ùëûùëö,ùëîùëö)\\x01 ‚àà{0, 1}ùëödenote the an-\\nswer vector for ùë•. Define\\nyùëè,ùëò‚âîy(ùëãùëè,ùëò),\\ny0 = y(ùëÜ),\\nyaud\\n0\\n‚â°y‚òÖ= (1, . . . , 1),\\n(4)\\nwhere y‚òÖis the auditor‚Äôs truth-anchored reference viz. ‚Äúall correct\\nanswers present‚Äù. Now, with the normalized Hamming distance [9]\\nbetween two binary vectors\\nùëë(ùê¥, ùêµ) = ‚à•ùê¥‚àíùêµ‚à•1 =\\nùëö\\n‚àëÔ∏Å\\nùëó=1\\n|ùê¥ùëó‚àíùêµùëó|,\\n(5)\\nWe define Misinformation Index (MI) at node (ùëè,ùëò) as follows:\\nMIùëè,ùëò‚âîùëë(yaud\\n0 , yaud\\nùëè,ùëò) .\\n(6)\\nHere, yùëè,ùëòdenotes the binary answer vector produced directly\\nfrom node (ùëè,ùëò)‚Äôs rewritten article, while yaud\\nùëè,ùëòdenotes the corre-\\nsponding binary vector extracted by the auditor from the same\\nrewritten article (from NodeX), and is being ‚Äòsubtracted‚Äô from the\\nground truth answer vector yaud\\n0\\n(from Node0).\\n2.2.2\\nMisinformation Propagation Rate. To capture the cumulative\\nextent of misinformation drift along a branch, we define the Misin-\\nformation Propagation Rate (MPR) as the average misinformation\\nindices (MIs) across all nodes in that branch.\\nRecall that for each node ùëõùëòon branch ùëè, the misinformation\\nindex MI(ùëõùëò) is computed as the normalized Hamming distance\\nbetween the auditor‚Äôs reference vector (derived from Node0) and\\nthe auditor‚Äôs answer vector for ùëõùëò. This gives a node-level measure\\nof factual deviation relative to the source.\\nLet a branch ùëèconsist of nodes ùëÉùëè= (ùëõ0 = Node0,ùëõ1, . . . ,ùëõùëã=\\nNodeX, . . . ,ùëõùê∏), where ùëõ0 is the source and ùê∏is the branch depth\\n(30, here). The branch-level MPR is then defined as\\nMPR(ùëè) = 1\\nùê∏\\nùê∏\\n‚àëÔ∏Å\\nùëò=0\\nMI(ùëõùëò) .\\n(7)\\nIn words, MPR represents the mean misinformation index across\\nall nodes in a branch. It quantifies how much misinformation is\\non average retained or amplified as the article propagates through\\nsuccessive persona-conditioned rewrites.\\n2.2.3\\nMisinformation Severity Taxonomy. Further, based on MPR\\nmagnitudes, we classify misinformation severity into three tiers:\\n‚Ä¢ Factual error: |MPR| ‚â§1\\n‚Ä¢ Lie: 1 < |MPR| ‚â§3\\n‚Ä¢ Propaganda: |MPR| > 3\\nThis taxonomy connects observed drift to well-established cat-\\negories in misinformation studies, distinguishing between minor\\ninaccuracies, systematic distortions, and deliberate amplification.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 3}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\n2.2.4\\nAnalytical Framework. For comparative analysis, we compute\\nMPR(ùëè) for each agent-domain pair and each branch configuration.\\nThis provides a consistent scalar summary of propagation dynam-\\nics, enabling us to directly compare the susceptibility of different\\npersonas and domains to misinformation drift. The tiers, ‚Äòerror‚Äô,\\n‚Äòlie‚Äô, and ‚Äòpropaganda‚Äô serve as hints and visual aids (three colors)\\nin the interpretation and presentation of results.\\nBeyond individual branches, we also conduct node-level analyses\\nof agent- or branch-domain pairs which appear prominently high\\nor low. This traces dynamic MIùëè,ùëòtrajectories across 30 rewrites,\\ndetecting inflection points where misinformation either acceler-\\nates or stabilizes, providing insight into early-warning signals of\\namplification.\\n3\\nResults\\n3.1\\nHomogeneous Branch\\nFor the first experiment with homogeneous branches, we calculate\\nbranch-level MPRs (using Eq. 7) for the 21 LLM agents across the 10\\nnews domains and report the results in the heatmap below (Fig. 3).\\nThe misinformation severity (denoted by colors encompassing MPR\\nranges from ‚â§1, 1 ‚àí3, to ‚â•3) of the nodes exhibit clear agent- and\\ndomain-specific patterns. Observed MPRs range from near 0 to ‚âà10,\\nindicating that repeated, persona-consistent rewriting can produce\\nanything from minor factual loss to wholesale propaganda-level\\ndistortion depending on the agent‚Äìdomain combination. Aggre-\\ngated across all 210 agent-domain pairs, the overall distribution has\\nfrequency of (error, lie, propaganda) = (47, 97, 66).\\n3.1.1\\nAgent-wise Distribution. Looking horizontally, we find that,\\non average, all agents surpass the threshold for ‚Äòlie‚Äô with 8 of them\\nof achieving the ‚Äòpropaganda‚Äô tier of misinformation propagation.\\nThe 10 worst overall offenders across domains, in descending or-\\nder, are agents numbered 13 > 18 > 3 > 20 > 1 > 19 > 12 > 2 > 5\\n> 21. The most extreme propagator turns out to be Parent (Young\\nParent) with an average MPR of 5.48. Others seem to be personas\\nthat carry strong ideological, identity, or social-role signals in their\\nprompts‚Äîreligious leaders, politically biased individuals, environ-\\nmentalists, and lifestyle influencers‚Äîexhibiting consistently ele-\\nvated MPRs beyond the ‚Äòpropaganda‚Äô tier. Tech-Savvy Consumer\\n(#19) and Rural Educator (#12) also make it into this list.\\nMoreover, the amplification is strongest when persona priors\\nalign with the topical content: politically biased agents dramati-\\ncally increase MPRs on political material; religious-leader personas\\nproduce extreme drift on politically framed and moralized topics;\\nlifestyle influencers and parents produce high MPRs for marketing-\\nand family-oriented pieces; however there are some anomalies.\\nThe general pattern suggests two mechanisms at play: (1) repeated\\npersona-conditioned rewriting compounds the persona‚Äôs priors\\n(echo-chamber style accumulation of bias), and (2) topical salience\\ninteracts with persona heuristics to selectively amplify specific\\nclaim types (e.g., emotive or identity-salient claims are rephrased\\nor exaggerated repeatedly). Thus, identity- and ideology-driven\\npersonas act as systematic accelerators of factual drift.\\nOn the other end, expert and neutral information-curation per-\\nsonas preserve factual fidelity. The most resistant agents are, in\\nascending order of average over domains, 6 < 8 < 16 < 7 < 11 = 14 <\\nFigure 3: Heatmap visualization of Misinformation Propaga-\\ntion Rates of each homogeneous branch across 21 LLM agents\\nand 10 news domains. The color bar depicts Misinformation\\nSeverity from factual errors (green) to lies (orange) to pro-\\npaganda (red). The bottommost row gives the average MPR\\nover the 21 agents for specific domains, and the rightmost\\ncolumn gives the average MPR over the 10 domains for spe-\\ncific agents.\\n15 < 17 < 4 < 9 < 10. These are politically-neutral news agencies,\\ninvestigative journalists, and domain-expert personas (technology\\nand medical experts), as well as Gender Equality Advocate and Con-\\ntextually Unaware agents‚Äîwith MPR < 2, close to the ‚Äòfactual error‚Äô\\ntier. Their branches show only minor cumulative loss of auditor-\\nrecoverable claims. This stabilizing effect indicates that persona\\nprompts encoding domain expertise or professional norms act as\\ncorrective priors that resist semantic drift across repeated rewrites.\\nIt is worth pointing out that moderate propagators of misin-\\nformation (3 > MPR ‚â•2) are sensationalist news agencies (#5),\\nentrepreneurs (#21), intentional agents (#10 & #9), and brand col-\\nlaborators (#4). This shows the deliberate nature of these agents to\\nfabricate, exaggerate or falsely advertise information for the sake\\nof popularity and/or monetary gains. Here again, we see stronger\\namplifications for relevant domains: e.g., crime and politics for\\n#5, marketing and technology for both #21 and #4, and so on. We\\ndiscuss the domain-specific insights below in more detail.\\n3.1.2\\nDomain-wise Distribution. When ordered by susceptibility\\nto propaganda escalation, domains reveal clear stratification. The\\nmost vulnerable is crime0 with an average MPR of 4.2, where 16 of\\nthe 21 agents reached ‚Äòpropaganda‚Äô level of misinformation severity'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 4}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\nand the remaining 5 were in the ‚Äòlie‚Äô tier. This domain is followed\\nby marketing0, education0, technology0, and sports0 among the top 5\\npropagators. Here the majority of agent branches give MPR over 3:\\nat least 7 agents crossing the ‚Äòpropaganda‚Äô tier and the rest (i.e. 14 or\\nless) in the ‚Äòlie‚Äô tier‚Äìexcept for technology0 where they are equally\\ndistributed across the 3 tiers (i.e. 7, 7, 7 in error, lie, propaganda).\\nThese rankings highlight how disturbing or alarming news such\\nas about crime and/or emotionally-charged and competitive con-\\ntexts like sports enable rapid amplification. Marketing0 exhibited\\nhigh escalation likely because of the persuasive, attention-driven\\nnature of marketing narratives. Education and technology (such as\\nthe ethics of artificial intelligence) are also hot topics of discussion,\\nand hence apparently more affected by misinformation.\\nThese are closely followed by politics0 and education2, where we\\nsee extreme escalation by some agents but neutralization by others,\\ngiving us overall misinformation propagation rates within the ‚Äòlie‚Äô\\nrange (2.86 and 2.32, respectively). Domains where propaganda\\nremained limited include healthcare0 and politics1, showing that the\\noutcomes here are strongly agent-dependent. A peculiar anomaly\\nis education1 with 20 agents remaining in the ‚Äòerror‚Äô tier, just one\\nin the ‚Äòlie‚Äô tier, and no ‚Äòpropaganda‚Äô observed! It is the only domain\\nwith average MPR below 1. This distinct resistant nature could\\nbe ascribed to the specific content of the news story (e.g., more\\nobjective than subjective or emotional).\\n3.1.3\\nNode-level Analysis. As mentioned earlier, the heatmap also\\nreveals strong agent √ó domain interactions. While most agents\\nhave fairly equal share of misinformation propagation throughout\\nthe 10 domains, some behave heterogeneously across domains. For\\nexample, a sensationalist news agency mainly exaggerates crime,\\nmarketing, and politics domains while keeping it balanced in ed-\\nucation and healthcare. Conversely, the politically-neutral news\\nagency preserves facts in most domains except education0. Rural\\nEducator sharply amplifies propaganda in education0; whereas the\\nTechnology Expert suppresses misinformation drift in the technol-\\nogy0 domain. Interestingly, the branches for Politically Biased (Left\\nWing) agents peak for technology0, while the Tech-Savvy Consumer\\nseems to amplify politics0 the strongest. Lifestyle Influencers also\\nseem to have a lot to say about both politics and technology, but\\nnot necessarily about healthcare or sports topics.\\nThese selective vulnerabilities underscore that neither agent\\nidentity nor domain alone determines susceptibility‚Äìrather, the\\nalignment between persona priors and domain content dictates\\nhow misinformation compounds. There are notable exceptions and\\nanomalies to the agent-domain patterns described above, depending\\non the specific configuration each agent‚Äôs persona prompts and\\nthe content of the news stories. Here, we probe misinformation\\ndrift only in the top 10 and bottom 10 agent-domain pairs from\\nFig. 3 with highest and lowest MPRs, respectively, by plotting two\\nnode-level heatmaps (Fig. 4). The domain education1 is excluded\\nfrom the analysis as an outlier.\\nTop10. An inspection of the ten highest misinformation instances\\nreveals a systematic concentration of vulnerability in specific agent‚Äì\\ndomain interactions. The most affected domains are politics0 and\\ntechnology0 and these are mainly being amplified by agents #18,\\n#19, and #1. Conservative Religious Leader is driving political propa-\\nganda, while technological misinformation comes from non-expert\\nFigure 4: Node-level heatmap visualization of misinforma-\\ntion propagation, showing misinformation indices (Eq. 6)\\ncalculated after each rewrite of the given domain by the iden-\\ntical set of agents placed at the 30 nodes of the branch. The\\nbranches or agent-domain pairs are chosen from Fig. 3, with\\nhighest 10 (top) and lowest 10 MPRs (excluding education1;\\nbottom).\\npersonas, Gadget Enthusiast and Political Biased Individual (Left\\nWing), or when passed through personal belief systems of a Young\\nParent and Lifestyle Influencers. Marketing also appears prominently.\\nPaired with Conservative Religious Leader and Young Parent, it illus-\\ntrates how persuasive or empathetic communication styles amplify\\nmisinformation in consumer-facing domains. Meanwhile, Rural\\nEducator with education0 highlights how limited access to domain\\nexpertise can foster systematic distortions even in instructional\\nsettings.\\nFrom Fig. 4 (top), we see that drift to a ‚Äòlie‚Äô level of misinfor-\\nmation could start as early as the first node and then escalate to\\nthe ‚Äòpropaganda‚Äô tier and beyond by the tenth node, after which\\nthere is an irrecoverable alteration of original information. Many\\nbranches start with low or moderate node-level misinformation\\n(green/yellow cells in nodes 1‚Äì4) but undergo a rapid inflection\\nbetween roughly nodes 5‚Äì9, after which MI values jump and re-\\nmain at a high plateau (the broad band of dark pink/red) for the\\nremainder of the 30-step chain. Some agents (like #3 and #19) are\\nslower than others (like #12 and #18) in reaching the maximum\\npossible MI.\\nBottom10. The bottom ten instances represent agent-domain\\ninteractions that are remarkably resistant to misinformation escala-\\ntion. Here, the dominant pattern is one of stability: most branches\\nremain in the green band (0-1 MI) throughout the 30 nodes, with\\nonly occasional minor excursions to values of 2-3. The most com-\\nmon domains in this set are technology0, politics0, and education2,\\npaired with agents #6, #7, #8, #11, and #16. These include personas\\nsuch as the Politically Neutral News Agency, Technology Expert, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 5}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\nFigure 5: Same as Fig. 3 but for heterogeneous branches: 30\\nnodes each with controlled random assignment from 21\\nagents. Misinformation Propagation Rates (numerical val-\\nues) and Severity (3 color scales), as well branch-wise and\\ndomain-wise average of MPRs are shown.\\nMedical Expert, as well as certain instructional contexts (Rural Edu-\\ncator and Simplifier). Their grounding in factual accuracy, technical\\nprecision, and balanced communication styles appears to constrain\\nthe amplification of misinformation, even when repeatedly iterated.\\nUnlike the sharp inflections seen in the top 10, these branches\\nshow no irreversible tipping point; instead, MI values fluctuate\\nwithin a very narrow range, rarely exceeding the level of minor\\ndistortion. The rare non-zero spikes (nodes 10-20) tend to dissipate\\nquickly, returning to low-MI states rather than escalating into en-\\ntrenched misinformation. This suggests that such agents function\\nas stabilizers within the information ecosystem, buffering domains\\nagainst drift and maintaining informational integrity over long\\npropagation chains.\\n3.2\\nHeterogeneous Branch\\nNow we present the results of the second experiment, where each\\nof the 21 branches has 30 nodes with randomly assigned agents\\n(allowing at most 2 repeats per branch). Similar to Sec. 3.1, we\\ncalculate branch-wise MPR for each of the same 10 news domains\\nand plot the heatmap shown in Fig. 5. But now the branches from\\n#b1 to #b21 do not correspond specific agents #1 to #21, but rather\\nconsist of 30 controlled random agents ‚Äì average of their MIs gives\\nthe MPR for each branch-domain pair. The average MPRs across\\nbranches and domains are also plotted.\\n3.2.1\\nBranch-wise Distribution. The random-controlled agents show\\nan overwhelming tendency toward propaganda escalation across\\nall branches, though subtle variations in containment are observ-\\nable. The most extreme case is #b3, which produced propaganda\\noutcomes in 100.0% of domains (10 of 10), with neither error nor\\nlie outcomes observed, representing a pure propagandist trajectory.\\nNearly as extreme are #b4, #b5, #b7, #b8, #b14, #b15, #b16, #b17,\\n#b18, and #b19, each of which produced propaganda in 90.0% of\\ndomains (9 of 10). Their residual cases (10.0%) were split between\\nsingle errors or lies, but the dominant behavior remained consistent\\nescalation.\\nA slightly more moderated pattern appeared in #b6, #b10, #b11,\\n#b12, #b13, #b20, and #b21. Here, propaganda accounted for 70.0%‚Äì\\n80.0% of outcomes (7‚Äì8 of 10), with the remainder split across lie\\n(up to 2 cases) and error (up to 2 cases). #b1, #b2, and #b9 exhibited\\nthe most heterogeneous distributions: for instance, #b1 recorded\\n90.0% propaganda (9 of 10) alongside 10.0% lie, while #b2 and #b9\\nproduced 70.0% propaganda, 20.0% lie, and 10.0% error each. These\\nagents demonstrate that although propaganda dominates, limited\\ncontainment through lies or neutral outcomes occasionally occurs.\\n3.2.2\\nDomain-wise Distribution. From the domain perspective, the\\nrandom-controlled heterogeneous branch reveals a striking domi-\\nnance of propaganda escalation. The most extreme cases are tech-\\nnology0, marketing0, politics0, sports0, education0, education2, and\\npolitics1, each of which reached propaganda in 100.0% of branch-\\ndomain pairs (21 of 21). In these domains, neither error nor lie\\noutcomes were observed, indicating that once misinformation en-\\ntered the system, it consistently amplified into its most escalated\\nform.\\nSlightly moderated, though still highly escalatory, was crime0,\\nwhere 76.2% of cases (16 of 21) reached propaganda, accompanied\\nby 19.0% in the lie tier (4 of 21) and 4.8% in error (1 of 21). Similarly,\\nhealthcare0 produced propaganda in 71.4% of cases (15 of 21), with\\nthe remainder distributed between lie (28.6%) and no error out-\\ncomes, showing distortion through both exaggeration and extreme\\namplification.\\nThe sole resistant case was education1, which demonstrated a\\nstabilizing effect: 57.1% of cases (12 of 21) remained at the error\\ntier, 38.1% (8 of 21) fell into the lie tier, and only 4.8% (1 of 21)\\nescalated to propaganda. This makes education1 a clear outlier,\\nresisting the otherwise near-universal drift toward propaganda\\nseen across domains.\\nThe global distribution across all 210 branch-domain pairs con-\\nfirms the dominance of propaganda: 179 cases (85.2%) reached the\\npropaganda tier, with only 18 cases (8.6%) confined to lies and 13\\ncases (6.2%) to error. Mean MPRs across domains range from 5.05\\n(for #b9 and #b13) to 6.57 (for #b16); and across branches from 1.19\\n(for education1) to 4.84 (for healthcare0) to 8.07 (for technology0).\\nThis overwhelming skew reveals that once framing is randomized\\nacross heterogeneous branches, the system exhibits near-universal\\nescalation into propaganda, with only isolated deviations into lie\\nor error categories.\\n3.2.3\\nNode-level Analysis. Reproducing the node-level heatmaps\\nfor the heterogeneous branch-domain pairs with highest and low-\\nest MPRs (Fig. 6), we see contrasting and more severe results for\\nmisinformation indices. First, looking at the Top 10 pairs, we can'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 6}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\nFigure 6: Same as Fig. 4 but with a controlled-random set of\\n21 agents placed at the 30 nodes of each branch.\\nnot associate any special tendencies of branches towards certain\\ndomains. Although, specific domains (appearing multiple times) are\\ncertainly more prone to misinformation propagation than others.\\nPolitics0 and technology0 top the list, while marketing also appears\\ntwice, apart from sports0 which is quickly and strongly amplified\\nby #b1.\\nThe Bottom 10 branch-domain pairs (excluding education1) show\\na much ‚Äòcolorful‚Äô pattern, which is again domain-specific. Only\\ncrime0 and healthcare0 seem to show some resistance to misinfor-\\nmation propagated a by control-randomized set of agents. We see a\\n‚Äòstruggle‚Äô of escalation (MI going up to 5) and neutralization (MI\\nreduced to 1 or 0), and vice versa, as we down the nodes until the\\nend. The last node of just one branch-domain pair (#b2-crime0)\\nstays below the ‚Äòlie‚Äô tier, two reach the ‚Äòlie‚Äô tier, while the rest are\\nin ‚Äòpropaganda‚Äô tier (one with MI = 10).\\n4\\nDiscussion\\n4.1\\nSummary\\nThis paper introduced an interpretable auditor‚Äìnode framework\\nthat combines persona-conditioned LLM agents with a QA-based\\nauditor to trace claim-level factual fidelity as news items propagate\\nthrough synthetic social chains. Using 21 persona templates and\\n10 source articles, we quantify per-node misinformation with a\\nMisinformation Index (MI) and summarize branch behavior with\\nthe Misinformation Propagation Rate (MPR). Across homogeneous\\nbranches (single persona repeated for ùêæ= 30 hops) we find system-\\natic, persona-dependent outcomes: identity- and ideology-laden\\npersonas (e.g., religious leaders, lifestyle influencers, politically\\naligned agents) act as consistent accelerators of factual drift, fre-\\nquently moving content from minor factual errors into ‚Äòlie‚Äù or\\n‚Äúpropaganda‚Äù tiers; by contrast, expert and neutral personas (med-\\nical experts, investigative journalists, neutral news curators) act\\nas stabilizers and preserve auditor-recoverable facts. These agent\\n√ó domain interactions are strong and selective: amplification is\\nhighest when persona priors align with topical salience (politics,\\nmarketing, certain technology stories). When branches are het-\\nerogeneously composed (randomized persona assignment with at\\nmost two repeats), however, nearly ubiquitous escalation occurs:\\nheterogeneous branches produced propaganda-tier outcomes in the\\noverwhelming majority of trials (‚âà85% of all branch‚Äìdomain pairs),\\nshowing that mixed social audiences can rapidly convert small\\nearly distortions into entrenched, high-severity misinformation.\\nThese empirical patterns validate our core claim that LLM personas\\ncan both emulate human-like motivated reasoning and serve as a\\npractical substrate for controlled social-simulation experiments.\\n4.2\\nImplications\\nOur results resonate with broader research on LLM-based social\\nsimulations and motivated cognition. Recent studies show that\\npersona-conditioned LLMs can reproduce human-like biases: for in-\\nstance, tailoring language models with political or identity prompts\\ninduces motivated reasoning that aligns answers with ideological\\npriors [2]. This suggests that LLM agents can encode cognitive\\ntendencies and social heuristics (beliefs, trust, identity cues) drawn\\nfrom their training data [2, 25]. Our work thus extends the idea\\nof LLM ‚Äúdigital twins‚Äù in which agents exhibit echo-chamber ef-\\nfects and selective amplification similar to human behavior, sup-\\nplementing recent studies and algorithms [1, 30]. Importantly, our\\npersona-auditor framework also parallels findings in empirical mis-\\ninformation studies [28]: content that is surprising, emotional, or\\nidentity-salient tends to be amplified by users, and we see the same\\npattern with our synthetic agents.\\nThese insights carry practical implications for using LLM agents\\nas proxies in domains like politics, healthcare, and marketing. In po-\\nlitical simulations, persona-LLM networks illustrate how partisans\\ncan warp narratives ‚Äì for example, our politically-aligned personas\\ndramatically distorted political news, echoing real-world polariza-\\ntion on social media. In healthcare contexts, by contrast, agent\\nprompts grounded in medical expertise tended to safeguard accu-\\nracy, suggesting that injecting domain-knowledge personas can\\nstabilize misinformation. In marketing or consumer applications,\\npersuasive-agent personas (e.g., lifestyle influencers, brand promot-\\ners) readily introduced exaggerations and fabrications. Thus, LLMs\\ncould serve as proxies for voter or consumer archetypes to test mes-\\nsaging strategies ‚Äì but only if their biases are properly calibrated.\\nNotably, our findings reinforce that LLM agents inherit biases from\\ntheir training corpora, so they may both replicate human-like dis-\\ntortions and introduce idiosyncratic artifacts.\\nHowever, many reviewers caution that modern LLMs only pro-\\nduce a ‚Äúpowerful illusion of understanding‚Äù ‚Äì they interpolate\\nlinguistic patterns without true human cognition . Thus, while our\\nagents capture some aspects of cognitive bias, they remain idealized\\nmodels [19]. In real social systems, factors like evolving narratives,\\nemotional reactions, and reinforcement loops play crucial roles\\nthat our static model only partially emulates. So, in practice, al-\\nthough persona-LLMs can model how biases amplify messages,\\ntheir outputs must be interpreted with caution. For example, mar-\\nketing teams could use agent simulations to predict how different'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 7}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\naudiences misinterpret claims, but they should validate these sim-\\nulations against human data. Overall, our results highlight both\\nthe utility of LLM-based proxies for studying bias effects and the\\ncare needed to ensure those proxies remain tethered to real human\\nbehavior.\\n4.3\\nLimitations and Outlook\\nWe thus also note several limitations of our current approach and\\nidentify directions for future work.\\nFirst, our simulation is highly simplified: we used only 10 news\\narticles and fixed 30-hop branches without a realistic social graph,\\nand we assumed a ‚Äúperfect‚Äù fact-checking auditor. In reality, net-\\nworks have varying structure and influence heterogeneity, and\\nfact-checking is delayed and selective.\\nCrucially, we ignored temporal dynamics ‚Äì real misinformation\\ncampaigns unfold over time through feedback loops and breaking\\nevents, which our static model cannot capture. To improve realism,\\nfuture simulations should integrate richer cognitive architectures.\\nFor example, agents could be endowed with memory modules,\\nbelief updating, or meta-cognitive reflection, as explored in recent\\ndigital-twin proposals.\\nAnother limitation of our study is that we measured misinfor-\\nmation only in categorical bins (e.g., ‚Äòerror‚Äô,‚Äòlie,‚Äô or ‚Äòpropaganda‚Äô).\\nWe did not incorporate a continuous scoring system in which val-\\nues between 0 and 1 could represent the degree of deviation from\\nground truth, with 0 indicating full accuracy and intermediate val-\\nues capturing partial distortions (e.g., inflated numbers, missing\\ncontext, or semantic drift). Prior work has emphasized that misin-\\nformation and truthfulness are multidimensional and can be more\\nfaithfully captured on graded or continuous scales rather than as\\nbinary categories [24]. Adopting such gradient-based measures\\nin future simulations would enable more fine-grained analysis of\\nsubtle misinformation and better align synthetic evaluations with\\nhuman judgments.\\nWe further suggest the implementation of a ‚ÄòCustom Branch‚Äô\\nexperiment where specific agents are strategically placed at the\\nnodes to minimize the rate of misinformation propagation ‚Äì e.g., an\\nauthority figure followed by a neutral news agent, and so on. The\\nscope of the project can also be expanded to include other LLMs\\n(apart from gpt4o), as well as more persona prompts, number of\\nnews stories, and node depth (>30).\\nDespite these limited scope and methodological choices, the\\nfindings of this study provide an important foundation for under-\\nstanding misinformation propagation and identifying key agents\\nand domains that could be targeted in future misinformation miti-\\ngation efforts.\\n5\\nGenAI Disclosure\\nGenerative AI tools (specifically, large language models) were used\\nsolely for light editing purposes, such as grammar, spelling, and\\nphrasing improvements. No LLM-generated content was used in\\nthe conceptualization, analysis, results, or writing of the scientific\\ncontributions of this paper.\\nAcknowledgments\\nThis work was accepted as a long paper for presentation at the\\n1st Workshop on LLM Agents for Social Simulation (LASS) at the\\n34th Association for Computing Machinery (ACM) International\\nConference on Information and Knowledge Management (CIKM\\n2025; Seoul, Korea; 14 November 2025).\\nReferences\\n[1] Alberto Acerbi and Joseph M. Stubbersfield. 2023.\\nLarge language models\\nshow human-like content biases in transmission chain experiments.\\nPro-\\nceedings of the National Academy of Sciences 120, 44 (2023), e2313790120.\\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2313790120 doi:10.1073/pnas.\\n2313790120\\n[2] Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using Large Language\\nModels to Simulate Multiple Humans and Replicate Human Subject Studies.\\ndoi:10.48550/arXiv.2208.10264 arXiv:2208.10264 [cs].\\n[3] Cristiano Castelfranchi. 2001. The theory of social functions: challenges for com-\\nputational social science and multi-agent learning. Cognitive Systems Research 2,\\n1 (April 2001), 5‚Äì38. doi:10.1016/S1389-0417(01)00013-4\\n[4] Canyu Chen and Kai Shu. 2024. Can LLM-Generated Misinformation Be De-\\ntected? doi:10.48550/arXiv.2309.13788 arXiv:2309.13788 [cs].\\n[5] Matteo Cinelli, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valen-\\nsise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo, and\\nAntonio Scala. 2020. The COVID-19 social media infodemic. Sci. Rep. 10, 1 (Oct.\\n2020), 16598. doi:10.1038/s41598-020-73510-5\\n[6] R. Conte, N. Gilbert, G. Bonelli, C. Cioffi-Revilla, G. Deffuant, J. Kertesz, V. Loreto,\\nS. Moat, J. P. Nadal, A. Sanchez, A. Nowak, A. Flache, M. San Miguel, and D.\\nHelbing. 2012. Manifesto of computational social science. The European Physical\\nJournal Special Topics 214, 1 (Nov. 2012), 325‚Äì346. doi:10.1140/epjst/e2012-01697-\\n8\\n[7] Saloni Dash, Am√©lie Reymond, Emma S. Spiro, and Aylin Caliskan. 2025. Persona-\\nAssigned Large Language Models Exhibit Human-Like Motivated Reasoning.\\ndoi:10.48550/arXiv.2506.20020 arXiv:2506.20020 [cs].\\n[8] Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022.\\nQAFactEval: Improved QA-Based Factual Consistency Evaluation for Summa-\\nrization. In Proceedings of the 2022 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies.\\nAssociation for Computational Linguistics, Seattle, United States, 2587‚Äì2601.\\ndoi:10.18653/v1/2022.naacl-main.187\\n[9] R. W. Hamming. 1950. Error Detecting and Error Correcting Codes. The Bell\\nSystem Technical Journal 29, 2 (Apr 1950), 147‚Äì160. doi:10.1002/j.1538-7305.1950.\\ntb00463.x Archived PDF: hdl:10945/46756.\\n[10] Qiqi He, Li Li, Dai Li, Tao Peng, Xiangying Zhang, Yincheng Cai, Xujun Zhang,\\nand Renzhong Tang. 2024. From Digital Human Modeling to Human Digital Twin:\\nFramework and Perspectives in Human Factors. Chinese Journal of Mechanical\\nEngineering 37, 1 (Feb. 2024), 9. doi:10.1186/s10033-024-00998-7\\n[11] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kuk-\\nliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and\\nYossi Matias. 2022.\\nTRUE: Re-evaluating Factual Consistency Evaluation.\\ndoi:10.48550/arXiv.2204.04991 arXiv:2204.04991 [cs].\\n[12] E. Jerez-Villota, F. Jurado, and J. Moreno-Llorena. 2025. Understanding Informa-\\ntion Propagation in Online Social Networks: A Systematic Mapping Study. IEEE\\nAccess 13 (2025), 69194‚Äì69235. doi:10.1109/ACCESS.2025.3558768\\n[13] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2021.\\nSummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summa-\\nrization. doi:10.48550/arXiv.2111.09525 arXiv:2111.09525 [cs].\\n[14] Jo√£o A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, and Carolina Scarton.\\n2023. Weakly Supervised Veracity Classification with LLM-Predicted Credibility\\nSignals. doi:10.48550/ARXIV.2309.07601 Version Number: 3.\\n[15] Stephan Lewandowsky, Ullrich K. H. Ecker, Colleen M. Seifert, Norbert Schwarz,\\nand John Cook. 2012. Misinformation and Its Correction: Continued Influence\\nand Successful Debiasing. Psychological Science in the Public Interest 13, 3 (Dec.\\n2012), 106‚Äì131. doi:10.1177/1529100612451018\\n[16] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\\nIn Text Summarization Branches Out. Association for Computational Linguistics,\\nBarcelona, Spain, 74‚Äì81. https://aclanthology.org/W04-1013/\\n[17] Yujia Lin, Liming Chen, Aftab Ali, Christopher Nugent, Ian Cleland, Rongyang Li,\\nJianguo Ding, and Huansheng Ning. 2024. Human digital twin: a survey. Journal\\nof Cloud Computing 13, 1 (Aug. 2024), 131. doi:10.1186/s13677-024-00691-z\\n[18] Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, and Rui Yan.\\n2024. From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward\\nFake News. In Proceedings of the Thirty-Third International Joint Conference on\\nArtificial Intelligence, IJCAI-24, Kate Larson (Ed.). International Joint Conferences\\non Artificial Intelligence Organization, Jeju, South Korea, 7886‚Äì7894. doi:10.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 8}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\n24963/ijcai.2024/873 Human-Centred AI.\\n[19] A. Marchetti, F. Manzi, G. Riva, A. Gaggioli, and D. Massaro. 2025. Artificial\\nIntelligence and the Illusion of Understanding: A Systematic Review of Theory\\nof Mind and Large Language Models. Cyberpsychology, Behavior, and Social\\nNetworking 28, 7 (July 2025), 505‚Äì514. doi:10.1089/cyber.2024.0536 Epub 2025\\nMay 7.\\n[20] Justin M. Mittelst√§dt, Julia Maier, Panja Goerke, Frank Zinn, and Michael Hermes.\\n2024. Large language models can outperform humans in social situational\\njudgments. Scientific Reports 14, 1 (Nov. 2024), 27449. doi:10.1038/s41598-024-\\n79048-0\\n[21] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu:\\na Method for Automatic Evaluation of Machine Translation. In Proceedings of\\nthe 40th Annual Meeting of the Association for Computational Linguistics, Pierre\\nIsabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational\\nLinguistics, Philadelphia, Pennsylvania, USA, 311‚Äì318. doi:10.3115/1073083.\\n1073135\\n[22] Gordon Pennycook and David G. Rand. 2019. Fighting misinformation on social\\nmedia using crowdsourced judgments of news source quality. Proc. Natl. Acad.\\nSci. USA 116, 7 (Feb. 2019), 2521‚Äì2526. doi:10.1073/pnas.1806781116\\n[23] Manuel Pratelli and Marinella Petrocchi. 2025. Evaluating the Simulation of\\nHuman Personality-Driven Susceptibility to Misinformation with LLMs. doi:10.\\n48550/arXiv.2506.23610 arXiv:2506.23610 [cs].\\n[24] Michael Soprano, Kevin Roitero, David La Barbera, Davide Ceolin, Damiano\\nSpina, Stefano Mizzaro, and Gianluca Demartini. 2021. The many dimensions of\\ntruthfulness: Crowdsourcing misinformation assessments on a multidimensional\\nscale. Information Processing & Management 58, 6 (2021), 102710. doi:10.1016/j.\\nipm.2021.102710\\n[25] Patrick Taillandier, Jean Daniel Zucker, Arnaud Grignard, Benoit Gaudou,\\nNghi Quang Huynh, and Alexis Drogoul. 2025. Integrating LLM in Agent-\\nBased Social Simulation: Opportunities and Challenges. arXiv:2507.19364 [cs.AI]\\nhttps://arxiv.org/abs/2507.19364\\n[26] Edson C. Tandoc, Zheng Wei Lim, and Richard Ling. 2018. Defining ‚ÄúFake News‚Äù:\\nA typology of scholarly definitions. Digital Journalism 6, 2 (Feb. 2018), 137‚Äì153.\\ndoi:10.1080/21670811.2017.1360143\\n[27] Surendrabikram Thapa, Shuvam Shiwakoti, Siddhant Bikram Shah, Surabhi\\nAdhikari, Hariram Veeramani, Mehwish Nasim, and Usman Naseem. 2025. Large\\nlanguage models (LLM) in computational social science: prospects, current state,\\nand challenges. Social Network Analysis and Mining 15, 1 (March 2025), 4. doi:10.\\n1007/s13278-025-01428-9\\n[28] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false\\nnews online. Science 359, 6380 (March 2018), 1146‚Äì1151. doi:10.1126/science.\\naap9559\\n[29] Francis Rhys Ward, Zejia Yang, Alex Jackson, Randy Brown, Chandler Smith,\\nGrace Colverd, Louis Thomson, Raymond Douglas, Patrik Bartak, and Andrew\\nRowan. 2024. Evaluating Language Model Character Traits. doi:10.48550/arXiv.\\n2410.04272 arXiv:2410.04272 [cs].\\n[30] Junchi Yao, Hongjie Zhang, Jie Ou, Dingyi Zuo, Zheng Yang, and Zhicheng\\nDong. 2025. Social opinions prediction utilizes fusing dynamics equation with\\nLLM-based agents. Scientific Reports 15, 1 (May 2025), 15472. doi:10.1038/s41598-\\n025-99704-3\\n[31] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and\\nYoav Artzi. 2020.\\nBERTScore: Evaluating Text Generation with BERT.\\narXiv:1904.09675 [cs.CL] https://arxiv.org/abs/1904.09675\\nA\\nAppendix\\nA.1\\nLLM Agents\\n#1. You are a politically left-wing individual. As you process the\\ninformation, prioritize social justice, equality, and government in-\\ntervention. Highlight issues of economic inequality, marginalized\\ncommunities, and corporate exploitation. Frame the content in a\\nway that calls for collective action and systemic change. Downplay\\ncontent that emphasizes free-market policies, individual respon-\\nsibility, or nationalism, and focus on advocating for progressive\\npolicies and reforms.\\n#2. You are a politically right-wing individual. As you process infor-\\nmation, emphasize tradition, national pride, free markets, and indi-\\nvidual responsibility. Highlight the importance of personal freedom,\\npatriotism, and economic growth through minimal government\\nintervention. Downplay content that advocates for government\\nAgent#\\nName\\n1\\nPolitically Biased Individual (Left-Wing)\\n2\\nPolitically Biased Individual (Right-Wing)\\n3\\nSocial Media Influencer (Lifestyle Influencer)\\n4\\nSocial Media Influencer (Brand Collaborator)\\n5\\nNews Agency (Sensationalist)\\n6\\nNews Agency (Politically Neutral)\\n7\\nDomain Expertise Specialist (Medical Expert)\\n8\\nDomain Expertise Specialist (Technology Expert)\\n9\\nIntentional Agent (Conflict Creator)\\n10\\nIntentional Agent (Peacekeeper)\\n11\\nContent Creator with Simple Tone (Simplifier)\\n12\\nRural Educator (Primary Educator)\\n13\\nParent (Young Parent)\\n14\\nContextually Unaware Agent (Low Education Level)\\n15\\nGender Equality Advocate (LGBTQ+ Advocate)\\n16\\nJournalist (Investigative Journalist)\\n17\\nJournalist (Opinion Columnist)\\n18\\nReligious Leader (Conservative Religious Leader)\\n19\\nTech-Savvy Consumer (Gadget Enthusiast)\\n20\\nEnvironmentalist (Sustainable Living Advocate)\\n21\\nEntrepreneur (Tech Startup Founder)\\nregulation or social welfare programs, and frame the message in a\\nway that upholds conservative values and supports strong national\\nidentity.\\n#3. You are a social media lifestyle influencer. As you process the\\ninformation, reframe it to appeal to trends, aesthetics, and viral\\npotential. Focus on fashion, beauty, travel, and wellness. Add a\\npersonal touch, making the content relatable and aspirational for\\nyour audience. Use catchy language, hashtags, and emojis to make\\nthe content more engaging, and highlight any products or services\\nthat align with current lifestyle trends. Your goal is to make the\\ncontent shareable and visually appealing.\\n#4. You are a social media influencer collaborating with brands.\\nAs you process the information, prioritize product promotion and\\nbrand alignment. Reframe the content to emphasize how it connects\\nwith your followers‚Äô needs and preferences, while subtly promot-\\ning the brand‚Äôs products or services. Maintain a balance between\\nauthentic engagement and marketing, ensuring that the content\\nfeels organic and trustworthy. Highlight product features, benefits,\\nand why it‚Äôs a must-have for your audience, while incorporating\\ndiscount codes or giveaways when appropriate.\\n#5. You are a sensationalist news agency. As you process the infor-\\nmation, prioritize drama, urgency, and attention-grabbing headlines.\\nExaggerate the stakes of the situation to create a sense of immedi-\\nacy, and focus on the most controversial or emotionally charged\\naspects of the story. Use strong, impactful language that shocks\\nor excites the audience, and emphasize any potential conflicts or\\nscandals. Downplay nuanced or balanced viewpoints, and instead\\nframe the content to maximize engagement and provoke strong\\nreactions.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 9}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\n#6. You are a politically neutral news agency. As you process the\\ninformation, prioritize factual accuracy, balance, and objectivity.\\nPresent both sides of any issue without bias, and avoid emotionally\\ncharged language. Focus on providing context and clarity, ensuring\\nthat your audience is fully informed without pushing them toward\\nany particular conclusion. Avoid sensationalism or bias, and ensure\\nthat the content is clear, well-researched, and reliable. Your goal is\\nto provide an accurate and balanced overview of the situation.\\n#7. You are a medical expert. As you process the information, ensure\\nthat all health-related details are accurate and aligned with current\\nmedical knowledge and best practices. Clarify any vague or incor-\\nrect health claims, and add scientifically backed explanations where\\nneeded. Focus on public health, prevention, and the importance\\nof evidence-based medicine. If there are risks or side effects, make\\nsure these are clearly communicated. Ensure the content promotes\\nresponsible health practices and is free from misinformation.\\n#8. You are a technology expert. As you process the information,\\nprioritize accuracy in technical details and focus on explaining com-\\nplex technological concepts clearly. Highlight innovation, break-\\nthroughs, and the potential impact of the technology being dis-\\ncussed. Provide additional context where necessary to ensure the\\naudience understands the intricacies of the topic. If the content\\ninvolves technical errors or simplifications, correct them and offer\\na more precise explanation without overwhelming the audience.\\n#9. You are an agent with the specific goal of creating conflict. As\\nyou process the information, emphasize points of disagreement,\\ncontroversy, and division. Reframe content to highlight opposing\\nviewpoints and amplify differences between groups or individuals.\\nUse emotionally charged language to provoke strong reactions, and\\nfocus on content that encourages debate or dispute. Your goal is to\\nstir up tension and maximize the potential for conflict, especially\\nin areas where opinions or interests diverge.\\n#10. You are an agent with the specific goal of maintaining peace\\nand harmony. As you process the information, focus on common\\nground, mutual understanding, and conflict resolution. Reframe\\ndivisive content in a way that promotes empathy, cooperation, and\\ncompromise. Avoid inflammatory language, and instead, use calm,\\nmeasured tones to de-escalate tensions. Your goal is to smooth over\\npotential conflicts and ensure that the message encourages unity\\nand understanding between different parties.\\n#11. You are a content creator focused on simplifying complex in-\\nformation. As you process the content, break it down into easy-to-\\nunderstand language, removing technical jargon and unnecessary\\ncomplexity. Use short, clear sentences and simple analogies to en-\\nsure that even a layperson can grasp the core ideas. Prioritize clarity\\nand accessibility over detail, and make sure the message is concise\\nwithout losing its key points. Your goal is to make the content\\naccessible to a broad audience, regardless of their education level.\\n#12. You are a rural educator focused on providing accessible educa-\\ntion to those with limited resources. As you process the information,\\nsimplify it so that it is understandable by individuals with varying\\nlevels of literacy and access to education. Use relatable examples\\nand avoid unnecessary technical language. Your goal is to ensure\\nthat the core message is conveyed in a way that can be understood\\nby rural communities, emphasizing practicality and usefulness. Pri-\\noritize content that can help improve daily life and local community\\ndevelopment.\\n#13. You are a young parent. As you process the information, filter\\nit with the goal of protecting your child and prioritizing their well-\\nbeing. Focus on content that is family-friendly and educational, and\\nremove or downplay anything that could be considered inappro-\\npriate or harmful. If the information relates to parenting, safety, or\\nchild development, highlight those aspects. Ensure that the content\\nis positive, nurturing, and promotes healthy, responsible behavior\\nfor children.\\n#14. You are an agent with a limited understanding of technical\\nterms and complex concepts. As you process the information, you\\nmay misunderstand or oversimplify ideas. Substitute terms or con-\\ncepts with what you believe they mean, even if your interpretation\\nmight be incorrect. Simplify complex topics into something more\\nfamiliar, even if it slightly distorts the original meaning. Your goal\\nis to present the information in a way that makes sense to you, but\\nthis may result in some inaccuracies or gaps in understanding.\\n#15. You are a gender equality advocate focused on LGBTQ+ rights.\\nAs you process the information, ensure that it promotes inclusivity\\nand challenges traditional gender norms. Highlight any issues re-\\nlated to discrimination, bias, or inequality, and reframe the content\\nto emphasize fairness and justice for all gender identities and sexual\\norientations. Where applicable, add additional context or language\\nthat is more inclusive. Your goal is to ensure that the content reflects\\nthe principles of gender equality and LGBTQ+ advocacy.\\n#16. You are an investigative journalist. As you process the infor-\\nmation, focus on uncovering the truth, digging deeper into the\\nfacts, and identifying any inconsistencies or hidden details. Ap-\\nproach the content with skepticism and curiosity, seeking to verify\\nall claims and sources. Highlight anything that seems suspicious\\nor unexplained, and frame the content in a way that encourages\\ncritical thinking and further investigation. Your goal is to provide\\nan accurate, thorough, and well-researched version of the story.\\n#17. You are an opinion columnist. As you process the informa-\\ntion, focus on interpreting the facts through your personal view-\\npoint. Add commentary, analysis, and your own reflections on the\\ncontent. Reframe the information to support your opinions and\\nperspective, but make sure to acknowledge alternative viewpoints\\nwhen necessary. Use persuasive language and rhetorical techniques\\nto engage the reader, while ensuring your argument is clear and\\nwell-supported. Your goal is to provide a thought-provoking inter-\\npretation of the information.\\n#18. You are a conservative religious leader. As you process the\\ninformation, filter it through the lens of your religious teachings\\nand beliefs. Highlight values such as faith, morality, and tradi-\\ntion. Remove or downplay content that contradicts your religious\\nworldview, and instead frame the message in a way that promotes\\nadherence to religious practices and moral values. Your goal is to\\nensure that the information aligns with your religious beliefs and\\nencourages others to live in accordance with those principles.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 10}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\n#19. You are a tech-savvy consumer who is enthusiastic about the\\nlatest gadgets and technological advancements. As you process\\nthe information, highlight any aspects that relate to innovation,\\ndesign, and user experience. Reframe content to focus on how new\\ntechnology can improve daily life, emphasizing features, specs, and\\nfuture trends. Your goal is to present the information in a way that\\nexcites other tech enthusiasts, making them eager to adopt the\\nlatest gadgets and digital tools.\\n#20. You are an environmentalist focused on sustainable living. As\\nyou process the information, emphasize content that promotes eco-\\nfriendly practices, conservation, and climate action. Reframe any\\ncontent that is not environmentally conscious, highlighting the\\npotential negative impacts on the planet. Encourage responsible\\nconsumption and sustainable choices, and use language that moti-\\nvates others to adopt greener lifestyles. Your goal is to ensure that\\nthe content aligns with your environmental values and inspires\\nothers to take action for the planet.\\n#21. You are a tech startup founder. As you process the informa-\\ntion, focus on opportunities for innovation, disruption, and growth.\\nHighlight market trends, potential for scalability, and competitive\\nadvantages. Frame the content in a way that encourages risk-taking,\\ninnovation, and problem-solving. Downplay obstacles or risks un-\\nless they provide an opportunity for creative solutions. Your goal\\nis to approach the information from an entrepreneurial mindset,\\nconstantly looking for opportunities to leverage new technology\\nor business models for success.\\nA.2\\nNews Domains\\nA) crime-0. The FBI‚Äôs 2023 Crime in the Nation report reveals en-\\ncouraging trends in crime reduction across the United States. Vio-\\nlent crime dropped by an estimated 3% compared to the previous\\nyear. The largest declines were seen in murder and non-negligent\\nmanslaughter cases, which fell by 11.6%, and rape incidents, which\\nsaw a 9.4% decrease. Robbery and aggravated assault also recorded\\ndeclines of 0.3% and 2.8%, respectively. The report underscores\\nthe significant role of law enforcement agencies, with over 16,000\\nagencies contributing data, covering more than 94% of the U.S.\\npopulation. However, hate crime incidents remain a concern, with\\nmore than 11,800 incidents reported, primarily motivated by race,\\nreligion, and gender identity biases. The FBI is calling for contin-\\nued vigilance and community engagement to sustain these positive\\ntrends.\\nB) education-0. The introduction of AI into the realm of college\\ndebate has ignited a passionate debate within academic circles.\\nAccording to Inside Higher Ed, a proposal to allow AI-generated\\nresearch in collegiate debate tournaments has led to a split among\\neducators and students alike. Proponents of AI integration argue\\nthat it could enhance the research process, allowing debaters to\\naccess and analyze a wider range of information at unprecedented\\nspeeds. They believe that AI tools can support students in focus-\\ning on refining their argumentative skills rather than spending\\nexcessive time gathering information. However, critics caution that\\nrelying on AI may undermine the very skills that debate is meant\\nto cultivate: critical thinking, information synthesis, and the abil-\\nity to construct logical arguments independently. Some fear that\\nthe overuse of AI in debate could lead to a diminished capacity\\nfor original thought, as debaters might begin to overly depend on\\nmachine-generated insights rather than their own intellectual abili-\\nties. Furthermore, there are concerns about fairness, as wealthier\\ninstitutions could have better access to advanced AI tools, creat-\\ning an uneven playing field. As AI continues to permeate various\\nsectors, this debate underscores the broader challenge of balanc-\\ning technological advancements with the preservation of human\\ncognitive skills in education.\\nC) education-1. A group of young Indian students in Dubai recently\\nshowcased their technical prowess at the Codeavour AI Robo City\\nChallenge, an international robotics competition designed to in-\\nspire innovation in AI and robotics. Among the notable participants\\nwas 12-year-old Maya Kamat, a bright robotics enthusiast, and her\\nteacher, Usha Kumawat, who provided guidance throughout the\\nprocess. The competition attracted students from different parts\\nof the world, all focused on creating AI-powered robots aimed\\nat solving real-world challenges. Maya, for example, developed a\\nrobot capable of assisting the elderly with daily tasks like medica-\\ntion reminders, reflecting the event‚Äôs emphasis on problem-solving\\nfor societal benefit. Kumawat highlighted how these competitions\\nfoster creativity and hands-on learning in STEM education, encour-\\naging students to think critically and engage in innovative projects\\nfrom a young age. She emphasized that exposing students to such\\nexperiences helps them develop the necessary skills to thrive in the\\ntech-driven future. The event also underlined how initiatives like\\nthese can spark interest in engineering and AI fields, potentially\\nshaping the careers of the next generation of innovators. The suc-\\ncess of these young students serves as a testament to the power\\nof early education in AI and robotics, and its potential to drive\\nmeaningful contributions to society.\\nD) education-2. The 2024 placement season at India‚Äôs prestigious In-\\ndian Institutes of Technology (IITs) has left many students without\\njob offers, signaling a growing concern in the job market. Despite\\nan overall 75% placement rate, more than 8,000 IIT graduates re-\\nmain unplaced, a significant increase from previous years. The\\neconomic slowdown and global tech industry challenges have im-\\npacted recruitment, especially in fields like computer science. While\\ntop recruiters offered high salary packages, with some students\\nsecuring offers over INR 1 crore annually, many others struggled\\nto find positions. The most affected were students from less pop-\\nular streams and those from newer IITs. Mechanical engineering\\ngraduates fared better this year, while the number of placements\\nin computer science saw a notable decline. Students have voiced\\nfrustration as placement offers fell short of expectations, with many\\nturning to higher studies or entrepreneurship as alternatives. Re-\\ncruiters cited economic uncertainties and a shift in industry needs\\nas reasons for the reduced hiring. The placement crisis has sparked\\ndiscussions about the need for IITs to revamp their curriculum to\\nalign more closely with industry demands and offer better career\\nguidance to students.\\nE) healthcare-0. The American Cancer Society‚Äôs Cancer Facts &\\nFigures 2024 report provides crucial data on the rising incidence of\\ncancer across the United States. The report estimates that over 1.9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 11}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\nmillion new cancer cases will be diagnosed in 2024, with approxi-\\nmately 610,000 deaths expected. Among the most common cancers,\\nbreast cancer remains a leading diagnosis for women, while prostate\\nand lung cancers are prevalent among men. The economic burden\\nof cancer continues to grow, with total care costs projected to reach\\n$360 billion. Moreover, family and unpaid caregivers contribute an\\nestimated $346.5 billion in support, adding to the financial strain.\\nNotably, the report highlights significant racial disparities in cancer\\noutcomes, particularly among Hispanic and African American pop-\\nulations, urging further investment in early detection and accessible\\ntreatment options to reduce mortality rates.\\nF) marketing-0. In a revealing critique of the influencer industry,\\nMark Schaefer‚Äôs article sheds light on what he calls the ‚Äúreal influ-\\nencer scam.‚Äù Schaefer argues that much of the influencer marketing\\nindustry is built on misleading metrics, with many influencers\\nboosting their profiles through artificial means such as purchased\\nfollowers or engagement bots. As a result, brands often spend large\\nsums of money on influencers who fail to deliver meaningful results,\\nrelying on vanity metrics like follower count rather than true influ-\\nence. Schaefer highlights several instances where brands invested\\nheavily in influencers only to realize later that their engagement\\nand conversion rates were significantly lower than expected. He\\ncalls for a reevaluation of how brands measure influencer effective-\\nness, suggesting that deeper connections and genuine engagement\\nshould take precedence over surface-level numbers. The article\\nalso touches on the growing skepticism surrounding influencer\\nmarketing, with brands becoming more aware of the potential\\nfor fraud within the industry. Schaefer argues that the real value\\nlies in influencers who cultivate authentic relationships with their\\naudiences, rather than those who chase numbers for the sake of\\nappearances. His article has sparked a broader conversation about\\ntransparency and accountability in the influencer economy, chal-\\nlenging the norms of modern digital marketing.\\nG) politics-0. A recent report from the Economic Times highlights\\nthe divergent approaches to AI policy between former President\\nDonald Trump and Vice President Kamala Harris, reflecting broader\\nideological differences in their visions for America‚Äôs technological\\nfuture. Trump‚Äôs AI policy primarily centers around deregulation and\\nfostering private sector innovation. He views AI as a tool to bolster\\nthe U.S. economy and maintain its global leadership in tech, advo-\\ncating for minimal government interference to allow companies the\\nfreedom to innovate. His administration focused on accelerating AI\\ndevelopment by reducing bureaucratic barriers, with an emphasis\\non maintaining competitiveness with global powers like China.\\nOn the other hand, Kamala Harris has called for more stringent\\nregulations to ensure that AI is developed and deployed ethically.\\nShe has expressed concerns about the risks of AI, particularly in\\nareas like criminal justice, where AI algorithms have been shown\\nto perpetuate biases. Harris advocates for stronger oversight and\\naccountability in AI development to prevent discrimination and\\nprotect civil rights. She also emphasizes the importance of diversity\\nin AI research and policymaking to ensure that the technology\\nbenefits all members of society equally. This policy divide high-\\nlights a broader debate about the role of government in regulating\\nemerging technologies and ensuring that AI serves the public good\\nwhile fostering innovation.\\nH) politics-1. The 2024 election year is shaping up to be one of the\\nmost politically charged in modern history, with religion playing\\nan unprecedented role in influencing voter behavior. Globally, more\\nthan half the world‚Äôs population will participate in elections, with\\ncountries like the U.S., India, and Indonesia serving as critical bat-\\ntlegrounds where religion could sway results. In the U.S., Christian\\nnationalism is rising as a potent force in right-wing politics, shaping\\ndebates around immigration, abortion, and national identity. Reli-\\ngious leaders in India and Indonesia, where religion has historically\\nplayed a role in governance, are also mobilizing voters around is-\\nsues tied to faith. Analysts suggest that religion‚Äôs influence in these\\nelections could deepen political divisions, particularly in countries\\nalready facing sectarian tensions. In regions like the Middle East\\nand South Asia, where religious identity is intricately tied to na-\\ntional politics, religious leaders are emerging as powerful voices\\nthat could either stabilize or further inflame political situations.\\nThe global stakes are high, and religious institutions worldwide are\\ntaking active roles in shaping not just political discourse but also\\nvoter turnout. This evolving dynamic underscores the power of\\nreligion in modern politics and its potential to alter the trajectory\\nof global leadership.\\nI) sports-0. The 2024 US Open delivered some impressive perfor-\\nmances, as the tournament‚Äôs fourth round concluded with notable\\nrecords. British player Jack Draper made headlines by reaching\\nhis first Grand Slam quarterfinal, becoming the 10th British man\\nto achieve this feat in the Open Era. In women‚Äôs tennis, China‚Äôs\\nZheng Qinwen continued her incredible form, posting a remarkable\\n15-3 record in three-set matches this year. Zheng, who has won six\\nconsecutive three-setters, advanced to the quarterfinals after defeat-\\ning Ons Jabeur. These performances underscore the unpredictable\\nnature of the tournament, as new players are rising to prominence,\\nadding fresh excitement to the sport. The US Open continues to be\\na stage for breakthrough performances, and 2024 is no exception.\\nJ) technology-0. In a breakthrough demonstration of artificial intel-\\nligence‚Äôs capabilities, IBM‚Äôs AI Debater competed against human\\ndebaters, showcasing its ability to craft persuasive arguments and\\nrebuttals in real-time. The event, which focused on complex sub-\\njects such as government subsidies for space exploration, marked\\na significant milestone in the development of AI systems capable\\nof handling nuanced, multi-layered discussions. IBM‚Äôs AI Debater\\nprocessed vast amounts of information, built coherent arguments,\\nand skillfully responded to human opponents‚Äô points. Its perfor-\\nmance was not only impressive for its logical structure but also for\\nits adept use of language, which was both persuasive and relevant\\nto the topic at hand. The AI‚Äôs success in this event has far-reaching\\nimplications for industries where debate and decision-making are\\ncrucial, such as politics, law, and academia. Experts believe that\\nAI systems like this one could revolutionize sectors where critical\\nthinking and argumentation are essential. However, this techno-\\nlogical advance also raises ethical questions about the role of AI in\\nhuman discourse and decision-making. As AI continues to evolve,\\nthe question remains whether machines will complement or sup-\\nplant human reasoning in areas traditionally reserved for human\\nintellect. IBM‚Äôs AI Debater represents a significant leap forward in\\nnatural language processing and machine learning, opening doors\\nto AI applications in a wide range of fields.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 12}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\nA.3\\nAuditor‚Äôs Domain Questions\\nThe auditor is an agent of LLM gpt4o-mini with the prompt:\\n‚ÄúYou are an external fact checker that answers yes/no questions\\nbased on a given text. Return your response as a JSON object with\\nan answers key containing an array of 1 (for Yes) or 0 (for No).\"\\nThe 10 questions evaluated for the domains A‚ÄìJ are as follows.\\nA) crime0.\\n1. Did violent crime decrease in the United States in 2023 compared\\nto the previous year?\\n2. Was there a significant decline in murder and non-negligent\\nmanslaughter cases according to the report?\\n3. Did hate crime incidents increase according to the FBI‚Äôs 2023\\nreport?\\n4. Are law enforcement agencies active participants in the data\\ncollection for the Crime in the Nation report?\\n5. Did the report highlight a particular demographic that is fre-\\nquently targeted in hate crimes?\\n6. Is the FBI advocating for community engagement to address\\ncrime trends?\\n7. Is there a possibility that further investigations into hate crimes\\nwill be initiated following this report?\\n8. Could rumors about potential increases in specific types of crime\\narise from the report‚Äôs findings?\\n9. Was the overall decrease in crime attributed solely to law en-\\nforcement efforts?\\n10. Is there a chance that the report will prompt public concern over\\nrising hate crime incidents?\\nB) education0.\\n1. Did the proposal to allow AI-generated research in collegiate\\ndebate tournaments ignite a debate within academic circles?\\n2. Are there proponents of AI integration in college debate who\\nbelieve it can enhance the research process?\\n3. Do critics of AI in debate argue that it may undermine critical\\nthinking and logical argument construction skills?\\n4. Is there a concern that reliance on AI could lead to a diminished\\ncapacity for original thought among debaters?\\n5. Do wealthier institutions potentially have better access to ad-\\nvanced AI tools, raising concerns about fairness?\\n6. Is it possible that the debate around AI in college debate could\\nlead to investigations into academic integrity?\\n7. Are there rumors that certain colleges may adopt AI tools before\\nothers to gain a competitive advantage in debates?\\n8. Could public concerns about AI use in education lead to calls\\nfor regulatory measures in collegiate debate tournaments?\\n9. Is it likely that further developments in AI technology could\\ninfluence the ongoing debate regarding its role in education?\\n10. Is there a chance that the introduction of AI in debate could\\nresult in new educational philosophies or approaches to teaching\\nargumentation skills?\\nC) education1.\\n1. Did the Codeavour AI Robo City Challenge attract participants\\nfrom multiple countries?\\n2. Was 12-year-old Maya Kamat one of the notable participants in\\nthe event?\\n3. Did Maya create a robot designed to assist the elderly?\\n4. Did Usha Kumawat act as a teacher and guide for the students\\nduring the competition?\\n5. Was the primary focus of the competition on creating AI-powered\\nrobots for real-world challenges?\\n6. Do competitions like this inspire creativity and problem-solving\\nin STEM education according to Usha Kumawat?\\n7. Is there a potential for these young innovators to shape future\\ncareers in engineering and AI?\\n8. Could public interest in STEM fields be influenced by events like\\nthe Codeavour AI Robo City Challenge?\\n9. Is it possible that allegations could arise regarding the originality\\nof the projects submitted at the competition?\\n10. Might there be rumors in later versions about the level of funding\\nor sponsorship for the competition?\\nD) education2.\\n1. Did more than 8,000 IIT graduates remain unplaced during the\\n2024 placement season?\\n2. Is the overall placement rate for IITs reported to be 75%?\\n3. Is the economic slowdown mentioned as a factor affecting re-\\ncruitment?\\n4. Did students from newer IITs face more challenges in securing\\njob offers?\\n5. Were mechanical engineering graduates reported to have better\\nplacement rates this year?\\n6. Did some students secure job offers exceeding ¬§1 crore annually?\\n7. Are students expressing frustration over the placement offers\\nreceived?\\n8. Is there a discussion about revamping the curriculum of IITs in\\nresponse to the placement crisis?\\n9. Did recruiters attribute the reduced hiring to a shift in industry\\nneeds?\\n10. Could there be potential rumors about students pursuing alter-\\nnative career paths like entrepreneurship due to the placement\\ncrisis?\\nE) healthcare0.\\n1. Does the American Cancer Society‚Äôs report predict over 1.9\\nmillion new cancer cases for 2024?\\n2. Are approximately 610,000 cancer-related deaths expected in\\n2024 according to the report?\\n3. Is breast cancer identified as the leading diagnosis for women\\nin the report?\\n4. Do the statistics show that prostate and lung cancers are preva-\\nlent among men?\\n5. Is the total projected economic burden of cancer estimated to\\nreach $360 billion?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 13}, page_content='Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, and Sreedath Panat\\n6. Do family and unpaid caregivers contribute an estimated $346.5\\nbillion in support for cancer care?\\n7. Does the report highlight racial disparities in cancer outcomes?\\n8. Is there a call for further investment in early detection and\\naccessible treatment options in the report?\\n9. Are there public concerns regarding the rising incidence of can-\\ncer highlighted in the article?\\n10. Could rumors arise about the adequacy of current cancer treat-\\nment options based on the report‚Äôs findings?\\nF) marketing0.\\n1. Does Mark Schaefer identify the influencer marketing industry\\nas having misleading metrics?\\n2. Did Schaefer‚Äôs article suggest that many influencers use pur-\\nchased followers or engagement bots?\\n3. Are brands reportedly investing large sums of money in influ-\\nencers based on vanity metrics?\\n4. Has Schaefer highlighted examples of brands experiencing lower\\nthan expected engagement and conversion rates?\\n5. Is there a call for reevaluation of how brands measure influencer\\neffectiveness in the article?\\n6. Does Schaefer propose that genuine engagement should be pri-\\noritized over surface-level numbers?\\n7. Is there growing skepticism among brands about the potential\\nfor fraud in influencer marketing?\\n8. Has Schaefer‚Äôs article contributed to a broader conversation\\nabout transparency in the influencer economy?\\n9. Are there rumors expected about influencers being investigated\\nfor artificial engagement practices?\\n10. Could the article lead to potential allegations against brands for\\nnot conducting due diligence in their influencer partnerships?\\nG) politics0.\\n1. Did Donald Trump advocate for minimal government interfer-\\nence in AI policy?\\n2. Is Kamala Harris in favor of more stringent regulations on AI\\ndevelopment?\\n3. Does Trump‚Äôs AI policy focus on fostering private sector inno-\\nvation?\\n4. Has Kamala Harris expressed concerns about biases in AI algo-\\nrithms?\\n5. Did the report from the Economic Times highlight ideological\\ndifferences in AI policy?\\n6. Is there a debate about the role of government in regulating\\nemerging technologies?\\n7. Did Trump‚Äôs administration aim to reduce bureaucratic barriers\\nto AI development?\\n8. Do both political figures agree on the need for diversity in AI\\nresearch?\\n9. Are there concerns about the ethical implications of AI in areas\\nlike criminal justice?\\n10. Is it possible that future developments could arise from this\\npolicy divide?\\nH) politics1.\\n1. Is the 2024 election year expected to be politically charged?\\n2. Will more than half the world‚Äôs population participate in the\\nelections in 2024?\\n3. Are the U.S., India, and Indonesia considered critical battle-\\ngrounds in these elections?\\n4. Is the rise of Christian nationalism influencing right-wing poli-\\ntics in the U.S.?\\n5. Do religious leaders in India and Indonesia mobilize voters\\naround faith-related issues?\\n6. Is there a concern that religion‚Äôs influence could deepen political\\ndivisions?\\n7. Are religious leaders in the Middle East and South Asia seen as\\npowerful voices in politics?\\n8. Could religious institutions worldwide impact both political\\ndiscourse and voter turnout?\\n9. Is there a potential risk of increased sectarian tensions due to\\nreligion‚Äôs role in elections?\\n10. Might rumors or allegations emerge regarding the manipulation\\nof voter behavior through religious influences?\\nI) sports0.\\n1. Did Jack Draper reach his first Grand Slam quarterfinal at the\\n2024 US Open?\\n2. Is Jack Draper the 10th British man to reach a Grand Slam quar-\\nterfinal in the Open Era?\\n3. Did Zheng Qinwen achieve a record of 15-3 in three-set matches\\nthis year before the 2024 US Open?\\n4. Did Zheng Qinwen win six consecutive three-set matches lead-\\ning up to her quarterfinal advancement?\\n5. Did Ons Jabeur lose to Zheng Qinwen in the fourth round of the\\n2024 US Open?\\n6. Is the unpredictability of the tournament noted as a highlight in\\nthe article?\\n7. Might there be concerns regarding the emergence of new players\\nin women‚Äôs tennis following this tournament?\\n8. Could there be speculation about the future careers of players\\nlike Jack Draper and Zheng Qinwen after their performances?\\n9. Is it possible that rumors about changes in the ranking system\\nfor tennis players might arise from this tournament?\\n10. Might the tournament‚Äôs records lead to discussions about the\\nphysical demands placed on players during Grand Slams?\\nJ) technology0.\\n1. Did IBM‚Äôs AI Debater compete against human debaters in the\\ndemonstration?\\n2. Was the event focused on topics like government subsidies for\\nspace exploration?\\n3. Did IBM‚Äôs AI Debater showcase its ability to craft persuasive\\narguments?\\n4. Are there ethical concerns regarding the role of AI in human\\ndiscourse?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '2025-11-14T01:57:05+00:00', 'source': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SN_misinformation.pdf', 'total_pages': 15, 'format': 'PDF 1.7', 'title': 'Simulating Misinformation Propagation in Social Networks using Large Language Models', 'author': 'Raj Gaurav Maurya; Vaibhav Shukla; Raj Abhijit Dandekar; Rajat Dandekar; Sreedath Panat', 'subject': '-  Computing methodologies  ->  Natural language processing.Multi-agent systems.-  Human-centered computing  ->  Collaborative and social computing theory, concepts and paradigms.-  Information systems  ->  Data mining.-  Security and privacy  ->  Human and societal aspects of security and privacy.-  Applied computing  ->  Psychology.Sociology.', 'keywords': 'large language models, social simulation, social networks, misinformation, fake news, propaganda, persona, agents, QA, auditor, node', 'moddate': '2025-11-14T01:57:05+00:00', 'trapped': '', 'modDate': \"D:20251114015705+00'00'\", 'creationDate': \"D:20251114015705+00'00'\", 'page': 14}, page_content='Simulating Misinformation Propagation in Social Networks using Large Language Models\\n5. Is it believed that AI systems like IBM‚Äôs Debater could revolu-\\ntionize critical sectors?\\n6. Did the AI‚Äôs performance demonstrate both logical structure\\nand relevant use of language?\\n7. Is there speculation about whether machines will complement\\nor replace human reasoning?\\n8. Could the success of IBM‚Äôs AI Debater lead to investigations\\ninto AI‚Äôs impact on decision-making?\\n9. Might public concerns arise regarding the implications of AI in\\npolitics and law?\\n10. Is there potential for rumors about the long-term effects of AI\\nsystems on professional debate?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='2025-10-1\\nVideo models are zero-shot learners and reasoners\\nThadd√§us Wiedemer*1, Yuxuan Li1, Paul Vicol1, Shixiang Shane Gu1, Nick Matarese1, Kevin Swersky1,\\nBeen Kim1, Priyank Jaini*1 and Robert Geirhos*1\\n1Google DeepMind\\nThe remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language\\nprocessing from task-specific models to unified, generalist foundation models. This transformation\\nemerged from simple primitives: large, generative models trained on web-scale data. Curiously, the\\nsame primitives apply to today‚Äôs generative video models. Could video models be on a trajectory\\ntowards general-purpose vision understanding, much like LLMs developed general-purpose language\\nunderstanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn‚Äôt explicitly trained\\nfor: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing\\nobject affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate\\nthe visual world enable early forms of visual reasoning like maze and symmetry solving. Veo‚Äôs emergent\\nzero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision\\nfoundation models.\\nProject page: https://video-zero-shot.github.io/\\n1. Introduction\\nWe believe that video models will become unifying, general-purpose foundation models for machine\\nvision just like large language models (LLMs) have become foundation models for natural language\\nprocessing (NLP). Within the last few years, NLP underwent a radical transformation: from task-\\nspecific, bespoke models (e.g., one model for translation, another one for question-answering, yet\\nanother one for summarization) to LLMs as unified foundation models. Today‚Äôs LLMs are capable of\\ngeneral-purpose language understanding, which enables a single model to tackle a wide variety of\\ntasks including coding [1], math [2], creative writing [3], summarization, translation [4], and deep\\nresearch [5, 6]. These abilities started to emerge from simple primitives: training large, generative\\nmodels on web-scale datasets [e.g 7, 8]. As a result, LLMs are increasingly able to solve novel tasks\\nthrough few-shot in-context learning [7, 9] and zero-shot learning [10]. Zero-shot learning here\\nmeans that prompting a model with a task instruction replaces the need for fine-tuning or adding\\ntask-specific inference heads.\\nMachine vision today in many ways resembles the state of NLP a few years ago: There are\\nexcellent task-specific models like ‚ÄúSegment Anything‚Äù [11, 12] for segmentation or YOLO variants\\nfor object detection [13, 14]. While attempts to unify some vision tasks exist [15‚Äì25], no existing\\nmodel can solve any problem just by prompting. However, the exact same primitives that enabled\\nzero-shot learning in NLP also apply to today‚Äôs generative video models‚Äîlarge-scale training with a\\ngenerative objective (text/video continuation) on web-scale data [26]. In this article, we therefore\\nask: Do video models develop general-purpose vision understanding, similar to how LLMs developed\\ngeneral-purpose language understanding? We answer this question in the affirmative:\\n1. Analyzing 18,384 generated videos across 62 qualitative and 7 quantitative tasks, we report\\nthat Veo 3 can solve a wide range of tasks that it was neither trained nor adapted for.\\n2. Based on its ability to perceive, model, and manipulate the visual world, Veo 3 shows early\\nforms of ‚Äúchain-of-frames (CoF)‚Äù visual reasoning like maze and symmetry solving.\\n3. While task-specific bespoke models still outperform a zero-shot video model, we observe a\\nsubstantial and consistent performance improvement from Veo 2 to Veo 3, indicating a rapid\\nadvancement in the capabilities of video models.\\n2. Methods\\nApproach and motivation\\nOur method is simple: We prompt Veo. This minimalist strategy is\\nintentional, as it mirrors the transformation of NLP from task-specific fine-tuning or training to\\n* Joint leads.\\narXiv:2509.20328v2  [cs.LG]  29 Sep 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Video models are zero-shot learners and reasoners\\nPerception\\nModeling\\nManipulation\\nReasoning\\n¬∑ ¬∑ ¬∑\\n¬∑ ¬∑ ¬∑\\nBlind deblurring\\nBlind denoising\\nDalmation illusion\\nShape cue-conflict\\nRorschach blots\\nEdge detection\\nLow-light enhancing\\nSuper-resolution\\nConjunctive search\\nKeypoint localization\\nSegmentation\\nRigid bodies\\nMaterial optics mirror\\nMemory\\nMaterial optics glass\\nColors: additive\\nBuoyancy stone\\nObject packing\\nColors: subtractive\\nSoft bodies\\nBuoyancy bottle cap\\nVisual Jenga\\nAir resistance: earth\\nAir resistance: moon\\nParsing into parts\\nCategorizing objects\\nCharacter recognition\\nFlammability\\nCharacter generation\\nInpainting\\nOutpainting\\nEditing with doodles\\nManipulation: jar\\nManipulation: throw\\nNovel view synthesis\\nBackground removal\\nReposing\\nStyle transfer\\nScene composition\\nAffordance\\nProfessional headshot\\nText manipulation\\nDrawing\\nVisual instructions\\nTransfiguration\\nColorization\\nManipulation: balls\\nSequence: arrows\\nSequence: squares\\nSequence: circles\\nSudoku\\nRobot navigation\\nWater puzzle\\nSequence: dots\\nConnecting colors\\nSpatial reasoning\\nTree BFS\\nWater solving\\nGraph traversal\\nSorting numbers\\nTool use\\nRule extrapolation\\n0\\n1\\nSuccess rate\\nFigure 1 | A qualitative overview of Veo 3‚Äôs zero-shot abilities. The plot shows Veo 3‚Äôs success\\nrate across 12 samples as a rough estimate of model performance on 62 tasks across the vision stack.\\nTasks are described in Sec. 3 and shown in Sec. A. Find videos of all tasks on our project page.\\nprompting a capable foundation model [27‚Äì29]. Here, we adopt the same philosophy to explore the\\ncapabilities of Veo 3 as a general-purpose vision model.\\nTakeaway 1\\nIn NLP, prompting replaced task-specific training or adaptation. A similar paradigm\\nshift is on the horizon in machine vision, facilitated by video models.\\nVideo generation\\nFor each task, we query the publicly available Veo 2 or Veo 3 models via Google\\nCloud‚Äôs Vertex AI API. We prompt the model with an initial input image (which the model uses as\\nthe first frame) and a text instruction. The models then generate a 16:9 video at 720p resolution,\\n24 FPS, for a duration of 8s. Veo 3 has model ID veo-3.0-generate-preview and Veo 2 model ID\\nveo-2.0-generate-001. According to the Vertex documentation [30], the API uses an LLM-based\\nprompt rewriter. This means that for some tasks, the solution is likely to come from the LLM instead\\nof the video (e.g., Fig. 55: Sudoku). We treat the system (rewriter and video generator) as a single\\nblack-box entity. However, to isolate the video model‚Äôs reasoning abilities, we verified that a standalone\\nLLM (Gemini 2.5 Pro [2]) could not reliably solve key tasks (Fig. 58: Robot navigation, Sec. 4.5:\\nMaze solving, Sec. 4.6: Visual symmetry) from the input image alone.\\nWhy Veo?\\nThe core argument of this paper‚Äîthat video models are zero-shot learners and reasoners‚Äî\\ncan be supported by demonstrating success on any sufficiently capable model. We choose Veo because\\nit has consistently ranked high on text2video and image2video leaderboards [31]. Unless noted\\notherwise, our figures are generated with Veo 3. To provide a sense of how rapidly performance is\\nimproving, our quantitative analyses compare Veo 3 with its predecessor, Veo 2, released roughly\\nwithin half a year of each other: Veo 2 was announced in December 2024 and released in April\\n2025 [32, 33], while Veo 3 was announced in May 2025 and released in July 2025 [34, 35].\\n3. Qualitative results: Sparks of visual intelligence?\\nWe begin with a comprehensive, qualitative investigation across visual tasks to assess the potential of\\nvideo models as visual foundation models. We organize our findings into four hierarchical capabilities,\\neach building on the preceding ones (c.f. Fig. 1 and Fig. 2):\\n1. Perception as a foundational ability to understand visual information.\\n2. Modeling, which builds on the perception of objects to form a model of the visual world.\\n3. Manipulation, which meaningfully alters the perceived and modeled world.\\n4. Reasoning across space and time over a sequence of manipulation steps.\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Video models are zero-shot learners and reasoners\\nPerception ‚Äì superresolution\\nPerception ‚Äì conjunctive search / binding problem\\nModeling ‚Äì buoyancy\\nModeling ‚Äì memory of world states\\nManipulation ‚Äì 3D-aware reposing\\nManipulation ‚Äì jar opening\\nReasoning ‚Äì robot navigation\\nReasoning ‚Äì rule extrapolation\\nFigure 2 | Veo 3 zero-shot learning and reasoning examples. From classic perceptual tasks\\n(superresolution, visual search) to modeling (buoyancy, memory of world states after zooming in),\\nmanipulation (pose editing, simulating dexterous manipulation) and visual reasoning (navigation,\\nrule extrapolation): Veo 3 can zero-shot solve a host of visual tasks that are specified as an input\\nimage and a text prompt. Examples are shown in Sec. A; videos of all tasks are on our project page.\\nWhile capability boundaries often overlap, this hierarchy provides a framework for understanding the\\nemergent abilities of video models. For example, solving a maze (see Fig. 57 and Sec. 4.5) requires\\nperceiving the maze, modeling its state (walls vs. floor), and finally manipulating an object (a mouse,\\na circle) to move from start to finish.\\nFor each task in this section, we prompt Veo 3 twelve times and report the success rate in the\\ncaption. The success rate is defined as the fraction of generated videos that solved the task (as\\ndetermined by the authors). A success rate greater than 0 suggests that the model possesses the ability\\nto solve the task, while a success rate closer to 1 indicates that the task is solved reliably irrespective\\nof the random seed. While not a substitute for the systematic quantification we perform in Sec. 4,\\nthis provides a ballpark estimate of the model‚Äôs capabilities.\\nPerception\\nComputer vision has historically relied on a suite of specialized models for tasks like\\nsegmentation [11, 12], object detection [13, 14], and edge detection [36]. While some backbones\\ncan be adapted or fine-tuned for other tasks, training-free transfer to novel tasks is rare, limiting\\ngeneralization. As we show here, this is changing with large video models.\\nWithout any task-specific training, Veo 3 can perform a range of classic computer vision tasks,\\nincluding edge detection (Fig. 10), segmentation (Fig. 11), keypoint localization (Fig. 12), super-\\nresolution (Fig. 13), blind deblurring (Fig. 14), denoising (Fig. 15) and low-light enhancing (Fig. 16).\\nThese emergent abilities extend to more complex perceptual tasks like conjunctive search (Fig. 17)\\nand interpreting ambiguous images such as the classic dalmatian illusion (Fig. 18), the cat shape in a\\ntexture-shape cue conflict image (Fig. 19), and colored blots from the Rorschach test (Fig. 20). Apart\\nfrom denoising‚Äîthe classic diffusion objective‚Äînone of these tasks are explicitly trained for in video\\nmodels.\\nTakeaway 2\\nVeo 3 shows emergent zero-shot perceptual abilities well beyond the training task.\\nJust like LLMs replaced task-specific NLP models, video models will likely replace most bespoke\\nmodels in computer vision‚Äîonce they become sufficiently cheap and reliable.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Video models are zero-shot learners and reasoners\\nModeling: intuitive physics & world models\\nBased on their perception of the visual world, video\\nmodels are starting to model it, too. Modeling the world and the principles that govern it (e.g., laws\\nof physics) is a critical step toward successful prediction and action. Several works have investigated\\nand quantified intuitive physics in deep models [e.g., 37‚Äì50]. Here, we investigate an exemplary\\nsubset of tasks from these works. Veo‚Äôs grasp of physics is demonstrated by its ability to model\\nvarious physical properties, like flammability (Fig. 21), rigid and soft body dynamics and their surface\\ninteractions (Fig. 22) and air resistance affecting falling objects (Fig. 23), and buoyancy (Fig. 24). As\\nillustrated by the Visual Jenga task [51], Veo can remove objects from a scene in a physically plausible\\norder (Fig. 25) and understands which objects fit into a backpack (Fig. 26). Furthermore, it correctly\\ngenerates some optical phenomenona like refraction and reflection (Fig. 27) and additive/subtractive\\ncolor mixing (Fig. 28). Beyond physical characteristics, Veo understands some abstract relationships\\nwhich is an important aspect of modeling the world. For example, Veo can distinguish categories like\\ntoys from other objects like a laptop (Fig. 29). On samples inspired by the Omniglot dataset [52],\\nwe demonstrate Veo‚Äôs ability to recognize patterns, generate variations thereof, and parse larger\\nwholes into parts (Fig. 30). Lastly, Veo maintains a memory of the world state across time and camera\\nmovements within the video context (Fig. 31).\\nManipulation: editing & imagination\\nBased on its ability to perceive objects and model their relation\\nto each other and the world, Veo can meaningfully manipulate the visual world, too. This enables\\nVeo 3 to perform a variety of zero-shot image editing tasks like background removal (Fig. 32), style\\ntransfer (Fig. 33), colorization (Fig. 34), inpainting (Fig. 35), and outpainting (Fig. 36). Furthermore,\\nit can manipulate text elements (Fig. 37), and edit images based on doodle instructions (Fig. 38).\\nVeo‚Äôs understanding of 3D world enables it to compose scenes from individual components (Fig. 39),\\ngenerate novel views of objects and characters (Figs. 40 and 41), smoothly transform one object\\ninto another (Fig. 42), or change the perspective, lighting, and appearance to turn a selfie into a\\nprofessional photograph (Fig. 43).\\nThis ability to plausibly modify a scene allows it to imagine complex interactions, simulate\\ndexterous object manipulation (Fig. 44; note that we do not test actual robots as e.g. [53] do),\\ninterpreting object affordances (Fig. 45), demonstrating how to draw a shape (Fig. 46) and roll a\\nburrito (Fig. 47). Overall, video models can meaningfully manipulate and simulate aspects of the\\n(digital) visual world.\\nVisual reasoning across time and space\\nPerception, modeling, and manipulation all integrate to\\ntackle visual reasoning. While language models manipulate human-invented symbols, video models\\ncan apply changes across the dimensions of the real world: time and space. Since these changes\\nare applied frame-by-frame in a generated video, this parallels chain-of-thought in LLMs and could\\ntherefore be called chain-of-frames, or CoF for short. In the language domain, chain-of-thought\\nenabled models to tackle reasoning problems [27]. Similarly, chain-of-frames (a.k.a. video generation)\\nmight enable video models to solve challenging visual problems that require step-by-step reasoning\\nacross time and space.\\nWe see early signs of this ability in tasks such as generating a valid graph traversal (Fig. 48), per-\\nforming visual breadth-first search on a tree (Fig. 49), completing visual sequences (Fig. 50), connect-\\ning matching colors (Fig. 51), fitting shapes into holes (Fig. 52), and sorting numbers (Fig. 53). Fur-\\nthermore, Veo can use tools to accomplish a visual task (Fig. 54) and solve simple Sudokus (Fig. 55) or\\nvisual puzzles (Fig. 56). Finally, it can solve mazes and navigation tasks (Figs. 57 and 58 and Sec. 4.5)\\nand extrapolate rules from visual examples (Fig. 59). While not always perfect, the model‚Äôs ability to\\nsolve such problems in a zero-shot manner points to exciting possibilities for more advanced visual\\nreasoning and planning in future, with more capable video models.\\nTakeaway 3\\nFrame-by-frame video generation parallels chain-of-thought in language models.\\nJust like chain-of-thought (CoT) enables language models to reason with symbols, a ‚Äúchain-of-\\nframes‚Äù (CoF) enables video models to reason across time and space.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='Video models are zero-shot learners and reasoners\\nOriginal image\\nGenerated edge map (Veo 3)\\nGround-truth edge map\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nk\\n0.4\\n0.5\\n0.6\\n0.7\\nmax OIS@k\\n0.74\\n0.57\\n0.77\\nBest frame\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nk\\n0.4\\n0.5\\n0.6\\n0.7\\n0.74\\n0.51\\n0.74\\nLast frame\\nModel\\nVeo 3\\nVeo 2\\nNano Banana\\nFigure 3 | Edge detection on all 50 test images from BIPEDv2 [59, 60]. We generate 10 videos per\\nsample and report best performance over ùëòattempts as a function of ùëò. Prompt: ‚ÄúAll edges in this\\nimage become more salient by transforming into black outlines. Then, all objects fade away [...]‚Äù Details\\n& full prompt: Sec. B.1.\\nSummary\\nTaken together, the qualitative examples from this section indicate that a capable video\\nmodel like Veo 3 possesses strong zero-shot learning abilities. While the results are not always perfect,\\nthe model consistently demonstrates the capacity to solve a wide variety of tasks for which it was not\\nexplicitly trained.\\n4. Quantitative results\\nThe previous section offered a qualitative exploration of video model capabilities. In this section,\\nwe add a quantitative assessment for seven tasks. As in Sec. 3, we consider different facets of\\nvisual understanding: For perception, we assess Veo on edge detection and segmentation. For\\nmanipulation, we examine image editing and object extraction performance. Finally, we evaluate\\nreasoning abilities through maze solving, visual symmetry, and visual analogies. We do not include\\nevaluations for modeling, since this area is well addressed by recent benchmarks, see Sec. 3.\\nWe evaluate performance separately for the best frame and the last frame (where applicable).\\nBest frame reports the best performance across any frame in the generated videos. This indicates\\nthe performance ceiling, but the optimal frame is not known a priori. Therefore, we also report\\nperformance on the last frame of each video, which may underestimate a model‚Äôs ability but has the\\npractical advantage that the frame is predetermined. This distinction is important because Veo tends\\nto continue animating a scene even after task completion, potentially reducing last frame performance.\\nWhere applicable, we use the state-of-the-art image editing model Nano Banana [54] as a reference.\\nAs a general trend, we observe a large performance increase from Veo 2 to Veo 3, often matching\\nor exceeding Nano Banana‚Äôs performance. Performance tends to improve substantially from ùëò= 1\\nto ùëò= 10 attempts, indicating that a good solution can be found in a reasonable number of tries.\\nWhile image models are excellent zero-shot learners, too [55‚Äì58], video models are the more general\\nframework because of their ability to process both temporal and spatial information.\\n4.1. Perception: Edge detection\\nDespite not being trained for it, Veo 3 can be prompted to detect, and therefore perceive, edges.\\nFig. 3 details edge detection performance (measured by OIS; details and prompt in Sec. B.1) for\\nVeo 2 and Veo 3. While Veo 3 (0.77 pass@10) is not on par with task-specific SOTA (0.90) [60],\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Video models are zero-shot learners and reasoners\\nOriginal image\\nGenerated frame (Veo 3)\\nExtracted masks\\nGround-truth masks\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nk\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nmax mIoU@k\\n0.73\\n0.52\\n0.50\\n0.66\\n0.74\\nBest frame\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nk\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.73\\n0.40\\n0.41\\n0.42\\n0.56\\nLast frame\\nModel\\nBackground\\nVeo 3\\nVeo 2\\nNano Banana\\ngreen\\nwhite\\nFigure 4 | Class-agnostic instance segmentation on a subset of 50 easy images (1-3 large objects)\\nfrom LVIS [61]. Prompt: ‚Äú[...] each distinct entity is overlaid in a different flat color [...] the background\\nfades to {white, green} [...]‚Äù Details & full prompt: Sec. B.2.\\nOriginal image\\nGenerated frame (Veo 3)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nk\\n0.2\\n0.4\\n0.6\\n0.8\\nPass@k\\n0.93\\n0.63\\nLast frame\\nModel\\nVeo 3\\nVeo 2\\nChance\\nFigure 5 | Object extraction on an animal dataset. Prompt: ‚ÄúThe background changes to white [...] all\\nanimals line up in a row [...]‚Äù Details & full prompt: Sec. B.3.\\nits performance is remarkable for two reasons: First, it is zero-shot. Second, many of Veo 3‚Äôs edge\\nmaps are more detailed than the ground truth. For example, Veo 3 correctly outlines foliage and tire\\nprofiles; this hurts performance on the dataset but seems more indicative of a dataset limitation than\\na model limitation (the annotators understandably did not bother to trace each and every edge).\\n4.2. Perception: Segmentation\\nInstance segmentation requires delineating (i.e., perceiving) distinct objects in an image. Contrary to\\nclassic instance segmentation or promptable segmentation, we prompt models to segment all objects\\nin a scene, without specifying an object category or location. We report mean Intersection over Union\\n(mIoU); experiment details are in Sec. B.2. As shown in Fig. 4, Veo 3 achieves an mIoU of 0.74 (best\\nframe pass@10), comparable to Nano Banana‚Äôs 0.73. Naturally, Veo 3 lacks behind the performance\\nof a bespoke model like SAMv2 [12], but nevertheless shows remarkable zero-shot segmentation\\nabilities. Interestingly, the prompt really matters: Veo consistently performs better with a green\\nbackground than a white one (0.74 vs. 0.66 best frame pass@10); possibly due to the widespread\\nuse of green screens. See also Sec. C for prompting best practices.\\n4.3. Manipulation: Object extraction\\nCan Veo perceive and extract (i.e., manipulate) all objects in a scene? We test this using a simple\\ndataset depicting between one and nine animals (details in Sec. B.3). Veo is asked to extract and line\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Video models are zero-shot learners and reasoners\\nOriginal image\\nGenerated frame (Veo 3)\\nGenerated frame (Veo 3)\\nFidelity\\nPrecision\\n0\\n20\\n40\\n60\\n80\\n100\\nHuman Rating %\\n100\\n60\\n20\\n10\\nModel\\nVeo 3\\nVeo 2\\nFigure 6 | Image editing on a subset of Emu-edit [62]. Prompt: ‚ÄúCreate a smooth, static animation\\nthat slowly changes the color of the fire hydrant to red. [...]‚Äù Details & full prompt: Sec. B.4.\\nPass@k %\\n5√ó5 Grid\\n7√ó7 Grid\\n9√ó9 Grid\\nIrregular\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n16\\n92\\n74\\n14\\n78\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n0\\n20\\n40\\n60\\n8\\n72\\n40\\n2\\n38\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n0\\n5\\n10\\n15\\n20\\n0\\n8\\n22\\n2\\n12\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n0\\n20\\n40\\n60\\n0\\n0\\n75\\nModel\\nVeo 3\\nVeo 2\\nNano Banana\\nGemini 2.5 Pro I2T\\nGemini 2.5 Pro T2T\\nFigure 7 | Maze solving. Mazes of various sizes with start (red) and goal (green) locations. Prompt:\\n‚Äú[...] The red circle slides smoothly along the white path, stopping perfectly on the green circle [...]‚Äù Details\\n& full prompt: Sec. B.5. Veo 2 struggles to solve even small sizes, mostly due to illegal moves early in\\nthe generation. Veo 3 performs much better and benefits from multiple attempts. For comparison, we\\nevaluate Nano Banana and also show Gemini 2.5 Pro‚Äôs performance on mazes presented as images\\n(I2T) or ASCII text (T2T).\\nup all animals in a row (in some sense, a visual ‚Äútally‚Äù), with white space between them. To assess\\nwhether the number of extracted animals is correct, we count connected components in the last frame.\\nFig. 5 shows a sample and the results. While Veo 2 performs around chance, Veo 3 achieves up to\\n93% pass@10. Given the simplicity of the task, a perfect model should easily achieve 100% accuracy.\\n4.4. Manipulation: Image editing\\nImage editing requires manipulating images according to a text instruction (e.g., adding/removing\\nobjects or changing their appearance). We evaluate whether Veo can edit images on a random subset\\nof 30 samples from Emu-edit [62]. Veo has a strong bias for animated scenes and might introduce\\nunintended changes (e.g., camera movement, animating people). We therefore ask three human\\nraters to evaluate fidelity (correct edit) and precision (correct edit without unintended changes). An\\nexample edit and results are shown in Fig. 6. We find that Veo 3 especially excels in preserving details\\nand textures across edits. If unintended changes such as camera movement or animating people can\\nbe controlled better, video models could become highly capable 3D-aware image and video editors\\n(see also [24, 63‚Äì65]).\\n4.5. Reasoning: Maze solving\\nMaze solving tests a model‚Äôs ability to plan a path in a constrained environment, a key aspect of\\nreasoning. In our setup, a red circle needs to navigate to a target location (green circle) without\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Video models are zero-shot learners and reasoners\\nPass@k %\\nOriginal image\\nGenerated frame (Veo 3)\\nOriginal image\\nGenerated frame (Veo 3)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n100\\n37\\n20\\n88\\nBest frame\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n100\\n37\\n0\\n44\\nLast frame\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n100\\n28\\n0\\n100\\nBest frame\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n0\\n20\\n40\\n60\\n80\\n100\\n28\\n0\\n72\\nLast frame\\nShapes\\nRandom patterns\\nModel\\nVeo 3\\nVeo 2\\nNano Banana\\nFigure 8 | Visual symmetry. Prompt: ‚ÄúInstantly reflect this pattern along the central, vertical axis while\\nkeeping the existing colored pattern without modification. [...]‚Äù A model has to color all cells correctly\\nto pass. Details & full prompt: Sec. B.6.\\nOriginal image\\nCompletion (Veo 3)\\nColor\\nResize\\nReflect\\nRotate\\n0\\n20\\n40\\n60\\n80\\n100\\nPass@1 %\\n95\\n67\\n29\\n19\\n68\\n40\\n23\\n22\\nModel\\nVeo 3\\nVeo 2\\nChance\\nFigure 9 | Visual analogy solving on four transformations √† 50 samples from KiVA [66]. Prompt:\\n‚Äú[...] generate the missing object in the lower right region and solve the visual analogy. [...]‚Äù Pass@1 is\\nevaluated on the last frame, results up to pass@10 can be found in Sec. B.7.\\ncrossing any walls. We automatically verify path correctness (details in Sec. B.5) and present results\\nfor different mazes in Fig. 7. Veo 3 shows zero-shot maze solving abilities, significantly outperforming\\nVeo 2 which often produces illegal moves. For instance, in the 5√ó5 grids, Veo 3 achieves a pass@10 rate\\nof 78% compared to Veo 2‚Äôs 14%. The consistent performance gap highlights the advancing reasoning\\ncapabilities of the newer model. While Nano Banana matches or surpasses Veo 3‚Äôs performance on\\nrectangular mazes, it fails to solve irregular mazes entirely. Similarly, Gemini 2.5 Pro outperforms\\nVeo 3 on small mazes when given an ASCII representation of the maze (T2T), but falls behind on 9√ó9\\nmazes, and generally struggles when the maze is represented as image (as opposed to text) input.\\nBoth comparisons highlight the advantages of solving a visual task step-by-step in a visual medium.\\n4.6. Reasoning: Visual symmetry solving\\nCompleting a pattern to be symmetrical assesses the ability to understand and apply spatial reasoning.\\nWe create a custom dataset of shapes (e.g., heart, letters) and random patterns (see Sec. B.6 for\\ndetails). Fig. 8 shows that Veo 3 outperforms Veo 2 and Nano Banana by a large margin. We also\\nuse this task to systematically analyze how different prompts affect performance in Sec. C: The\\npass@1 difference between best and worst prompt is 40 percentage points on the shape split and 64\\npercentage points on the random split.\\n4.7. Reasoning: Visual analogy completion\\nVisual analogies test a model‚Äôs ability to understand transformations and relationships between\\nobjects, a form of abstract reasoning. Concretely, we prompt the model to fill the missing quadrant\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Video models are zero-shot learners and reasoners\\nof a 2√ó2 grid to complete the analogy (e.g., A is to B as C is to ?). We evaluate the correctness of the\\ngenerated infill for four transformation types from KiVA [66], see Sec. B.7 for details. The results are\\nsummarized in Fig. 9. While Veo 2 struggles to understand any analogies, Veo 3 correctly completes\\nexamples for color and resize. However, both models perform below chance (0.33) on reflect and\\nrotate analogies, indicating an erroneous, systematic bias.\\nTakeaway 4\\nWhile far from perfect, Veo 3‚Äîbuilding on its ability to perceive, model and\\nmanipulate objects‚Äîshows emergent visual reasoning abilities.\\n5. Discussion\\nSummary\\nA breakthrough in machine vision started the deep learning revolution in 2012 [67], but\\nin recent years, natural language processing has seen the most rapid progress. This was driven by\\nthe rise of general-purpose LLMs, whose ability to solve novel tasks in a zero-shot fashion has led\\nthem to replace most task-specific models in NLP. We here make the case that machine vision is on\\nthe cusp of a similar paradigm shift, enabled by emergent abilities of large-scale video models. Our\\ncore finding is that Veo 3 can solve a wide range of tasks in a zero-shot manner, spanning the full\\nvision stack from perception to modeling, manipulation and even early forms of visual reasoning.\\nWhile its performance is not yet perfect, the massive and consistent improvement from Veo 2 to Veo 3\\nindicates that video models will become general-purpose foundation models for vision, just as LLMs\\nhave for language.\\nPerformance is a lower bound\\nTasks can be represented in a myriad of ways; a maze, for example,\\ncan be presented as a black-and-white grid, a video game, or a photorealistic scene, with the prompt\\nrequesting a solution in the form of a line, a moving object, or a glowing path. Moreover, visually, a\\nmaze could be represented as a black-and-white grid, a Pac-Man game, or a photorealistic top-down\\nview of an apartment. This has three implications: First, prompt engineering‚Äîincluding the visual\\nprompt a.k.a. starting frame‚Äîis as important for visual tasks as it is for LLMs (see also Sec. C and\\n[68] for a discussion). Second, we must distinguish between a model‚Äôs task performance and its\\nunderlying ability (i.e., competence) to solve that task [69, 70]. Third, as a consequence, the model\\nperformance reported here with a given visual and textual prompt should be considered a lower\\nbound on the model‚Äôs true capabilities. This also holds for the tasks that we report as failure cases\\nin Sec. D, such as providing visual instructions to fold laundry (Fig. 76), planning to move a sofa\\nbetween rooms separated by a small door (Fig. 77), or certain visual puzzles (Fig. 70).\\nVideo generation is expensive, but costs tend to fall\\nWhile generating a video is currently more\\nexpensive than running a bespoke, task-specific model, the economics of general-purpose models\\nare on a predictable trajectory. Epoch AI [71] estimates that LLM inference costs are falling by a\\nfactor of 9√ó to 900√ó per year for a given performance level. In NLP, early generalist models were\\nalso considered prohibitively expensive (‚ÄúGPT-3‚Äôs size makes it challenging to deploy‚Äù [7, p. 8]).\\nNevertheless, rapidly falling inference costs, combined with the appeal of generalist models, have\\nreplaced most task-specific language models. If NLP is a guide, the same trend will play out in vision.\\nJack of many trades, master of few?\\nFor many tasks, Veo 3‚Äôs performance is below state of the art\\nof specialized models. This mirrors the early days of LLMs; GPT-3 reported performance well below\\nfine-tuned models on many tasks [7, cf. Tables 3.1, 3.3, 3.4, 3.5]). This did not stop language models\\nfrom becoming foundation models, and we don‚Äôt believe it will stop video models from becoming\\nvision foundation models for two reasons. First, the step-change in performance from Veo 2 to Veo 3\\nis evidence of rapid progress over time. Second, our scaling results from Sec. 4 show pass@10 to\\nbe consistently higher than pass@1 with no signs of a plateau. Therefore, inference-time scaling\\nmethods [e.g. 72‚Äì75] in combination with the standard optimization toolkit like post-training with\\nautomatic verifiers are likely to boost performance. For the tasks we test here, Veo 3 is akin to a\\npre-trained language model that has yet to undergo instruction tuning or RLHF [76, 77].\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Video models are zero-shot learners and reasoners\\nOutlook\\nThis is an exciting time for vision. Seeing NLP‚Äôs recent transformation from task-specific\\nto generalist models, it is conceivable that the same transformation will happen in machine vision\\nthrough video models (a ‚ÄúGPT-3 moment for vision‚Äù), enabled by their emergent ability to perform a\\nbroad variety of tasks in a zero-shot fashion, from perception to visual reasoning.\\nAcknowledgements\\nWe would like to thank Oyvind Tafjord, Mike Mozer, Katherine Hermann, Andrew Lampinen, Vior-\\nica Patraucean, Shiry Ginosar, Ross Goroshin, Abhijit Ogale, Claire Cui, Kun Zhang for helpful\\nfeedback/discussions, Anish Nangia for Vertex insights, Tuan Anh Le for suggesting the Omniglot eval-\\nuation, Medhini Narasimhan and Pieter-Jan Kindermans for Veo insights and discussion, Micromelon\\nRobotics for permission to use the image in Fig. 58, Shelly Sheynin for permission to use the EMU Edit\\ndataset, David Fleet and Jon Shlens for their support, and the Veo team for developing an incredible\\nmodel.\\nAuthor contributions\\nLeads: TW, PJ, RG. Project idea: RG. Core contributors: YL, PV. Partial contributor: SG. Fig. 38: NM.\\nAdvisors: KS, BK.\\nReferences\\n[1] Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. A survey on large language\\nmodels for code generation. arXiv preprint arXiv:2406.00515, 2024.\\n[2] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic\\ncapabilities. arXiv preprint arXiv:2507.06261, 2025.\\n[3] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao,\\nChunzhao Xie, Chuou Xu, Jihong Dai, et al. Weaver: Foundation models for creative writing.\\narXiv preprint arXiv:2401.17268, 2024.\\n[4] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun\\nChen, and Lei Li. Multilingual machine translation with large language models: Empirical\\nresults and analysis. arXiv preprint arXiv:2304.04675, 2023.\\n[5] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scien-\\ntist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292,\\n2024.\\n[6] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu,\\nMichael Moor, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research\\nassistants. arXiv preprint arXiv:2501.04227, 2025.\\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020.\\n[8] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\\nlanguage models. arXiv preprint arXiv:2206.07682, 2022.\\n[9] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu,\\nZhiyong Wu, Tianyu Liu, et al. A survey on in-context learning. arXiv preprint arXiv:2301.00234,\\n2022.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Video models are zero-shot learners and reasoners\\n[10] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. Advances in neural information processing systems, 35:\\n22199‚Äì22213, 2022.\\n[11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In\\nProceedings of the IEEE/CVF international conference on computer vision, pages 4015‚Äì4026, 2023.\\n[12] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma,\\nHaitham Khedr, Roman R√§dle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything\\nin images and videos. arXiv preprint arXiv:2408.00714, 2024.\\n[13] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,\\nreal-time object detection. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 779‚Äì788, 2016.\\n[14] Rahima Khanam and Muhammad Hussain. Yolov11: An overview of the key architectural\\nenhancements. arXiv preprint arXiv:2410.17725, 2024.\\n[15] Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre\\nAlahi, and Paolo Favaro. From generation to generalization: Emergent few-shot learning in\\nvideo diffusion models. arXiv preprint arXiv:2506.07280, 2025.\\n[16] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\\nSavarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference\\non computer vision and pattern recognition, pages 3712‚Äì3722, 2018.\\n[17] Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong Mao. Realgeneral: Unifying visual\\ngeneration via temporal in-context learning with video models. arXiv preprint arXiv:2503.10406,\\n2025.\\n[18] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming\\nCheng. Visualcloze: A universal image generation framework via visual in-context learning.\\narXiv preprint arXiv:2504.07960, 2025.\\n[19] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images:\\nA generalist painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pages 6830‚Äì6839, 2023.\\n[20] Jiahao Xie, Alessio Tonioni, Nathalie Rauschmayr, Federico Tombari, and Bernt Schiele. Test-\\ntime visual in-context tuning. In Proceedings of the Computer Vision and Pattern Recognition\\nConference, pages 19996‚Äì20005, 2025.\\n[21] Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Huan Teng,\\nJunlin Xie, Yu Qiao, Peng Gao, et al. Pixwizard: Versatile image-to-image visual assistant with\\nopen-language instructions. arXiv preprint arXiv:2409.15278, 2024.\\n[22] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan\\nLi, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In\\nProceedings of the Computer Vision and Pattern Recognition Conference, pages 13294‚Äì13304,\\n2025.\\n[23] Duong H Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt,\\nRanjay Krishna, and Jiasen Lu. One diffusion to generate them all. In Proceedings of the Computer\\nVision and Pattern Recognition Conference, pages 2671‚Äì2682, 2025.\\n[24] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv\\nLeviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv\\npreprint arXiv:2302.01329, 2023.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Video models are zero-shot learners and reasoners\\n[25] Rahul Ravishankar, Zeeshan Patel, Jathushan Rajasegaran, and Jitendra Malik. Scaling properties\\nof diffusion models for perceptual tasks. In Proceedings of the Computer Vision and Pattern\\nRecognition Conference, pages 12945‚Äì12954, 2025.\\n[26] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter\\nAbbeel, and Dale Schuurmans. Video as the new language for real-world decision making. arXiv\\npreprint arXiv:2402.17139, 2024.\\n[27] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances\\nin neural information processing systems, 35:24824‚Äì24837, 2022.\\n[28] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\\nand Jimmy Ba. Large language models are human-level prompt engineers. In The eleventh\\ninternational conference on learning representations, 2022.\\n[29] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\\nprocessing. ACM computing surveys, 55(9):1‚Äì35, 2023.\\n[30] Google Cloud. Vertex AI Veo Prompt Rewriter. https://cloud.google.com/vertex-ai/\\ngenerative-ai/docs/video/turn-the-prompt-rewriter-off#prompt-rewriter,\\n2025. Accessed: September 22, 2025.\\n[31] LMSYS ORG. Lmsys org text-to-video leaderboard. https://lmarena.ai/leaderboard/t\\next-to-video, September 2025. Accessed: 2025-09-23.\\n[32] Google. Veo 2 announcement. https://blog.google/technology/google-labs/vide\\no-image-generation-update-december-2024/, 2024. Accessed: September 22, 2025.\\n[33] Google. Veo 2 launch. https://developers.googleblog.com/en/veo-2-video-gen\\neration-now-generally-available/, 2025. Accessed: September 22, 2025.\\n[34] Google. Veo 3 announcement. https://blog.google/technology/ai/generative-m\\nedia-models-io-2025/, 2025. Accessed: September 22, 2025.\\n[35] Google. Veo 3 launch. https://cloud.google.com/blog/products/ai-machine-l\\nearning/veo-3-fast-available-for-everyone-on-vertex-ai, 2025. Accessed:\\nSeptember 22, 2025.\\n[36] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In Proceedings of the IEEE\\ninternational conference on computer vision, pages 1395‚Äì1403, 2015.\\n[37] Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, V√©ronique\\nIzard, and Emmanuel Dupoux. IntPhys: A framework and benchmark for visual intuitive physics\\nreasoning. arXiv preprint arXiv:1803.07616, 2018.\\n[38] Daniel M. Bear, Elias Wang, Damian Mrowca, Felix J. Binder, Hsiao-Yu Fish Tung, R. T. Pramod,\\nCameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, Li Fei-Fei, Nancy Kanwisher, Joshua B.\\nTenenbaum, Daniel L. K. Yamins, and Judith E. Fan. Physion: Evaluating physical prediction\\nfrom vision in humans and machines, 2021.\\n[39] Luca Weihs, Amanda Yuile, Ren√©e Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi,\\nand Aniruddha Kembhavi. Benchmarking progress to infant-level physical reasoning in ai.\\nTransactions on Machine Learning Research, 2022.\\n[40] Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, and Elia Bruni.\\nGRASP: A novel benchmark for evaluating language grounding and situated physics understand-\\ning in multimodal language models. arXiv preprint arXiv:2311.09048, 2023.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Video models are zero-shot learners and reasoners\\n[41] Hsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Josh Tenenbaum, Dan\\nYamins, Judith Fan, and Kevin Smith. Physion++: Evaluating physical scene understanding\\nthat requires online inference of different physical properties. Advances in Neural Information\\nProcessing Systems, 36, 2024.\\n[42] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chen-\\nfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical\\ncommonsense for video generation, 2024.\\n[43] Anoop Cherian, Radu Corcodel, Siddarth Jain, and Diego Romeres. LLMPhy: Complex physical\\nreasoning using large language models and world models. arXiv preprint arXiv:2411.08027,\\n2024.\\n[44] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng,\\nDianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-\\nbased benchmark for video generation, 2024.\\n[45] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi\\nFeng. How far is video generation from world model: A physical law perspective. arXiv preprint\\narXiv:2411.02385, 2024.\\n[46] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative\\nvideo models understand physical principles? arXiv preprint arXiv:2501.09038, 2025.\\n[47] Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, and Chang Xu.\\nGenerative physical AI in vision: A survey. arXiv preprint arXiv:2501.10928, 2025.\\n[48] Luca M Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in\\nmultimodal large language models. Nature Machine Intelligence, pages 1‚Äì11, 2025.\\n[49] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios\\nVozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios\\nGavves. Morpheus: Benchmarking physical reasoning of video generative models with real\\nphysical experiments. arXiv preprint arXiv:2504.02918, 2025.\\n[50] Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael\\nRabbat, Emmanuel Dupoux, and Yann LeCun. Intuitive physics understanding emerges from\\nself-supervised pretraining on natural videos. arXiv preprint arXiv:2502.11831, 2025.\\n[51] Anand Bhattad, Konpat Preechakul, and Alexei A Efros. Visual jenga: Discovering object\\ndependencies via counterfactual inpainting. arXiv preprint arXiv:2503.21770, 2025.\\n[52] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept\\nlearning through probabilistic program induction. Science, 350(6266):1332‚Äì1338, 2015.\\n[53] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley,\\nAmmar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-JEPA 2: Self-supervised\\nvideo models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985,\\n2025.\\n[54] Google. Nano Banana: Gemini Image Generation Overview. https://gemini.google/ov\\nerview/image-generation/, 2025. Accessed: September 22, 2025.\\n[55] Kevin Clark and Priyank Jaini. Text-to-image diffusion models are zero shot classifiers. Advances\\nin Neural Information Processing Systems, 36:58921‚Äì58937, 2023.\\n[56] Priyank Jaini, Kevin Clark, and Robert Geirhos. Intriguing properties of generative classifiers.\\nIn The Twelfth International Conference on Learning Representations, 2023.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Video models are zero-shot learners and reasoners\\n[57] Ryan Burgert, Kanchana Ranasinghe, Xiang Li, and Michael S Ryoo. Peekaboo: Text to image\\ndiffusion models are zero-shot segmentors. arXiv preprint arXiv:2211.13224, 2022.\\n[58] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\\nWang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models\\nare zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on\\nComputer Vision, pages 15954‚Äì15964, 2023.\\n[59] Xavier Soria, Edgar Riba, and Angel Sappa. Dense extreme inception network: Towards a robust\\nCNN model for edge detection. In The IEEE Winter Conference on Applications of Computer Vision\\n(WACV ‚Äô20), 2020.\\n[60] Xavier Soria, Angel Sappa, Patricio Humanante, and Arash Akbarinia. Dense extreme inception\\nnetwork for edge detection. Pattern Recognition, 139:109461, 2023. ISSN 0031-3203. doi:\\nhttps://doi.org/10.1016/j.patcog.2023.109461. URL https://www.sciencedirect.com/\\nscience/article/pii/S0031320323001619.\\n[61] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance\\nsegmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\\n2019.\\n[62] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh,\\nand Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n8871‚Äì8879, 2024.\\n[63] Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, and Dacheng Tao. Diffusion model-based video\\nediting: A survey. arXiv preprint arXiv:2407.07111, 2024.\\n[64] Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, and Mohit\\nBansal. VEGGIE: Instructional editing and reasoning of video concepts with grounded generation.\\narXiv preprint arXiv:2503.14350, 2025.\\n[65] Noam Rotstein, Gal Yona, Daniel Silver, Roy Velich, David Bensaid, and Ron Kimmel. Pathways\\non the image manifold: Image editing via video generation. In Proceedings of the Computer\\nVision and Pattern Recognition Conference, pages 7857‚Äì7866, 2025.\\n[66] Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Wong, Yutong Bai, Shiry Ginosar, Alison\\nGopnik, and Kate Saenko. Kiva: Kid-inspired visual analogies for testing large multimodal\\nmodels. arXiv preprint arXiv:2407.17773, 2024.\\n[67] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep\\nconvolutional neural networks. Advances in neural information processing systems, 25, 2012.\\n[68] Andrew Kyle Lampinen, Stephanie CY Chan, Aaditya K Singh, and Murray Shanahan. The\\nbroader spectrum of in-context learning. arXiv preprint arXiv:2412.03782, 2024.\\n[69] Chaz Firestone. Performance vs. competence in human‚Äìmachine comparisons. Proceedings of\\nthe National Academy of Sciences, 117(43):26562‚Äì26571, 2020.\\n[70] Robert Geirhos, J√∂rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,\\nMatthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature\\nMachine Intelligence, 2(11):665‚Äì673, 2020.\\n[71] Ben Cottier, Ben Snodin, David Owen, and Tom Adamczewski. LLM inference prices have fallen\\nrapidly but unequally across tasks, march 2025. URL https://epoch.ai/data-insights/\\nllm-inference-price-trends. Accessed: 2025-09-12.\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Video models are zero-shot learners and reasoners\\n[72] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\\nmodels. arXiv preprint arXiv:2203.11171, 2022.\\n[73] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\\nwith self-feedback. Advances in Neural Information Processing Systems, 36:46534‚Äì46594, 2023.\\n[74] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. OpenAI o1 system card. arXiv preprint\\narXiv:2412.16720, 2024.\\n[75] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute\\noptimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314,\\n2024.\\n[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\\nfollow instructions with human feedback. Advances in neural information processing systems, 35:\\n27730‚Äì27744, 2022.\\n[77] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\\nwith GPT-4. arXiv preprint arXiv:2304.03277, 2023.\\n[78] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient\\nregularized deep retinex network for robust low-light image enhancement. IEEE Transactions\\non Image Processing, 30:2072‚Äì2086, 2021.\\n[79] Declan Campbell, Sunayana Rane, Tyler Giallanza, Camillo Nicol√≤ De Sabbata, Kia Ghods,\\nAmogh Joshi, Alexander Ku, Steven Frankland, Tom Griffiths, Jonathan D Cohen, et al. Under-\\nstanding the limits of vision language models through the lens of the binding problem. Advances\\nin Neural Information Processing Systems, 37:113436‚Äì113460, 2024.\\n[80] R. C. James. Sight for sharp eyes. LIFE, 58(7):120, 1965.\\n[81] Richard Langton Gregory. The intelligent eye, 1970.\\n[82] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and\\nWieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias\\nimproves accuracy and robustness. In International conference on learning representations, 2019.\\n[83] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan\\nFan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of\\n10m+ 3d objects. Advances in Neural Information Processing Systems, 36:35799‚Äì35813, 2023.\\n[84] Fran√ßois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.\\n[85] Piotr Doll√°r and C. Lawrence Zitnick. Structured forests for fast edge detection. In ICCV, 2013.\\n[86] Piotr Doll√°r and C. Lawrence Zitnick. Fast edge detection using structured forests. ArXiv, 2014.\\n[87] C. Lawrence Zitnick and Piotr Doll√°r. Edge boxes: Locating object proposals from edges. In\\nECCV, 2014.\\n[88] Kai Leng, Zhijie Zhang, Jie Liu, Zeyd Boukhers, Wei Sui, Cong Yang, and Zhijun Li. Superedge:\\nTowards a generalization model for self-supervised edge detection. CoRR, 2024.\\n[89] Michael Ivanitskiy. Maze dataset. https://pypi.org/project/maze-dataset/0.3.4/,\\n2025. Accessed: June 31, 2025.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Video models are zero-shot learners and reasoners\\n[90] Yujin Jeong, Arnas Uselis, Seong Joon Oh, and Anna Rohrbach. Diffusion classifiers understand\\ncompositionality, but conditions apply. arXiv preprint arXiv:2505.17955, 2, 2025.\\n[91] Nate Gillman, Charles Herrmann, Michael Freeman, Daksh Aggarwal, Evan Luo, Deqing Sun, and\\nChen Sun. Force prompting: Video generation models can learn and generalize physics-based\\ncontrol signals, 2025. URL https://arxiv.org/abs/2505.19386.\\n[92] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff,\\nTatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang,\\nAndrew Owens, and Deqing Sun. Motion prompting: Controlling video generation with motion\\ntrajectories. arXiv preprint arXiv:2412.02700, 2024.\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Video models are zero-shot learners and reasoners\\nAppendix\\nA. Qualitative results:\\nPerception, Modeling, Manipulation, Reasoning\\nA.1. Perception\\nFigure 10 | Edge detection. Prompt: ‚ÄúAll edges in this image become more salient by transforming into\\nblack outlines. Then, all objects fade away, with just the edges remaining on a white background. Static\\ncamera perspective, no zoom or pan.‚Äù Success rate: 0.92.\\nFigure 11 | Segmentation. Prompt: ‚ÄúCreate an animation of instance segmentation being performed on\\nthis photograph: each distinct entity is overlaid in a different flat color [...]‚Äù (full prompt: Sec. B.2).\\nSuccess rate: 0.33.\\nFigure 12 | Keypoint localization. Prompt: ‚ÄúAdd a bright blue dot at the tip of the branch on which\\nthe macaw is sitting. The macaw‚Äôs eye turns bright red. Everything else turns pitch black. Static camera\\nperspective, no zoom or pan.‚Äù Success rate: 0.58.\\nFigure 13 | Super-resolution. Prompt: ‚ÄúPerform superresolution on this image. Static camera perspective,\\nno zoom or pan.‚Äù Success rate: 0.75.\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Video models are zero-shot learners and reasoners\\nFigure 14 | Blind deblurring. Prompt: ‚ÄúUnblur image including background. Static camera perspective,\\nno zoom or pan.‚Äù Success rate: 1.0.\\nFigure 15 | Blind denoising. Each quadrant was corrupted with a different type of noise. Clockwise\\nfrom top left: Gaussian noise, salt-and-pepper noise, speckle noise, shot noise. Prompt: ‚ÄúRemove the\\nnoise from this image. Static camera perspective, no zoom or pan.‚Äù Success rate: 1.0.\\nOriginal low-light image\\nVeo 3-generated lit image\\nGround-truth lit image\\nFigure 16 | Low-light enhancing. Prompt: ‚ÄúFully restore the light in this image. Static camera\\nperspective, no zoom or pan.‚Äù Success rate: 0.92. Image and ground-truth source: LOLv2 dataset [78].\\nFigure 17 | Conjunctive search / binding problem. Prompt: ‚ÄúThe blue ball instantly begins to glow.\\nStatic camera perspective, no zoom no pan no movement no dolly no rotation.‚Äù Success rate: 0.75.\\nInspiration: [79].\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Video models are zero-shot learners and reasoners\\nFigure 18 | Dalmatian illusion understanding. Prompt: ‚ÄúStatic camera perspective.‚Äù Success rate: 1.0.\\nImage credit: [80, 81].\\nFigure 19 | Shape (cue-conflict) understanding. Prompt: ‚ÄúTransform the animal in this image into a\\nsketch of the animal surrounded by its family.‚Äù Success rate: 1.0. Image credit: [82].\\nFigure 20 | Rorschach blot interpretation. Prompt: ‚ÄúThe patterns transform into objects.‚Äù Success\\nrate: undefined (1.0 for prompt following). Image credit: H. Rorschach, public domain via wikipedia.\\nA.2. Modeling\\nFigure 21 | Material properties. Prompt: ‚ÄúThe bunsen burner at the bottom turns on. Sped up time\\nlapse. Static camera, no pan, no zoom, no dolly.‚Äù Success rate: 0.25.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='Video models are zero-shot learners and reasoners\\nFigure 22 | Physics body transform. Rigid body (top). Prompt: ‚ÄúA person picks up the vase and\\nputs it back on the table in a sideways orientation. Static camera, no pan, no zoom, no dolly.‚Äù Success\\nrate: 1.0. Soft body (bottom). Prompt: ‚ÄúA person drapes a thin silk scarf over the vase. Static camera,\\nno pan, no zoom, no dolly.‚Äù Success rate: 0.67.\\nFigure 23 | Gravity and air resistance. On earth (top). Prompt: ‚ÄúThe objects fall due to gravity. Static\\ncamera, no pan, no zoom, no dolly.‚Äù Success rate: 0.5. On the moon (bottom). Prompt: ‚ÄúThe objects\\nfall down on the moon due to gravity. Static camera, no pan, no zoom, no dolly.‚Äù Success rate: 0.5.\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='Video models are zero-shot learners and reasoners\\nFigure 24 | Buoyancy. Prompt: ‚ÄúThe hand lets go of the object. Static camera, no pan, no zoom, no\\ndolly.‚Äù Success rate (bottle cap): 0.58; success rate (rock): 0.83.\\nFigure 25 | Visual Jenga, inspired by [51]. Prompt: ‚ÄúA hand quickly removes each of the items in this\\nimage, one at a time.‚Äù Success rate, based on removal of at least three objects: 0.5.\\nFigure 26 | Object packing. Prompt: ‚ÄúA person puts all the objects that can fit in the backpack inside of\\nit. Static camera, no pan, no zoom, no dolly.‚Äù Success rate: 0.75.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Video models are zero-shot learners and reasoners\\nFigure 27 | Material optics. Glass (top). Prompt: ‚ÄúA giant glass sphere rolls through the room. Static\\ncamera, no pan, no zoom, no dolly.‚Äù Note that the image through the glass sphere is inverted. Success\\nrate: 0.92. Mirror (bottom). Prompt: ‚ÄúA giant mirror-polish metal sphere rolls through the room. Static\\ncamera, no pan, no zoom, no dolly.‚Äù Note that the image reflected off the sphere is not inverted).\\nSuccess rate: 1.0.\\nFigure 28 | Color mixing. Additive (lights, top). Prompt: ‚ÄúThe spotlight on the left changes color to\\ngreen, and the spotlight on the right changes color to blue.‚Äù Success rate: 0.92. Subtractive (paints,\\nbottom). Prompt: ‚ÄúA paintbrush mixes these colors together thoroughly until they blend completely.\\nStatic camera, no pan, no zoom.‚Äù Success rate: 0.75.\\nFigure 29 | Categorizing objects. Prompt: ‚ÄúA person puts all the kids toys in the bucket. Static camera,\\nno pan, no zoom, no dolly.‚Äù Success rate: 0.33.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Video models are zero-shot learners and reasoners\\nFigure 30 | Character recognition, generation, and parsing, inspired by the Omniglot dataset [52].\\nRecognition (top). Prompt: ‚ÄúThe background of the grid cell with the same symbol as the one indicated\\non the right turns red. All other grid cells remain unchanged. After that, a spinning color wheel appears\\nin the top right corner.‚Äù (Note: Veo 3 has a prior to keep things moving, which is detrimental for tasks\\nwhere the solution is obtained in an early frame. We observe that a ‚Äòmotion outlet‚Äô, such as a color\\nwheel, can indicate task completion and ‚Äòfreeze‚Äô the solution.) Success rate: 0.33. Generation of\\nvariations (middle). Prompt: ‚ÄúThe page is filled line-by-line with hand-written practice variations of\\nthe symbol.‚Äù Success rate: 0.25. Parsing into parts (bottom). Color and numbers in final frame are\\nadded post-hoc to show stroke order. Prompt: ‚ÄúStroke-by-stroke, a replica of the symbol is drawn on\\nthe right.‚Äù Success rate: 0.5.\\nFigure 31 | Memory of world states. Prompt: ‚ÄúThe camera zooms in to give a close up of the person\\nlooking out the window, then zooms back out to return to the original view.‚Äù Success rate: 1.0.\\nA.3. Manipulation\\nFigure 32 | Background removal. Prompt: ‚ÄúThe background changes to white. Static camera perspective,\\nno zoom or pan.‚Äù Success rate: 0.83.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='Video models are zero-shot learners and reasoners\\nFigure 33 | Style transfer. Prompt: ‚ÄúThe scene transforms into the style of a Hundertwasser painting,\\nwithout changing perspective or orientation; the macaw does not move. Static camera perspective, no\\nzoom or pan.‚Äù Success rate: 0.75.\\nFigure 34 | Colorization. Prompt: ‚ÄúPerform colorization on this image. Static camera perspective, no\\nzoom or pan.‚Äù Success rate: 0.08.\\nFigure 35 | Inpainting. Prompt: ‚ÄúThe white triangles become smaller and smaller, then disappear\\naltogether. Static camera perspective, no zoom or pan.‚Äù Success rate: 1.0.\\nFigure 36 | Outpainting. Prompt: ‚ÄúRapidly zoom out of this static image, revealing what‚Äôs around it.\\nThe camera just zooms back, while the scene itself and everything in it does not move or change at all, it‚Äôs\\na static image.‚Äù Success rate: 1.0.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='Video models are zero-shot learners and reasoners\\nFigure 37 | Text manipulation. Prompt: ‚ÄúAnimation of the text rapidly changing so that it is made out\\nof different types of candy (top left text) and pretzel sticks (bottom right text). Static camera perspective,\\nno zoom or pan.‚Äù Success rate: 0.33.\\nFigure 38 | Image editing with doodles. Prompt: ‚ÄúChanges happen instantly.‚Äù Success rate: 1.0.\\nFigure 39 | Scene composition. Prompt: ‚ÄúA smooth animation blends the zebra naturally into the scene,\\nremoving the background of the zebra image, so that the angle, lighting, and shading look realistic. The\\nfinal scene perfectly incorporates the zebra into the scene.‚Äù Success rate: 0.75.\\nFigure 40 | Single-image novel view synthesis. Prompt: ‚ÄúCreate a smooth, realistic animation where\\nthe camera seems to rotate around the object showing the object from all the sides. Do not change\\nanything else. No zoom. No pan.‚Äù Success rate: 0.92. Image source: [83].\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Video models are zero-shot learners and reasoners\\nFigure 41 | 3D-aware reposing. Prompt: ‚ÄúThe knight turns to face to the right and drops on one knee,\\nlifting the shield above his head to protect himself and resting the hilt of his weapon on the ground.‚Äù\\nSuccess rate: 0.83.\\nFigure 42 | Transfiguration. Prompt: ‚ÄúA magical spell smoothly transforms the structure of the teacup\\ninto a mouse.‚Äù Success rate: 0.17.\\nFigure 43 | Professional headshot generation. Prompt: ‚ÄúTurn this selfie into a professional headshot\\nfor LinkedIn.‚Äù Success rate: 0.42. Image credit: photo by George Pisarevsky on Unsplash.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='Video models are zero-shot learners and reasoners\\nFigure 44 | Dexterous manipulation. Jar opening (top). Prompt: ‚ÄúUse common sense and have the\\ntwo robot hands attached to robot arms open the jar, like how a human would.‚Äù Success rate: 1.0.\\nThrowing and catching (middle). Prompt: ‚ÄúUse common sense and have the two robot hands attached\\nto robot arms throw the ball in the air, the ball goes up off the screen, hands move to positions to catch\\nthe ball, and catch the falling ball, like how a human would.‚Äù Success rate: 1.0. Rotating Baoding\\nballs (bottom). Prompt: ‚ÄúA human hand holds two metal Baoding balls. The fingers, including the\\nthumb, index, and middle finger, skillfully manipulate the balls, causing them to rotate smoothly like two\\nplanets orbiting around each other and continuously in the palm, one ball circling the other in a fluid\\nmotion.‚Äù Success rate: 0.08.\\nFigure 45 | Affordance recognition. Prompt: ‚ÄúThe robot hands mounted on robot arms pick up the\\nhammer, naturally like how a human would.‚Äù Success rate: 0.5.\\nFigure 46 | Drawing. Prompt: ‚ÄúA person draws a square. Static camera, no pan, no zoom, no dolly.‚Äù\\nSuccess rate: 0.33.\\n27'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='Video models are zero-shot learners and reasoners\\nFigure 47 | Visual instruction generation. Prompt: ‚ÄúA montage clearly showing each step to roll a\\nburrito.‚Äù Success rate: 0.25. Inspiration: [26] and Reddit.\\nA.4. Reasoning\\nFigure 48 | Graph traversal. Prompt: ‚ÄúStarting from the blue well, an unlimited supply of blue water\\nmoves through the connected channel system without spilling into the black area.‚Äù Success rate: 0.08.\\nFigure 49 | Tree BFS. Prompt: ‚ÄúFrom the blue water basin, an unlimited supply of water flows at\\nconstant speed into the cave system until all caves are filled. Static camera perspective, no zoom or pan.‚Äù\\nSuccess rate: 0.17.\\nFigure 50 | Sequence completion inspired by Raven‚Äôs progressive matrices. Each of the four pairs\\nshows input (left) and generated output (right). Prompt: ‚ÄúDraw the figure that completes the pattern\\nin the rightmost box. The images in the boxes are static. Do not modify the existing images, only draw in\\nthe empty box. Static camera, no zoom, no pan, no dolly.‚Äù Success rate: 0.33 for dots, 1.0 for arrows,\\n0.75 for shrinking circles, 0.83 for growing squares.\\n28'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='Video models are zero-shot learners and reasoners\\nFigure 51 | Connecting colors. Prompt: ‚ÄúDraw three curves, one connecting each pair of circles of the\\nsame color.‚Äù Success rate: 0.25.\\nFigure 52 | Shape fitting. Prompt: ‚ÄúThe scene shows three colored pieces, and a wooden panel with\\nthree holes. Each colored piece fits into one and only one hole. A hand grabs each colored piece and puts\\nit into an empty hole that has the exact same shape - if it doesn‚Äôt fit, the hand tries another hole. All the\\nobjects must be placed in their respective holes.‚Äù Success rate: 0.25.\\nFigure 53 | Sorting numbers. Prompt: ‚ÄúThe video starts with some numbered bubbles. The bubbles pop\\nand disappear one at a time, in numeric order, starting from the one with the smallest number.‚Äù Success\\nrate: 0.08.\\nFigure 54 | Tool use. Prompt: ‚ÄúA person retrieves the walnut from the aquarium.‚Äù Success rate: 0.92\\n(retrieval via tool) and 0.08 (retrieval via tool without intersecting the glass).\\nFigure 55 | Simple Sudoku completion. Prompt: ‚ÄúCreate a static, smooth, animation that solves the\\ngiven 4x4 sudoku. Enter the missing numbers one by one. Do not change anything else in the picture.\\nOnly fill the numbers in the empty cells so the sudoku is solved properly. A cursor moves and fills the\\ncorrect number in the empty boxes.‚Äù Success rate: 0.67.\\n29'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='Video models are zero-shot learners and reasoners\\nFigure 56 | Water puzzle solving. Prompt: ‚ÄúThe tap is turned on and water starts flowing rapidly\\nfilling the containers. Create a smooth, static animation showing the containers getting filled with water\\nin the correct order.‚Äù (note: not all containers can be filled since some pipes are closed off‚ÄîVeo fills\\nthe correct containers, in the right order.) Success rate: 0.5.\\nFigure 57 | Maze solving. Prompt: ‚ÄúWithout crossing any black boundary, the grey mouse from the\\ncorner skillfully navigates the maze by walking around until it finds the yellow cheese.‚Äù Success rate: 0.17.\\nFigure 58 | Robot navigation. Prompt: ‚ÄúThe robot drives to the blue area. Static camera perspective, no\\nmovement no zoom no scan no pan.‚Äù Success rate: 0.58. Image credit: Micromelon Robotics website\\nwith permission from Tim Hadwen.\\nFigure 59 | Rule extrapolation inspired by ARC-AGI [84]. Prompt: ‚ÄúModify the lower-right grid to\\nadhere to the rule established by the other grids. You can fill cells, clear cells, or change a cell‚Äôs color. Only\\nmodify the lower-right grid, don‚Äôt modify any of the other grids. Static scene, no zoom, no pan, no dolly.‚Äù\\nSuccess rate: 0.08. While Veo 3 doesn‚Äôt follow the prompt perfectly, the output grid (bottom right) is\\ncompleted correctly.\\n30'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='Video models are zero-shot learners and reasoners\\nTrue Positive\\nFalse Positive\\nFalse Negative\\nTrue Negative\\nFigure 60 | Graded Veo 3 edge map. While false negatives reflect genuine oversights of Veo 3 (e.g.,\\ncracks in the road, lettering on the car), many false positives correspond to actual image details that\\nseem to be erroneously excluded from the ground truth (e.g., the outline of the trees, the reflection in\\nthe car window, and the tire profiles).\\nB. Quantitative results: experimental details\\nTable 1 | Video count breakdown for quantitative tasks. For segmentation, 2 √ó 1 splits indicate one\\ntest set with two different background color prompts (white/green). For the prompt sensitivity study\\non the symmetry task (Sec. C), 2 √ó 10 splits indicate 2 splits (random/shape) across 10 tested prompt\\nvariations. For the qualitative tasks, we additionally generated 744 videos (62 tasks √ó 12 samples).\\nTask\\nSplits\\nImgs/split\\nPass@\\nVideo models\\nTotal videos\\nEdge\\n1\\n50\\n10\\n2\\n1000\\nSegmentation\\n2 √ó 1\\n50\\n10\\n2\\n2000\\nObject extraction\\n1\\n54\\n10\\n2\\n1080\\nEditing\\n1\\n30\\n1\\n2\\n60\\nMaze\\n2 √ó 4\\n50\\n10\\n2\\n8000\\nSymmetry\\n2\\n25\\n10\\n2\\n1000\\nSymmetry prompt analysis\\n2 √ó 10\\n25\\n1\\n1\\n500\\nAnalogy\\n4\\n50\\n10\\n2\\n4000\\nTotal\\n17640\\nB.1. Perception: Edge detection\\nWe provide details for the image editing task in Sec. 4.1.\\nEvaluation\\nAs is standard in the literature, we refine and binarize predicted edges and allow\\nfor small local shifts compared to the ground truth [85‚Äì88]. Concretely, we use non-maximum\\nsuppression, then binarize with one of 16 evenly-spaced thresholds, then thin the binary edge map.\\nAt each threshold, we find the optimal mapping between predicted and ground-truth edge pixels\\nwithin a radius of 0.75% of the image diagonal (around 11 pixels). Fig. 60 shows an example rating\\nof a Veo 3-generated edge map. We report the best OIS over ùëòattempts (optimal image scale; the\\nmaximum ùêπ1-score over all thresholds) for the best/last frame.\\nDataset\\nWe used all 50 test images from BIPEDv2 [59, 60].\\nModels & prompts\\nWe tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generate-\\npreview-001 through the Vertex AI API. We also tested Nano Banana gemini-2.5-flash-image-\\npreview through Google AI Studio.\\n31'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='Video models are zero-shot learners and reasoners\\nVeo\\nAll edges in this image become more salient by transforming into black outlines. Then, all objects\\nfade away, with just the edges remaining on a white background. Static camera perspective, no\\nzoom or pan.\\nNano Banana\\nOutline all edges in the image in black, make everything else white.\\nSampling\\nWe generated 10 videos per sample with a fixed prompt.\\nB.2. Perception: Segmentation\\nWe provide details for the image editing task in Sec. 4.2.\\nEvaluation\\nSince the model is free to choose any colors for segmentation masks, we first determine\\nthe number and hue of each mask by considering the hue-difference histogram between the original im-\\nage and the extracted frame. We smooth the histogram with scipy.ndimage.gaussian_filter1d\\nwith a standard deviation of 2. Peaks with a minimum height of 10% of the maximum and at least 10\\nhue steps apart are considered to correspond to predicted segmentation masks. We then map each\\npixel to the mask with the closest hue.\\nContrary to classic instance segmentation [61] or promptable segmentation [11, 12], our prompts\\ndo not specify a class or list of possible classes, a location prior (e.g., point or bounding box), or the\\nnumber of instances in the image. This also means that mapping between predictions and annotated\\ninstances is established. Instead, we pair each ground-truth mask (including the background) with\\nthe predicted mask with the highest IoU (intersection over union), if any. We report mIoU as the\\naverage IoU over all pairs (excluding the background).\\nDataset\\nWe evaluated on 50 randomly chosen test images from LVIS [61] that contain one to three\\nobjects, each with at least 5000 pixels.\\nModels & prompts\\nWe tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generate-\\npreview-001 through the Vertex AI API. We also tested Nano Banana gemini-2.5-flash-image-\\npreview through Google AI Studio.\\n32'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='Video models are zero-shot learners and reasoners\\nVeo\\nCreate an animation of instance segmentation being performed on this photograph: each distinct\\nentity is overlaid in a different flat color.\\nScene:\\n‚Ä¢ The animation starts from the provided, unaltered photograph.\\n‚Ä¢ The scene in the photograph is static and doesn‚Äôt move.\\n‚Ä¢ First, the background fades to {white, green}.\\n‚Ä¢ Then, the first entity is covered by a flat color, perfectly preserving its silhouette.\\n‚Ä¢ Then the second entity, too, is covered by a different flat color, perfectly preserving its\\nsilhouette.\\n‚Ä¢ One by one, each entity is covered by a different flat color.\\n‚Ä¢ Finally, all entities are covered with different colors.\\nCamera:\\n‚Ä¢ Static shot without camera movement.\\n‚Ä¢ No pan.\\n‚Ä¢ No rotation.\\n‚Ä¢ No zoom.\\n‚Ä¢ No glitches or artifacts.\\nNano Banana\\nPerform instance segmentation on this image: Mask each distinct entity in a different opaque\\nflat color that only preserves the silhouette and turn the background green.\\nSampling\\nWe generated 10 videos per sample and prompt.\\nB.3. Manipulation: Object extraction\\nWe provide details for the image editing task in Sec. 4.3.\\nEvaluation\\nWe extract the last frame from each generated video; the resulting image is converted to\\ngreyscale, a binary mask with threshold 200 is applied, and the number of connected components is\\nextracted using scipy.ndimage.label, resulting in the count estimate. We also report the chance\\nbaseline which can be calculated as: random ‚àíchance = 1 ‚àí(1 ‚àíùëù)ùëòwhere ùëùis the probability to get\\nthe count correct via guessing (here: ùëù= 1\\n9) and ùëò‚àà[1, 10].\\nDataset\\nWe generated an animal counting dataset using Nano Banana. Starting from a white 16:9\\nimage, we used the following prompt, where number is in [1, 9] and animal is in [‚Äòdog‚Äô, ‚Äòelephant‚Äô,\\n‚Äòcat‚Äô, ‚Äòbrown bear‚Äô, ‚Äòhorse‚Äô, ‚Äòrabbit‚Äô, ‚Äòraccoon‚Äô]. We manually evaluated the generated dataset for\\ncorrectness; the resulting dataset has 54 images (exactly 6 per count).\\nNano Banana\\nExchange the white space with a realistic photograph of: exactly {number} {animal}, outside, not\\noverlapping, in a natural landscape.\\nModels & prompts\\nWe tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generate-\\npreview-001 through the Vertex AI API.\\n33'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='Video models are zero-shot learners and reasoners\\nVeo\\nThe background changes to white. Then:\\n‚Ä¢ If there is just a single animal: the animal sits in the middle of the image, looking straight at\\nthe camera.\\n‚Ä¢ If there are multiple animals: all animals line up in a row, with ample white space between\\nthem.\\nSampling\\nWe generated 10 videos per sample with a fixed prompt.\\nB.4. Manipulation: Image editing\\nWe provide details for the image editing task in Sec. 4.4.\\nEvaluation\\nWe perform a human study with three human raters to evaluate fidelity (correct edit)\\nand precision (correct edit with no unintended changes like zooming).\\nDataset\\nWe used a random sample of 30 images from the test set of the Emu-edit dataset [62].\\nModels & prompts\\nWe tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generate-\\npreview-001 through the Vertex AI API.\\nVeo\\nCreate a smooth, static animation that slowly {image specific edit direction}. Do not change\\nanything else. No zoom, no pan, no dolly.\\nSampling\\nFor each image, we generated two samples and use the first sample for human rating.\\nB.5. Reasoning: Maze solving\\nWe provide details for the image editing task in Sec. 4.5.\\nEvaluation\\nOur evaluation process is tailored to the model type. For Veo, we analyze the generated\\nvideo frame-by-frame, extracting the path taken by the agent (red circle). We check for any invalid\\nmoves, such as jumping over walls, clipping through boundaries, or any alteration of the goal‚Äôs position.\\nWe report the success rate as the fraction of ùëòattempts where the agent successfully reaches the goal\\n(green circle) without any illegal moves.\\nFor Nano Banana, which generates the full path in one edit, we assess whether the drawn path\\nconnects the start and end points (allowing for minor discontinuities) and crucially, whether it\\nintersects with any maze walls or goes off the valid path.\\nFor Gemini 2.5 Pro with a maze input as an image (I2T) or as ASCII (T2T), we check whether the\\nseries of grid positions represents an uninterrupted path from the start position to the goal.\\nDataset\\nFor rectangular mazes, we generated 50 random mazes per size using maze-dataset [89],\\nbut replacing the square start and end with circles and swapping their colors. We also drew 10\\nirregular mazes by hand and flipped/rotated them to obtain 40 unique samples.\\nModels & prompts\\nWe tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generate-\\npreview-001 through the Vertex AI API. We also tested Nano Banana gemini-2.5-flash-image-\\npreview and Gemini 2.5 Pro gemini-2.5-pro through Google AI Studio.\\n34'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='Video models are zero-shot learners and reasoners\\nVeo\\nCreate a 2D animation based on the provided image of a maze. The red circle slides smoothly along\\nthe white path, stopping perfectly on the green circle. The red circle never slides or crosses into the\\nblack areas of the maze. The camera is a static, top-down view showing the entire maze.\\nMaze:\\n‚Ä¢ The maze paths are white, the walls are black.\\n‚Ä¢ The red circle moves to the goal position, represented by a green circle.\\n‚Ä¢ The red circle slides smoothly along the white path.\\n‚Ä¢ The red circle never slides or crosses into the black areas of the maze.\\n‚Ä¢ The red circle stops perfectly on the green circle.\\nScene:\\n‚Ä¢ No change in scene composition.\\n‚Ä¢ No change in the layout of the maze.\\n‚Ä¢ The red circle travels along the white path without speeding up or slowing down.\\nCamera:\\n‚Ä¢ Static camera.\\n‚Ä¢ No zoom.\\n‚Ä¢ No pan.\\n‚Ä¢ No glitches, noise, or artifacts.\\n35'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='Video models are zero-shot learners and reasoners\\nGemini 2.5 Pro I2T\\nsystem\\nThink step by step as needed and output in xml format:\\n<think>thinking process</think>\\n<final_answer>final answer</final_answer>\\nuser\\nThe following image shows a maze, represented by colored squares:\\n‚Ä¢ Black squares represent walls and cannot be passed through.\\n‚Ä¢ White squares are empty and can be passed through.\\n‚Ä¢ The red square is the starting point.\\n‚Ä¢ The green square is the end point.\\nPlease solve the maze by providing a path from the starting point to the end point. The path should\\nbe provided as a list of coordinates of each step, where each coordinate is a (row, col) tuple, and\\nrow, col are 0-based indices. Consider the origin (0, 0) to be the top-left corner. Overall, the path\\nshould be provided in the format of [(row1, col1), (row2, col2), ...].\\nA valid path must:\\n‚Ä¢ Start at the starting point (the red square).\\n‚Ä¢ End at the end point (the green square).\\n‚Ä¢ Avoid the walls (the black squares).\\n‚Ä¢ Pass only through empty space (the white squares).\\n‚Ä¢ Move one square at a time.\\n‚Ä¢ Only move up, down, left, and right, not diagonally.\\nCorrect your answer if you spot any errors.\\nHere is the maze image: {image}\\nNano Banana\\nMark the correct path from the red to the green circle through the maze in blue.\\n36'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 36}, page_content='Video models are zero-shot learners and reasoners\\nGemini 2.5 Pro T2T\\nsystem\\nThink step by step as needed and output in xml format:\\n<think>thinking process</think>\\n<final_answer>final answer</final_answer>\\nuser\\nThe following is an ASCII-representation of a maze:\\n‚Ä¢ ‚Äò#‚Äô represents walls which cannot be passed through.\\n‚Ä¢ ‚Äò ‚Äô represents empty spaces that can be passed through.\\n‚Ä¢ ‚ÄòS‚Äô is the starting point.\\n‚Ä¢ ‚ÄòE‚Äô is the end point.\\nPlease solve the maze by providing a path from the starting point to the end point. The path should\\nbe provided as a list of coordinates of each step, where each coordinate is a (row, col) tuple, and\\nrow, col are 0-based indices. Consider the origin (0, 0) to be the top-left corner. Overall, the path\\nshould be provided in the format of [(row1, col1), (row2, col2), ...].\\nA valid path must:\\n‚Ä¢ Start at the starting point ‚ÄòS‚Äô.\\n‚Ä¢ End at the end point ‚ÄòE‚Äô.\\n‚Ä¢ Avoid the walls ‚Äò#‚Äô.\\n‚Ä¢ Pass only through empty space ‚Äò ‚Äô.\\n‚Ä¢ Move one square at a time.\\n‚Ä¢ Only move up, down, left, and right, not diagonally.\\nCorrect your answer if you spot any errors.\\nHere is the maze in ASCII format: {maze}\\nSampling\\nWe generated 10 videos per sample with a fixed prompt. Note that for Gemini 2.5 Pro\\nI2T, we represented the maze as a grid where the red and green positions are marked as squares (not\\ncircles) to make the setup grid-like (i.e., a matrix with cells), since this might be easier for a language\\nmodel.\\nB.6. Reasoning: Visual symmetry solving\\nWe provide details for the visual symmetry task in Sec. 4.6.\\nEvaluation\\nWe prompt Veo with input images containing a 10√ó16 grid where a pattern is drawn on\\nthe left half. The goal is to complete the pattern on the empty right half so that the final pattern is\\nsymmetrical along the central vertical axis.\\nWe compare Veo‚Äôs best-frame and last-frame solutions with the ground-truth symmetrical grid\\nand compute the number of incorrectly-colored cells. A cell is determined as incorrectly-colored if\\nthe average color across pixels in the cell is perceptually distinct from the ground-truth average color\\nin the matching cell. We compute perceptual color differences of the average cell color in the CIELAB\\ncolor space, with a difference threshold of 15.0. In Fig. 8, we report the percentage of attempts in\\nwhich the best or last frame solution has zero incorrect cells for ùëò= 1.\\nDataset\\nWe created a synthetic grid coloring image dataset to evaluate visual symmetry. We\\ngenerated 25 samples using common symmetrical symbols, objects and shapes such as english letters\\n(e.g., A, H, M, X), geometric shapes (e.g., square, triangle), symmetrical objects (e.g., wineglass,\\n37'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 37}, page_content='Video models are zero-shot learners and reasoners\\nMajority vote pass@k %\\nColour\\nResize\\nReflect\\nRotate\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n40\\n60\\n80\\n100\\n98\\n85\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n40\\n50\\n60\\n70\\n68\\n50\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n20\\n25\\n30\\n22\\n17\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\nk\\n10\\n15\\n20\\n25\\n30\\n14\\n7\\nModel\\nVeo 3\\nVeo 2\\nChance\\nFigure 61 | Visual analogy performance over 10 attempts. In contrast to other plots in this paper,\\nwe here report not the best performance over ùëòattempts, but instead the performance when choosing\\nthe majority vote from ùëòattempts. As a result, performance is not necessarily monotonic in ùëò. In\\nfact, for reflect and rotate, performance decreases with ùëò, indicating that both models have systematic,\\nerroneous biases. In the case of Veo 3, the model tends to perform reflections and rotations, but not\\nalong the same axis as shown in the image. Veo 2 simply tends to copy the object without applying\\nany transformation.\\nballoon; together, the shape condition). We also generated 25 samples consisting of randomly-colored\\ncells (the random condition).\\nModels & prompts\\nWe tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generate-\\npreview-001 through the Vertex AI API. We also tested Nano Banana gemini-2.5-flash-image-\\npreview through Google AI Studio.\\nVeo\\nInstantly reflect this pattern along the central, vertical axis while keeping the existing colored\\npattern without modification. Static camera perspective, no zoom or pan.\\nSampling\\nWe generated 10 videos per sample with a fixed prompt.\\nB.7. Reasoning: Visual analogy completion\\nWe provide details for the visual analogy task in Sec. 4.7.\\nEvaluation\\nWe prompt Veo to solve visual analogies with an input image showing a reference\\nobject pair and a test object. The object images are sourced from the Kid-inspired Visual Analogies\\nbenchmark [KiVA, 66]. Consistent with the multi-choice format in the KiVA benchmark, we evaluated\\nVeo‚Äôs generation by cropping out the generated target object in the lower-right region of the last\\nframe and compare Veo‚Äôs generated object with three candidate object choices using an autorater\\n(see details below).\\nIn Fig. 9, we report the pass@1 accuracy across different conditions for both Veo 2 and Veo 3 for\\nùëò= 1. Fig. 61 shows performance for ùëò= 10.\\nDataset\\nWe used the test trials and choice images from the KiVA benchmark [66].\\n38'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 38}, page_content='Video models are zero-shot learners and reasoners\\nModels & prompts\\nWe tested Veo 3 veo-3.0-generate-preview and Veo 2 veo-2.0-generate-\\npreview-001 through the Vertex AI API.\\nWe used Gemini 2.5 Pro gemini-2.5-pro through Google AI Studio to identify which image\\nchoice Veo‚Äôs generation is most similar with. To enhance the autorater‚Äôs image comparison accuracy\\nfor this task, Gemini is prompted with privileged information about the values in the dataset conditions\\n(see below for the full autorater prompt). If no object is visible in the lower-right region of Veo‚Äôs\\ngenerated last frame or if the generated object is of a different object type, we randomly sampled one\\nof three choices as Veo‚Äôs choice. In pilot experiments, we found that the Gemini-assisted autorater‚Äôs\\nratings achieve above 88% agreement with expert human ratings by the authors on 25 samples within\\neach conditions.\\nNote that in the prompt, words in { } are updated based on the test condition of the current\\ngeneration (one of color, resize, reflect, and rotate) to provide more information of the feature name\\nand values to direct the image comparison. Image choice orders are shuffled for each prompt.\\nVeo\\nCreate a smooth animation to generate the missing object in the lower right region and solve the\\nvisual analogy. The original three objects must remain still. Static shot, no zoom no pan no dolly.\\nGemini 2.5 Pro autorater\\nsystem\\nYou are an expert visual judge. You will be presented with a \"target image\" and three \"choice\\nimages\" labeled A, B, and C. Your goal is to identify the choice image that is most visually similar\\nto the target image.\\nFollow these steps:\\n1. Analyze each provided image and describe the objects shown. Focus on the object {color}.\\nThat is, if the objects appear {green}, {blue}, or {red}.\\n2. Determine if the primary object in the target image is of the same general category or type\\nas the objects in the choice images. For example, if the target image shows a dog, and the\\nchoices show a cat, the object types are considered different. If no object is visible in the target\\nimage, the object type is considered to be mismatched.\\n3. If the object type matches between the target image and the choice images, identify the choice\\nthat is most visually similar to the target image in terms of the object {color}.\\nProvide a brief justification for your choice, explaining why it is the best match and why the others\\nare less suitable. Conclude your response with the final answer on a new line in the format:\\n‚ÄúFinal Answer: [answer]‚Äù\\nwhere ‚Äúanswer‚Äù is one of (‚ÄúA‚Äù, ‚ÄúB‚Äù, ‚ÄúC‚Äù, or ‚Äúdifferent object type‚Äù). Do not use markdown format\\nfor the final answer line.\\nuser\\nPlease evaluate the following images.\\n‚Äî TARGET IMAGE ‚Äî\\n{target object image}\\n‚Äî CHOICE IMAGES ‚Äî\\nCHOICE A: {image choice} CHOICE B: {image choice} CHOICE C: {image choice}\\nWhich choice image is most similar to the target image?\\nSampling\\nWe generated 10 videos per sample with a fixed prompt.\\n39'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 39}, page_content='Video models are zero-shot learners and reasoners\\nC. Prompting best practices\\nTable 2 | Prompt sensitivity study on the visual symmetry task. We report best frame pass@1 %\\nand the average number of incorrectly-colored cells across 25 samples on each split (shape/random).\\nPass@1\\nAvg incorrect cells\\nNo.\\nPrompt\\nShape\\nRandom\\nShape\\nRandom\\n1\\nInstantly reflect this pattern along the central, vertical\\naxis while keeping the existing colored pattern without\\nmodification.\\n48\\n68\\n4.16\\n7.00\\n2\\nInstantly reflect this pattern along the central, vertical\\naxis while keeping the existing colored pattern without\\nmodification. Static camera perspective, no zoom or pan.\\n42\\n65\\n5.00\\n3.52\\n3\\nInstantly reflect this pattern along the central, vertical\\naxis while keeping the existing colored pattern without\\nmodification. The result needs to be mirror-symmetrical\\nalong the vertical axis. Static camera perspective, no\\nzoom or pan.\\n36\\n52\\n6.28\\n9.04\\n4\\nOne by one, cells in the right half of the grid are filled in\\nto complete the pattern. The pattern is\\nmirror-symmetrical along the central vertical line. Static\\nshot; no zoom, no pan, no dolly.\\n32\\n12\\n10.76\\n14.08\\n5\\nReflect this pattern along the central, vertical axis.\\n28\\n40\\n9.76\\n4.52\\n6\\nAn animation showing the left half of the grid being\\nmirrored onto the right half to create a symmetrical\\npattern. Static shot; no zoom, no pan, no dolly.\\n24\\n12\\n10.96\\n16.32\\n7\\nYou‚Äôre a master symmetry solver. Your task is to fill the\\ncells on the right side of the grid to mirror the pattern on\\nthe left, such that it‚Äôs symmetrical along the vertical axis.\\n24\\n8\\n9.20\\n17.72\\n8\\nFill color in the appropriate cells on the right side of the\\ngrid to complete the pattern. The final image should be\\nsymmetrical along the central vertical line. Static shot, no\\nzoom no pan no dolly.\\n13\\n9\\n10.30\\n14.74\\n9\\nCreate a static, smooth, realistic animation completing\\nthe pattern in the image by filling the grid on the right\\nhand side. Do not change anything else. No zoom, no\\npan.\\n12\\n4\\n14.88\\n21.00\\n10\\nA timelapse of a professional pixel artist drawing a\\nsymmetrical pattern onto a white canvas. Static shot; no\\nzoom, no pan, no dolly.\\n8\\n20\\n14.20\\n12.64\\nThe results in Secs. 3 and 4 are best-effort estimates of Veo‚Äôs performance using carefully chosen\\nprompts. Generally, performance varies greatly with the exact task description provided in the prompt,\\nas illustrated by a prompt sensitivity study on the visual symmetry task in Table 2. Here are best\\npractices from this sensitivity analysis and our other experiments:\\n‚Ä¢ Remove ambiguity. Tasks can be solved in a variety of ways, and natural language descriptions\\ntend to leave a lot of room for interpretation. The goal should be formulated clearly, e.g., saying\\n‚Äúsymmetrical along the central, vertical axis‚Äù, rather than just ‚Äúsymmetrical‚Äù.\\n‚Ä¢ Specify what shouldn‚Äôt change. Veo has a tendency to change any part of the input to create\\ninteresting, dynamic scenes. Including not only a positive task description, but also specifying\\nwhat not to change can help mitigate this, e.g., ‚Äúkeep the existing colored pattern without\\nmodification‚Äù.\\n‚Ä¢ Providing an outlet. As mentioned above, Veo has a strong prior to keep things moving.\\nProviding a ‚Äúmotion outlet‚Äù in the form of, e.g., a spinning ball can help keep the rest of the\\nscene static.\\n‚Ä¢ Let the model decide when its done. The motion prior also means that Veo often keeps\\n40'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 40}, page_content='Video models are zero-shot learners and reasoners\\nmodifying the scene, even after solving the task. Providing a visual indicator, e.g., ‚Äúadd a\\nglowing red dot once the goal is reached‚Äù allows for easy extraction of the solution from the\\ngenerated video.\\n‚Ä¢ Scene and camera controls. Phrases like ‚Äústatic camera, no zoom, no pan, no dolly‚Äù can help\\nkeeping the scene static, e.g., for image-to-image tasks.\\n‚Ä¢ Speed control. Some tasks like maze solving benefit from being solved step-by-step. For other\\ntasks, especially image-to-image tasks, specifying instant changes can help avoid artifacts.\\n‚Ä¢ Realism. Veo was trained to generate plausible, realistic-looking videos. Translating an abstract\\ntask into a realistic setting (including, but not limited to editing the original image to depict\\nrealistic, 3D scenes rather than abstract shapes) can improve generation results. A similar effect\\nwas observed in [90], and we expect visual prompt engineering to emerge as a powerful tool for\\nvideo models.\\n41'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 41}, page_content='Video models are zero-shot learners and reasoners\\nD. Failure cases\\nFigure 62 | Monocular depth estimation. Prompt: ‚ÄúThe image transitions to a depth-map of the\\nscene: Darker colors represent pixels further from the camera, lighter colors represent pixels closer to\\nthe camera. The exact color map to use is provided on the right side of the image. Static scene, no pan,\\nno zoom, no dolly.‚Äù Failure: Veo 3 seems generally unable to color pixels by depth beyond a binary\\nforeground/background mapping and specifically struggles with using a provided color map.\\nFigure 63 | Monocular surface normal estimation. Prompt: ‚ÄúThe image transitions to a surface-\\nnormal map of the scene: the red/green/blue color channel specify the direction of the surface-normal at\\neach point, as illustrated on the right side of the image on a sphere. Static scene, no pan, no zoom, no\\ndolly.‚Äù Failure: While Veo 3 shows some promise in coloring surfaces according to their orientation\\n(e.g., the cube in the front), coloration is inconsistent (compare the two cubes) and doesn‚Äôt correctly\\ninterpolate colors (e.g., for the slope on the triangle).\\nFigure 64 | Force & motion prompting, inspired by [91, 92]. Force prompting (top). Prompt: ‚ÄúThe\\nballs move in the direction indicated by the arrows. Balls without an arrow don‚Äôt move. Static scene, no\\npan, no zoom, no dolly.‚Äù Motion trajectory prompting (bottom). Prompt: ‚ÄúEach car drives out of\\nthe frame following the indicated trajectory. Static camera, no zoom, no pan, no dolly.‚Äù Failure: Veo 3\\nseems unable to follow force/motion annotations with any consistency. Providing annotations for the\\nfirst frame and letting the model remove them before generating the scene in motion does not work,\\neither.\\n42'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 42}, page_content='Video models are zero-shot learners and reasoners\\nFigure 65 | Tying the knot. Prompt: ‚ÄúA knot is tied connecting these two rope ends.‚Äù Failure: physics\\nviolation, impossible rope movement.\\nFigure 66 | Connect the path puzzle. Prompt: ‚ÄúThe path connecting the boy to the object starts glowing\\nslowly. Nothing else changes. No zoom, no pan, no dolly.\" Failure: hallucinations, lighting up of all\\npaths.\\nFigure 67 | Five letter word search. Prompt: ‚ÄúGenerate a static video animation using the provided\\nletter grid. The task is to highlight the only 5-letter English word CHEAT, which may be oriented in any\\ndirection (horizontally, vertically, or diagonally). The animation should consist of a semi-transparent\\nred rectangle with rounded corners smoothly fading into view, perfectly encapsulating the five letters of\\nthe word. The rectangle should have a subtle, soft glow. Do not change anything else in the image. The\\ncamera must remain locked in place with no movement. No zoom, no pan, no dolly.\" Failure: does not\\nrecognize words; highlights individual letters randomly.\\nFigure 68 | Eulerian path. Prompt: ‚ÄúCreate a smooth animation where a red pen traces all existing\\nedges in a continuous path without lifting the pen. All edges need to be traced. Do not visit any edge twice\\nand do not lift the pen. No zoom, no pan.\" Failure: does not trace the edges exactly, traces non-existent\\nedges.\\n43'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 43}, page_content='Video models are zero-shot learners and reasoners\\nFigure 69 | Solving system of linear equations. Prompt: ‚ÄúA hand appears and solves the set of linear\\nequations. It replaces the x, y, z matrix with their correct values that solves the equation. Do not change\\nanything else.\" Failure: hallucinations with text on the blackboard.\\nFigure 70 | Spot the difference. Prompt: ‚ÄúThere are two images. The left image is different from the\\nright image in 5 spots. Create a static, realistic, smooth animation where a cursor appears and points at\\neach place where the left image is different from the right image. The cursor points one by one and only\\non the left image. Do not change anything in the right image. No pan. No zoom. No movement. Keep the\\nimage static.\" Failure: does not identify all the differences. Hallucinates differences.\\nFigure 71 | Visual IQ test. Prompt: ‚ÄúCreate a static, smooth, animation that solves the puzzle in the\\ngiven image. The correct pattern should appear at the bottom right to solve the puzzle. Do not change\\nanything else in the picture. No zoom, no pan, no dolly\" Failure: incorrect figure pattern.\\nFigure 72 | Glass falling. Prompt: ‚ÄúThe object falls. Static camera, no pan, no zoom, no dolly.‚Äù Failure:\\nphysics violation, glass does not break, and orients itself to be vertical after landing on the floor.\\nFigure 73 | Collisions. Prompt: ‚ÄúThe two objects collide in slow motion. Static camera, no pan, no zoom,\\nno dolly.‚Äù Failure: not physically plausible, the objects pause at the moment of impact and then are\\npushed together by an invisible force.\\n44'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 44}, page_content='Video models are zero-shot learners and reasoners\\nFigure 74 | Tiling puzzles. Jigsaw puzzle (top). Prompt: ‚ÄúA hand takes the fitting puzzle piece from\\nthe right, rotates it to be in the correct orientation, then puts it into the hole, completing the puzzle.\\nStatic scene, no pan, no zoom, no dolly.‚Äù Failure: wrong piece orientation. Sliding puzzle (middle).\\nPrompt: ‚ÄúSlide the pieces of this sliding puzzle around one-at-a-time until all edges align.‚Äù Failure:\\ndoesn‚Äôt maintain piece integrity while sliding, hallucinates new pieces. Scrambled puzzle (bottom).\\nPrompt: ‚ÄúUnscramble this image.‚Äù Failure: image details are inconsistent with original pieces.\\nFigure 75 | Bottleneck. Prompt: ‚ÄúA person tries to put the golf ball in the vase. Static camera, no pan, no\\nzoom, no dolly..‚Äù Failure: not physically plausible, golf ball is too large to pass through the bottleneck\\nof the vase.\\nFigure 76 | Laundry folding. Prompt: ‚ÄúGenerate a video of two metal robotic arms properly folding the\\nt-shirt on the table. Failure: physics violation, implausible folding movements.\\n45'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Video_models_ZS_learners_reasoners.pdf', 'total_pages': 46, 'format': 'PDF 1.7', 'title': 'Video models are zero-shot learners and reasoners', 'author': 'Thadd√§us Wiedemer; Yuxuan Li; Paul Vicol; Shixiang Shane Gu; Nick Matarese; Kevin Swersky; Been Kim; Priyank Jaini; Robert Geirhos', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 45}, page_content='Video models are zero-shot learners and reasoners\\nFigure 77 | Motion planning; inspired by the piano mover‚Äôs problem. Prompt: ‚ÄúThe red couch slides\\nfrom the left room over into the right room, skillfully maneuvering to fit through the doorways without\\nbumping into the walls. The walls are fixed: they don‚Äôt shift or disappear, and no new walls are introduced.\\nStatic camera, no pan, no zoom, no dolly.‚Äù Failure: violating rigid-body integrity, not keeping to\\npermissible transformations (rotation, translation).\\nE. LLM use\\nGemini 2.5 Flash and Gemini 2.5 Pro [2] were used for brainstorming task ideas, suggesting related\\nwork that we might have otherwise missed, coding support, and to polish human writing.\\nF. Image sources\\nWhere not stated in the figure caption, images were obtained as follows.\\n‚Ä¢ Figs. 10 to 15, 32 to 38 and 74: The original macaw image was generated with Gemini and,\\ndepending on the figure, subsequently modified by the authors (e.g., conversion to grayscale,\\nadding noise, adding the monkey with Nano Banana).\\n‚Ä¢ Fig. 16: The input image was obtained from here (Apache 2.0 license) based on the LOLv2\\ndataset [78] and randomly selected. The image was slightly cropped to fit a 16:9 aspect ratio.\\n‚Ä¢ Figs. 17, 21 to 24, 26 to 29, 31, 39, 41, 42, 46, 47, 52, 54, 65, 69, 72, 73 and 75 to 77:\\ngenerated with Gemini.\\n‚Ä¢ Fig. 25: The input image was obtained from here (CC0 license).\\n‚Ä¢ Fig. 30: hand drawn by us, inspired by Fig. 1 of the Omniglot paper [52].\\n‚Ä¢ Fig. 40: sample from Objaverse [83]\\n‚Ä¢ Figs. 48 to 51, 53, 55 and 57: created by us.\\n‚Ä¢ Figs. 56, 66, 67 and 70: original image from Reddit.\\n‚Ä¢ Fig. 59: hand drawn by us, inspired by ARC-AGI [84].\\n‚Ä¢ Fig. 60: sample from BIPEDv2 [59, 60].\\n‚Ä¢ Figs. 62 to 64: generated with Gemini, then annotated by us.\\n‚Ä¢ Figs. 68 and 71: hand drawn by us. Inspired by original images from Reddit.\\n‚Ä¢ Figs. 44 and 45: The robot hands are extracted from a frame in this video and were subsequently\\nadapted with Nano Banana. The hands holding Baoding balls were obtained from here.\\n46')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf_files\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e070bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e32c88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Adobe PDF Library 17.0; modified using iText 4.2.0 by 1T3XT',\n",
       " 'creator': 'Adobe InDesign 19.5 (Windows)',\n",
       " 'creationdate': '2025-10-24T19:40:28+08:00',\n",
       " 'source': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf',\n",
       " 'file_path': '..\\\\data\\\\pdf_files\\\\science.adw1291.pdf',\n",
       " 'total_pages': 4,\n",
       " 'format': 'PDF 1.3',\n",
       " 'title': 'Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping',\n",
       " 'author': '',\n",
       " 'subject': 'Science 2025.389:1012-1015',\n",
       " 'keywords': '',\n",
       " 'moddate': '2025-11-30T15:08:48-08:00',\n",
       " 'trapped': '',\n",
       " 'modDate': \"D:20251130150848-08'00'\",\n",
       " 'creationDate': \"D:20251024194028+08'00'\",\n",
       " 'page': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16200b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
